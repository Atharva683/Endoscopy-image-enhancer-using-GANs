{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Enhancement GAN - Local Setup\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a Generative Adversarial Network (GAN) for enhancing medical images from the Kvasir-SEG dataset. The model takes degraded medical images and restores them to high quality using a U-Net generator and discriminator architecture.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Install Required Dependencies\n",
    "```bash\n",
    "pip install torch torchvision matplotlib pillow numpy scikit-image\n",
    "```\n",
    "\n",
    "### 2. Download and Prepare Dataset\n",
    "1. Download the Kvasir-SEG dataset from: https://datasets.simula.no/kvasir-seg/\n",
    "2. Extract the dataset in your project directory\n",
    "3. Ensure the folder structure is:\n",
    "   ```\n",
    "   your_project/\n",
    "   ├── data/\n",
    "   │   └── Kvasir-SEG/\n",
    "   │       └── images/\n",
    "   │           ├── image_00001.jpg\n",
    "   │           ├── image_00002.jpg\n",
    "   │           └── ...\n",
    "   ├── output/ (will be created automatically)\n",
    "   └── ganproject (2).ipynb\n",
    "   ```\n",
    "\n",
    "### 3. System Requirements\n",
    "- GPU recommended (CUDA compatible) for faster training\n",
    "- At least 8GB RAM\n",
    "- Python 3.7+ with PyTorch installed\n",
    "\n",
    "### 4. Expected Output\n",
    "- Training will create enhanced images in `output/generated_images/`\n",
    "- Model checkpoints will be saved in `output/generated_images/checkpoints/`\n",
    "- Training progress will be displayed with loss metrics and sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:47.871735Z",
     "iopub.status.busy": "2025-06-05T08:30:47.871221Z",
     "iopub.status.idle": "2025-06-05T08:30:53.495744Z",
     "shell.execute_reply": "2025-06-05T08:30:53.495155Z",
     "shell.execute_reply.started": "2025-06-05T08:30:47.871710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg19\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.497312Z",
     "iopub.status.busy": "2025-06-05T08:30:53.496916Z",
     "iopub.status.idle": "2025-06-05T08:30:53.584816Z",
     "shell.execute_reply": "2025-06-05T08:30:53.584028Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.497291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.586056Z",
     "iopub.status.busy": "2025-06-05T08:30:53.585771Z",
     "iopub.status.idle": "2025-06-05T08:30:53.597077Z",
     "shell.execute_reply": "2025-06-05T08:30:53.596396Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.586031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    # Local paths for running on your device\n",
    "    DATA_ROOT = 'kvasir-seg\\Kvasir-SEG\\images'  # Local path to the dataset\n",
    "    OUTPUT_DIR = 'output/generated_images'  # Local output directory\n",
    "    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
    "\n",
    "    IMAGE_SIZE = 256\n",
    "    NUM_CHANNELS = 3\n",
    "\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE_G = 2e-4\n",
    "    LEARNING_RATE_D = 1e-4\n",
    "    BETA1 = 0.5\n",
    "    LAMBDA_L1 = 70.0\n",
    "    LAMBDA_PERCEPTUAL = 10.0\n",
    "\n",
    "   \n",
    "    G_TRAIN_MULTIPLIER = 3 \n",
    "\n",
    "    NOISE_STD_DEV = 0.15\n",
    "    NOISE_STD_DEV_VARIATION = 0.05\n",
    "    \n",
    "    SALT_VS_PEPPER_RATIO = 0.5\n",
    "    SP_NOISE_PROB = 0.01\n",
    "    \n",
    "    BLUR_KERNEL_SIZE = 5\n",
    "    BRIGHTNESS_FACTOR = 0.2\n",
    "    CONTRAST_FACTOR = 0.2\n",
    "    SATURATION_FACTOR = 0.2\n",
    "    HUE_FACTOR = 0.05\n",
    "\n",
    "    \n",
    "    LR_DECAY_START_EPOCH = 30 \n",
    "    SAVE_EVERY_N_EPOCHS = 5\n",
    "    LOG_EVERY_N_BATCHES = 10\n",
    "\n",
    "    DEVICE = DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.598211Z",
     "iopub.status.busy": "2025-06-05T08:30:53.597940Z",
     "iopub.status.idle": "2025-06-05T08:30:53.609662Z",
     "shell.execute_reply": "2025-06-05T08:30:53.609022Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.598187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_degradation(image_tensor, config):\n",
    "    image_tensor_cpu = image_tensor.cpu()\n",
    "    degraded_tensor = image_tensor_cpu.clone() \n",
    "\n",
    "    current_noise_std_dev = config.NOISE_STD_DEV + random.uniform(-config.NOISE_STD_DEV_VARIATION, config.NOISE_STD_DEV_VARIATION)\n",
    "    current_noise_std_dev = max(0, current_noise_std_dev)\n",
    "\n",
    "    gaussian_noise = torch.randn_like(degraded_tensor) * current_noise_std_dev\n",
    "    degraded_tensor = degraded_tensor + gaussian_noise\n",
    "    degraded_tensor = torch.clamp(degraded_tensor, 0, 1)\n",
    "\n",
    "\n",
    "    num_pixels = degraded_tensor.numel()\n",
    "    num_sp_noise_pixels = int(num_pixels * config.SP_NOISE_PROB)\n",
    "\n",
    "    noise_indices = torch.randperm(num_pixels)[:num_sp_noise_pixels]\n",
    "\n",
    "    salt_mask = torch.rand(num_sp_noise_pixels) < config.SALT_VS_PEPPER_RATIO\n",
    "    pepper_mask = ~salt_mask\n",
    "\n",
    "    degraded_tensor_flat = degraded_tensor.view(-1)\n",
    "\n",
    "    # Apply pepper noise (set to 0)\n",
    "    degraded_tensor_flat[noise_indices[pepper_mask]] = 0.0\n",
    "\n",
    "    # Apply salt noise (set to 1)\n",
    "    degraded_tensor_flat[noise_indices[salt_mask]] = 1.0\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    degraded_tensor = degraded_tensor_flat.view(degraded_tensor.shape)\n",
    "\n",
    "\n",
    "    if config.BLUR_KERNEL_SIZE > 0:\n",
    "        blur_transform = transforms.GaussianBlur(\n",
    "            kernel_size=(config.BLUR_KERNEL_SIZE, config.BLUR_KERNEL_SIZE),\n",
    "            sigma=(0.1, 2.0)\n",
    "        )\n",
    "        degraded_tensor = blur_transform(degraded_tensor)\n",
    "\n",
    "    jitter_transform = transforms.ColorJitter(\n",
    "        brightness=config.BRIGHTNESS_FACTOR,\n",
    "        contrast=config.CONTRAST_FACTOR,\n",
    "        saturation=config.SATURATION_FACTOR,\n",
    "        hue=config.HUE_FACTOR\n",
    "    )\n",
    "    degraded_tensor = jitter_transform(degraded_tensor)\n",
    "\n",
    "    return degraded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.611669Z",
     "iopub.status.busy": "2025-06-05T08:30:53.611397Z",
     "iopub.status.idle": "2025-06-05T08:30:53.624904Z",
     "shell.execute_reply": "2025-06-05T08:30:53.624298Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.611651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_images(low_res, real_high_res, fake_high_res, epoch, batch_idx, output_dir):\n",
    "    low_res_np = (low_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    real_high_res_np = (real_high_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    fake_high_res_np = (fake_high_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(Config.BATCH_SIZE, 3, figsize=(9, 3 * Config.BATCH_SIZE))\n",
    "    fig.suptitle(f'Epoch {epoch}, Batch {batch_idx}', fontsize=16)\n",
    "\n",
    "    for i in range(Config.BATCH_SIZE):\n",
    "        axes[i, 0].imshow(low_res_np[i])\n",
    "        axes[i, 0].set_title('Low-Quality Input')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(real_high_res_np[i])\n",
    "        axes[i, 1].set_title('Real High-Quality')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(fake_high_res_np[i])\n",
    "        axes[i, 2].set_title('Generated High-Quality')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    filename = os.path.join(output_dir, f'epoch_{epoch}_batch_{batch_idx}.png')\n",
    "    \n",
    "    try:\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Successfully saved image to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image {filename}: {e}\")\n",
    "    finally:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.625880Z",
     "iopub.status.busy": "2025-06-05T08:30:53.625639Z",
     "iopub.status.idle": "2025-06-05T08:30:53.636995Z",
     "shell.execute_reply": "2025-06-05T08:30:53.636402Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.625863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(real_images, fake_images):\n",
    "    real_images_np = (real_images * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    fake_images_np = (fake_images * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "\n",
    "    for i in range(real_images_np.shape[0]):\n",
    "        psnr = peak_signal_noise_ratio(real_images_np[i], fake_images_np[i], data_range=1.0)\n",
    "        ssim = structural_similarity(real_images_np[i], fake_images_np[i], data_range=1.0, channel_axis=-1)\n",
    "        psnr_scores.append(psnr)\n",
    "        ssim_scores.append(ssim)\n",
    "    \n",
    "    return np.mean(psnr_scores), np.mean(ssim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.637827Z",
     "iopub.status.busy": "2025-06-05T08:30:53.637646Z",
     "iopub.status.idle": "2025-06-05T08:30:53.646914Z",
     "shell.execute_reply": "2025-06-05T08:30:53.646268Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.637812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KvasirSEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size, transform=None, degradation_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.transform = transform\n",
    "        self.degradation_transform = degradation_transform\n",
    "\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.image_files)} images from {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            high_quality_image = self.transform(image)\n",
    "\n",
    "            high_quality_image_0_1 = (high_quality_image * 0.5 + 0.5)\n",
    "            \n",
    "            if self.degradation_transform:\n",
    "                low_quality_image_0_1 = self.degradation_transform(high_quality_image_0_1)\n",
    "            else:\n",
    "                low_quality_image_0_1 = high_quality_image_0_1\n",
    "\n",
    "            low_quality_image = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])(low_quality_image_0_1)\n",
    "\n",
    "            return low_quality_image, high_quality_image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a dummy tensor if image loading fails\n",
    "            dummy_tensor = torch.zeros(3, self.image_size, self.image_size)\n",
    "            return dummy_tensor, dummy_tensor\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.648267Z",
     "iopub.status.busy": "2025-06-05T08:30:53.647820Z",
     "iopub.status.idle": "2025-06-05T08:30:53.663750Z",
     "shell.execute_reply": "2025-06-05T08:30:53.663066Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.648238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.664800Z",
     "iopub.status.busy": "2025-06-05T08:30:53.664496Z",
     "iopub.status.idle": "2025-06-05T08:30:53.678162Z",
     "shell.execute_reply": "2025-06-05T08:30:53.677497Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.664778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.679039Z",
     "iopub.status.busy": "2025-06-05T08:30:53.678814Z",
     "iopub.status.idle": "2025-06-05T08:30:53.686805Z",
     "shell.execute_reply": "2025-06-05T08:30:53.686170Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.679023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(pretrained=True).features\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            vgg[:2],\n",
    "            vgg[2:7],\n",
    "            vgg[7:12],\n",
    "            vgg[12:21]\n",
    "        ).to(DEVICE).eval()\n",
    "\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, fake_img, real_img):\n",
    "        normalize_vgg = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        fake_img_norm = normalize_vgg((fake_img * 0.5 + 0.5))\n",
    "        real_img_norm = normalize_vgg((real_img * 0.5 + 0.5))\n",
    "\n",
    "        fake_features = self.features(fake_img_norm)\n",
    "        real_features = self.features(real_img_norm)\n",
    "        \n",
    "        return self.criterion(fake_features, real_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.688104Z",
     "iopub.status.busy": "2025-06-05T08:30:53.687691Z",
     "iopub.status.idle": "2025-06-05T08:30:53.704177Z",
     "shell.execute_reply": "2025-06-05T08:30:53.703438Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.688087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    dataset = KvasirSEGDataset(\n",
    "        root_dir=Config.DATA_ROOT,\n",
    "        image_size=Config.IMAGE_SIZE,\n",
    "        degradation_transform=lambda img_tensor: apply_degradation(img_tensor, Config)\n",
    "    )\n",
    "    # Reduced num_workers for local execution (use 0 for Windows compatibility)\n",
    "    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "    if len(dataloader) == 0:\n",
    "        print(f\"Error: Dataloader is empty. No images found in {Config.DATA_ROOT} or batch size is too large for the dataset.\")\n",
    "        print(\"Please ensure the Kvasir-SEG dataset is correctly placed and contains images.\")\n",
    "        return # Exit if no data is found\n",
    "\n",
    "    generator = UnetGenerator(Config.NUM_CHANNELS, Config.NUM_CHANNELS).to(DEVICE)\n",
    "    discriminator = Discriminator(Config.NUM_CHANNELS).to(DEVICE)\n",
    "\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "    perceptual_loss_fn = PerceptualLoss()\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=Config.LEARNING_RATE_G, betas=(Config.BETA1, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=Config.LEARNING_RATE_D, betas=(Config.BETA1, 0.999))\n",
    "\n",
    "    def lambda_rule(epoch):\n",
    "        if epoch < Config.LR_DECAY_START_EPOCH:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 1.0 - (epoch - Config.LR_DECAY_START_EPOCH) / (Config.NUM_EPOCHS - Config.LR_DECAY_START_EPOCH)\n",
    "\n",
    "    scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
    "    scheduler_D = optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=lambda_rule)\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        for i, (low_res_img, high_res_img) in enumerate(dataloader):\n",
    "            low_res_img = low_res_img.to(DEVICE)\n",
    "            high_res_img = high_res_img.to(DEVICE)\n",
    "\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            real_labels = torch.ones(low_res_img.size(0), 1, 30, 30, device=DEVICE)\n",
    "            fake_high_res_for_D = generator(low_res_img)\n",
    "            \n",
    "            output_real = discriminator(low_res_img, high_res_img)\n",
    "            loss_D_real = criterion_GAN(output_real, real_labels)\n",
    "\n",
    "            fake_labels = torch.zeros(low_res_img.size(0), 1, 30, 30, device=DEVICE)\n",
    "            output_fake = discriminator(low_res_img, fake_high_res_for_D.detach())\n",
    "            loss_D_fake = criterion_GAN(output_fake, fake_labels)\n",
    "\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "\n",
    "            for _ in range(Config.G_TRAIN_MULTIPLIER):\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                fake_high_res_img = generator(low_res_img)\n",
    "\n",
    "                output_fake_for_G = discriminator(low_res_img, fake_high_res_img)\n",
    "                loss_G_GAN = criterion_GAN(output_fake_for_G, real_labels)\n",
    "\n",
    "                loss_G_L1 = criterion_L1(fake_high_res_img, high_res_img) * Config.LAMBDA_L1\n",
    "\n",
    "                loss_G_perceptual = perceptual_loss_fn(fake_high_res_img, high_res_img) * Config.LAMBDA_PERCEPTUAL\n",
    "\n",
    "                loss_G = loss_G_GAN + loss_G_L1 + loss_G_perceptual\n",
    "                loss_G.backward() \n",
    "                optimizer_G.step()\n",
    "\n",
    "            if (i + 1) % Config.LOG_EVERY_N_BATCHES == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{Config.NUM_EPOCHS}], Batch [{i+1}/{len(dataloader)}] | \"\n",
    "                      f\"D Loss: {loss_D.item():.4f} | G Loss: {loss_G.item():.4f} \"\n",
    "                      f\"(GAN: {loss_G_GAN.item():.4f}, L1: {loss_G_L1.item():.4f}, Perceptual: {loss_G_perceptual.item():.4f})\")\n",
    "        \n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        print(f\"Epoch {epoch+1} completed. Current LR G: {optimizer_G.param_groups[0]['lr']:.6f}, LR D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "        if (epoch + 1) % Config.SAVE_EVERY_N_EPOCHS == 0 or (epoch + 1) == Config.NUM_EPOCHS:\n",
    "            print(f\"--- Entering save/checkpoint block for epoch {epoch+1} ---\")\n",
    "            print(f\"Saving samples and checkpoint for epoch {epoch+1}...\")\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    sample_low_res, sample_high_res = next(iter(dataloader))\n",
    "                    sample_low_res = sample_low_res.to(DEVICE)\n",
    "                    sample_high_res = sample_high_res.to(DEVICE)\n",
    "                    \n",
    "                    generated_high_res = generator(sample_low_res) \n",
    "                    save_images(sample_low_res, sample_high_res, generated_high_res, epoch + 1, i + 1, Config.OUTPUT_DIR)\n",
    "                except StopIteration:\n",
    "                    print(f\"Warning: Dataloader exhausted at epoch {epoch+1} for sample saving. Skipping image save for this epoch.\")\n",
    "\n",
    "\n",
    "            try:\n",
    "                torch.save(generator.state_dict(), os.path.join(Config.CHECKPOINT_DIR, f'generator_epoch_{epoch+1}.pth'))\n",
    "                torch.save(discriminator.state_dict(), os.path.join(Config.CHECKPOINT_DIR, f'discriminator_epoch_{epoch+1}.pth'))\n",
    "                print(f\"Models saved to {Config.CHECKPOINT_DIR}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model checkpoints for epoch {epoch+1}: {e}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.705118Z",
     "iopub.status.busy": "2025-06-05T08:30:53.704905Z",
     "iopub.status.idle": "2025-06-05T08:30:53.715757Z",
     "shell.execute_reply": "2025-06-05T08:30:53.715029Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.705102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Generator architecture for image-to-image translation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False) # 256 -> 128\n",
    "        self.down2 = UNetDown(64, 128) # 128 -> 64\n",
    "        self.down3 = UNetDown(128, 256) # 64 -> 32\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5) # 32 -> 16\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5) # 16 -> 8\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5) # 8 -> 4\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5) # 4 -> 2\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5) # 2 -> 1\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5) # 1 -> 2 (concat with down7)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5) # 2 -> 4 (concat with down6)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5) # 4 -> 8 (concat with down5)\n",
    "        self.up4 = UNetUp(1024, 512) # 8 -> 16 (concat with down4)\n",
    "        self.up5 = UNetUp(1024, 256) # 16 -> 32 (concat with down3)\n",
    "        self.up6 = UNetUp(512, 128) # 32 -> 64 (concat with down2)\n",
    "        self.up7 = UNetUp(256, 64) # 64 -> 128 (concat with down1)\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1), # 128 -> 256\n",
    "            nn.Tanh() # Output pixels in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final_conv(u7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.716683Z",
     "iopub.status.busy": "2025-06-05T08:30:53.716434Z",
     "iopub.status.idle": "2025-06-05T08:30:53.727347Z",
     "shell.execute_reply": "2025-06-05T08:30:53.726795Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.716663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(generator_path):\n",
    "    print(\"\\nStarting Evaluation...\")\n",
    "    generator = UnetGenerator(Config.NUM_CHANNELS, Config.NUM_CHANNELS).to(DEVICE)\n",
    "    generator.load_state_dict(torch.load(generator_path, map_location=DEVICE))\n",
    "    generator.eval()\n",
    "\n",
    "    dataset = KvasirSEGDataset(\n",
    "        root_dir=Config.DATA_ROOT,\n",
    "        image_size=Config.IMAGE_SIZE,\n",
    "        degradation_transform=lambda img_tensor: apply_degradation(img_tensor, Config)\n",
    "    )\n",
    "    # Reduced num_workers for local execution\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "    if len(eval_dataloader) == 0:\n",
    "        print(f\"Error: Evaluation Dataloader is empty. Cannot perform evaluation.\")\n",
    "        return\n",
    "\n",
    "    all_psnr = []\n",
    "    all_ssim = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (low_res_img, high_res_img) in enumerate(eval_dataloader):\n",
    "            low_res_img = low_res_img.to(DEVICE)\n",
    "            high_res_img = high_res_img.to(DEVICE)\n",
    "\n",
    "            fake_high_res_img = generator(low_res_img)\n",
    "            \n",
    "            psnr, ssim = calculate_metrics(high_res_img, fake_high_res_img)\n",
    "            all_psnr.append(psnr)\n",
    "            all_ssim.append(ssim)\n",
    "\n",
    "            if (i + 1) % Config.LOG_EVERY_N_BATCHES == 0:\n",
    "                print(f\"Eval Batch [{i+1}/{len(eval_dataloader)}] | Avg PSNR: {np.mean(all_psnr):.2f} | Avg SSIM: {np.mean(all_ssim):.4f}\")\n",
    "\n",
    "    avg_psnr = np.mean(all_psnr)\n",
    "    avg_ssim = np.mean(all_ssim)\n",
    "    print(f\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.2f}\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "    print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.730066Z",
     "iopub.status.busy": "2025-06-05T08:30:53.729680Z",
     "iopub.status.idle": "2025-06-05T11:06:32.352399Z",
     "shell.execute_reply": "2025-06-05T11:06:32.351355Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.730049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directory: output/generated_images\n",
      "Output directory 'output/generated_images' created and ready.\n",
      "Using device: cpu\n",
      "Dataset initialized with 1000 images from kvasir-seg\\Kvasir-SEG\\images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\ATHARVA KHOLLAM/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\ATHARVA KHOLLAM/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [04:05<00:00, 2.34MB/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Epoch [1/50], Batch [10/250] | D Loss: 0.7300 | G Loss: 25.0315 (GAN: 0.8251, L1: 11.4368, Perceptual: 12.7696)\n",
      "Epoch [1/50], Batch [10/250] | D Loss: 0.7300 | G Loss: 25.0315 (GAN: 0.8251, L1: 11.4368, Perceptual: 12.7696)\n",
      "Epoch [1/50], Batch [20/250] | D Loss: 0.7201 | G Loss: 20.8651 (GAN: 0.8032, L1: 9.3744, Perceptual: 10.6875)\n",
      "Epoch [1/50], Batch [20/250] | D Loss: 0.7201 | G Loss: 20.8651 (GAN: 0.8032, L1: 9.3744, Perceptual: 10.6875)\n",
      "Epoch [1/50], Batch [30/250] | D Loss: 0.7109 | G Loss: 17.9303 (GAN: 0.7601, L1: 6.8540, Perceptual: 10.3162)\n",
      "Epoch [1/50], Batch [30/250] | D Loss: 0.7109 | G Loss: 17.9303 (GAN: 0.7601, L1: 6.8540, Perceptual: 10.3162)\n",
      "Epoch [1/50], Batch [40/250] | D Loss: 0.6940 | G Loss: 15.7866 (GAN: 0.7347, L1: 5.7114, Perceptual: 9.3405)\n",
      "Epoch [1/50], Batch [40/250] | D Loss: 0.6940 | G Loss: 15.7866 (GAN: 0.7347, L1: 5.7114, Perceptual: 9.3405)\n",
      "Epoch [1/50], Batch [50/250] | D Loss: 0.7171 | G Loss: 17.4491 (GAN: 0.7770, L1: 6.5125, Perceptual: 10.1596)\n",
      "Epoch [1/50], Batch [50/250] | D Loss: 0.7171 | G Loss: 17.4491 (GAN: 0.7770, L1: 6.5125, Perceptual: 10.1596)\n",
      "Epoch [1/50], Batch [60/250] | D Loss: 0.6947 | G Loss: 15.2275 (GAN: 0.7218, L1: 5.0707, Perceptual: 9.4350)\n",
      "Epoch [1/50], Batch [60/250] | D Loss: 0.6947 | G Loss: 15.2275 (GAN: 0.7218, L1: 5.0707, Perceptual: 9.4350)\n",
      "Epoch [1/50], Batch [70/250] | D Loss: 0.6926 | G Loss: 15.9626 (GAN: 0.7976, L1: 5.5325, Perceptual: 9.6325)\n",
      "Epoch [1/50], Batch [70/250] | D Loss: 0.6926 | G Loss: 15.9626 (GAN: 0.7976, L1: 5.5325, Perceptual: 9.6325)\n",
      "Epoch [1/50], Batch [80/250] | D Loss: 0.7140 | G Loss: 14.6025 (GAN: 0.7103, L1: 4.8161, Perceptual: 9.0761)\n",
      "Epoch [1/50], Batch [80/250] | D Loss: 0.7140 | G Loss: 14.6025 (GAN: 0.7103, L1: 4.8161, Perceptual: 9.0761)\n",
      "Epoch [1/50], Batch [90/250] | D Loss: 0.7125 | G Loss: 15.8092 (GAN: 0.7687, L1: 5.1554, Perceptual: 9.8851)\n",
      "Epoch [1/50], Batch [90/250] | D Loss: 0.7125 | G Loss: 15.8092 (GAN: 0.7687, L1: 5.1554, Perceptual: 9.8851)\n",
      "Epoch [1/50], Batch [100/250] | D Loss: 0.6965 | G Loss: 14.0578 (GAN: 0.7675, L1: 4.0883, Perceptual: 9.2021)\n",
      "Epoch [1/50], Batch [100/250] | D Loss: 0.6965 | G Loss: 14.0578 (GAN: 0.7675, L1: 4.0883, Perceptual: 9.2021)\n",
      "Epoch [1/50], Batch [110/250] | D Loss: 0.7077 | G Loss: 14.8440 (GAN: 0.6755, L1: 5.2359, Perceptual: 8.9326)\n",
      "Epoch [1/50], Batch [110/250] | D Loss: 0.7077 | G Loss: 14.8440 (GAN: 0.6755, L1: 5.2359, Perceptual: 8.9326)\n",
      "Epoch [1/50], Batch [120/250] | D Loss: 0.7001 | G Loss: 16.2835 (GAN: 0.7605, L1: 5.6446, Perceptual: 9.8784)\n",
      "Epoch [1/50], Batch [120/250] | D Loss: 0.7001 | G Loss: 16.2835 (GAN: 0.7605, L1: 5.6446, Perceptual: 9.8784)\n",
      "Epoch [1/50], Batch [130/250] | D Loss: 0.6880 | G Loss: 15.0063 (GAN: 0.7470, L1: 4.9798, Perceptual: 9.2796)\n",
      "Epoch [1/50], Batch [130/250] | D Loss: 0.6880 | G Loss: 15.0063 (GAN: 0.7470, L1: 4.9798, Perceptual: 9.2796)\n",
      "Epoch [1/50], Batch [140/250] | D Loss: 0.6499 | G Loss: 15.6552 (GAN: 0.6934, L1: 5.5829, Perceptual: 9.3789)\n",
      "Epoch [1/50], Batch [140/250] | D Loss: 0.6499 | G Loss: 15.6552 (GAN: 0.6934, L1: 5.5829, Perceptual: 9.3789)\n",
      "Epoch [1/50], Batch [150/250] | D Loss: 0.6544 | G Loss: 14.2890 (GAN: 0.6970, L1: 4.8108, Perceptual: 8.7812)\n",
      "Epoch [1/50], Batch [150/250] | D Loss: 0.6544 | G Loss: 14.2890 (GAN: 0.6970, L1: 4.8108, Perceptual: 8.7812)\n",
      "Epoch [1/50], Batch [160/250] | D Loss: 0.6446 | G Loss: 14.1133 (GAN: 0.6816, L1: 4.9845, Perceptual: 8.4473)\n",
      "Epoch [1/50], Batch [160/250] | D Loss: 0.6446 | G Loss: 14.1133 (GAN: 0.6816, L1: 4.9845, Perceptual: 8.4473)\n",
      "Epoch [1/50], Batch [170/250] | D Loss: 0.6524 | G Loss: 14.3091 (GAN: 0.7524, L1: 4.9532, Perceptual: 8.6035)\n",
      "Epoch [1/50], Batch [170/250] | D Loss: 0.6524 | G Loss: 14.3091 (GAN: 0.7524, L1: 4.9532, Perceptual: 8.6035)\n",
      "Epoch [1/50], Batch [180/250] | D Loss: 0.6298 | G Loss: 15.6552 (GAN: 0.8398, L1: 6.4596, Perceptual: 8.3558)\n",
      "Epoch [1/50], Batch [180/250] | D Loss: 0.6298 | G Loss: 15.6552 (GAN: 0.8398, L1: 6.4596, Perceptual: 8.3558)\n",
      "Epoch [1/50], Batch [190/250] | D Loss: 0.6623 | G Loss: 12.9725 (GAN: 0.7968, L1: 3.9769, Perceptual: 8.1989)\n",
      "Epoch [1/50], Batch [190/250] | D Loss: 0.6623 | G Loss: 12.9725 (GAN: 0.7968, L1: 3.9769, Perceptual: 8.1989)\n",
      "Epoch [1/50], Batch [200/250] | D Loss: 0.6093 | G Loss: 15.2986 (GAN: 0.7844, L1: 5.7507, Perceptual: 8.7635)\n",
      "Epoch [1/50], Batch [200/250] | D Loss: 0.6093 | G Loss: 15.2986 (GAN: 0.7844, L1: 5.7507, Perceptual: 8.7635)\n",
      "Epoch [1/50], Batch [210/250] | D Loss: 0.6857 | G Loss: 13.9351 (GAN: 0.8602, L1: 4.1780, Perceptual: 8.8968)\n",
      "Epoch [1/50], Batch [210/250] | D Loss: 0.6857 | G Loss: 13.9351 (GAN: 0.8602, L1: 4.1780, Perceptual: 8.8968)\n",
      "Epoch [1/50], Batch [220/250] | D Loss: 0.6819 | G Loss: 13.2732 (GAN: 0.6219, L1: 4.0803, Perceptual: 8.5710)\n",
      "Epoch [1/50], Batch [220/250] | D Loss: 0.6819 | G Loss: 13.2732 (GAN: 0.6219, L1: 4.0803, Perceptual: 8.5710)\n",
      "Epoch [1/50], Batch [230/250] | D Loss: 0.7018 | G Loss: 15.1417 (GAN: 0.8503, L1: 5.4020, Perceptual: 8.8894)\n",
      "Epoch [1/50], Batch [230/250] | D Loss: 0.7018 | G Loss: 15.1417 (GAN: 0.8503, L1: 5.4020, Perceptual: 8.8894)\n",
      "Epoch [1/50], Batch [240/250] | D Loss: 0.6856 | G Loss: 12.8461 (GAN: 0.7775, L1: 4.0364, Perceptual: 8.0321)\n",
      "Epoch [1/50], Batch [240/250] | D Loss: 0.6856 | G Loss: 12.8461 (GAN: 0.7775, L1: 4.0364, Perceptual: 8.0321)\n",
      "Epoch [1/50], Batch [250/250] | D Loss: 0.6568 | G Loss: 14.0886 (GAN: 0.8987, L1: 4.4727, Perceptual: 8.7172)\n",
      "Epoch 1 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [1/50], Batch [250/250] | D Loss: 0.6568 | G Loss: 14.0886 (GAN: 0.8987, L1: 4.4727, Perceptual: 8.7172)\n",
      "Epoch 1 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [2/50], Batch [10/250] | D Loss: 0.6518 | G Loss: 13.2938 (GAN: 0.8863, L1: 4.2097, Perceptual: 8.1978)\n",
      "Epoch [2/50], Batch [10/250] | D Loss: 0.6518 | G Loss: 13.2938 (GAN: 0.8863, L1: 4.2097, Perceptual: 8.1978)\n",
      "Epoch [2/50], Batch [20/250] | D Loss: 0.6295 | G Loss: 14.3540 (GAN: 0.8136, L1: 4.5165, Perceptual: 9.0239)\n",
      "Epoch [2/50], Batch [20/250] | D Loss: 0.6295 | G Loss: 14.3540 (GAN: 0.8136, L1: 4.5165, Perceptual: 9.0239)\n",
      "Epoch [2/50], Batch [30/250] | D Loss: 0.5912 | G Loss: 14.9630 (GAN: 0.8610, L1: 5.1595, Perceptual: 8.9425)\n",
      "Epoch [2/50], Batch [30/250] | D Loss: 0.5912 | G Loss: 14.9630 (GAN: 0.8610, L1: 5.1595, Perceptual: 8.9425)\n",
      "Epoch [2/50], Batch [40/250] | D Loss: 0.6780 | G Loss: 13.6389 (GAN: 1.1015, L1: 4.1416, Perceptual: 8.3958)\n",
      "Epoch [2/50], Batch [40/250] | D Loss: 0.6780 | G Loss: 13.6389 (GAN: 1.1015, L1: 4.1416, Perceptual: 8.3958)\n",
      "Epoch [2/50], Batch [50/250] | D Loss: 0.6360 | G Loss: 13.7295 (GAN: 0.8635, L1: 4.6158, Perceptual: 8.2502)\n",
      "Epoch [2/50], Batch [50/250] | D Loss: 0.6360 | G Loss: 13.7295 (GAN: 0.8635, L1: 4.6158, Perceptual: 8.2502)\n",
      "Epoch [2/50], Batch [60/250] | D Loss: 0.6668 | G Loss: 12.6702 (GAN: 0.6864, L1: 3.9707, Perceptual: 8.0131)\n",
      "Epoch [2/50], Batch [60/250] | D Loss: 0.6668 | G Loss: 12.6702 (GAN: 0.6864, L1: 3.9707, Perceptual: 8.0131)\n",
      "Epoch [2/50], Batch [70/250] | D Loss: 0.5525 | G Loss: 13.4044 (GAN: 1.1995, L1: 4.2684, Perceptual: 7.9365)\n",
      "Epoch [2/50], Batch [70/250] | D Loss: 0.5525 | G Loss: 13.4044 (GAN: 1.1995, L1: 4.2684, Perceptual: 7.9365)\n",
      "Epoch [2/50], Batch [80/250] | D Loss: 0.6463 | G Loss: 15.0659 (GAN: 0.9862, L1: 5.3949, Perceptual: 8.6849)\n",
      "Epoch [2/50], Batch [80/250] | D Loss: 0.6463 | G Loss: 15.0659 (GAN: 0.9862, L1: 5.3949, Perceptual: 8.6849)\n",
      "Epoch [2/50], Batch [90/250] | D Loss: 0.6072 | G Loss: 13.0509 (GAN: 0.8136, L1: 4.2181, Perceptual: 8.0192)\n",
      "Epoch [2/50], Batch [90/250] | D Loss: 0.6072 | G Loss: 13.0509 (GAN: 0.8136, L1: 4.2181, Perceptual: 8.0192)\n",
      "Epoch [2/50], Batch [100/250] | D Loss: 0.7452 | G Loss: 13.0272 (GAN: 0.9141, L1: 4.2206, Perceptual: 7.8925)\n",
      "Epoch [2/50], Batch [100/250] | D Loss: 0.7452 | G Loss: 13.0272 (GAN: 0.9141, L1: 4.2206, Perceptual: 7.8925)\n",
      "Epoch [2/50], Batch [110/250] | D Loss: 0.6730 | G Loss: 13.6200 (GAN: 0.8793, L1: 4.2038, Perceptual: 8.5369)\n",
      "Epoch [2/50], Batch [110/250] | D Loss: 0.6730 | G Loss: 13.6200 (GAN: 0.8793, L1: 4.2038, Perceptual: 8.5369)\n",
      "Epoch [2/50], Batch [120/250] | D Loss: 0.6280 | G Loss: 13.5887 (GAN: 0.8922, L1: 4.4706, Perceptual: 8.2259)\n",
      "Epoch [2/50], Batch [120/250] | D Loss: 0.6280 | G Loss: 13.5887 (GAN: 0.8922, L1: 4.4706, Perceptual: 8.2259)\n",
      "Epoch [2/50], Batch [130/250] | D Loss: 0.7125 | G Loss: 12.3374 (GAN: 0.7279, L1: 3.8285, Perceptual: 7.7810)\n",
      "Epoch [2/50], Batch [130/250] | D Loss: 0.7125 | G Loss: 12.3374 (GAN: 0.7279, L1: 3.8285, Perceptual: 7.7810)\n",
      "Epoch [2/50], Batch [140/250] | D Loss: 0.6901 | G Loss: 14.3682 (GAN: 0.7933, L1: 4.9271, Perceptual: 8.6479)\n",
      "Epoch [2/50], Batch [140/250] | D Loss: 0.6901 | G Loss: 14.3682 (GAN: 0.7933, L1: 4.9271, Perceptual: 8.6479)\n",
      "Epoch [2/50], Batch [150/250] | D Loss: 0.5748 | G Loss: 12.3442 (GAN: 0.6705, L1: 3.9027, Perceptual: 7.7710)\n",
      "Epoch [2/50], Batch [150/250] | D Loss: 0.5748 | G Loss: 12.3442 (GAN: 0.6705, L1: 3.9027, Perceptual: 7.7710)\n",
      "Epoch [2/50], Batch [160/250] | D Loss: 0.6132 | G Loss: 13.0541 (GAN: 0.7507, L1: 3.9046, Perceptual: 8.3988)\n",
      "Epoch [2/50], Batch [160/250] | D Loss: 0.6132 | G Loss: 13.0541 (GAN: 0.7507, L1: 3.9046, Perceptual: 8.3988)\n",
      "Epoch [2/50], Batch [170/250] | D Loss: 0.4390 | G Loss: 12.4615 (GAN: 0.4311, L1: 3.9405, Perceptual: 8.0899)\n",
      "Epoch [2/50], Batch [170/250] | D Loss: 0.4390 | G Loss: 12.4615 (GAN: 0.4311, L1: 3.9405, Perceptual: 8.0899)\n",
      "Epoch [2/50], Batch [180/250] | D Loss: 0.7715 | G Loss: 14.1050 (GAN: 0.7624, L1: 4.5937, Perceptual: 8.7489)\n",
      "Epoch [2/50], Batch [180/250] | D Loss: 0.7715 | G Loss: 14.1050 (GAN: 0.7624, L1: 4.5937, Perceptual: 8.7489)\n",
      "Epoch [2/50], Batch [190/250] | D Loss: 0.7476 | G Loss: 12.7472 (GAN: 0.6964, L1: 4.1525, Perceptual: 7.8983)\n",
      "Epoch [2/50], Batch [190/250] | D Loss: 0.7476 | G Loss: 12.7472 (GAN: 0.6964, L1: 4.1525, Perceptual: 7.8983)\n",
      "Epoch [2/50], Batch [200/250] | D Loss: 0.7966 | G Loss: 14.2666 (GAN: 1.0593, L1: 5.0145, Perceptual: 8.1928)\n",
      "Epoch [2/50], Batch [200/250] | D Loss: 0.7966 | G Loss: 14.2666 (GAN: 1.0593, L1: 5.0145, Perceptual: 8.1928)\n",
      "Epoch [2/50], Batch [210/250] | D Loss: 0.5103 | G Loss: 14.6381 (GAN: 1.0601, L1: 5.1197, Perceptual: 8.4583)\n",
      "Epoch [2/50], Batch [210/250] | D Loss: 0.5103 | G Loss: 14.6381 (GAN: 1.0601, L1: 5.1197, Perceptual: 8.4583)\n",
      "Epoch [2/50], Batch [220/250] | D Loss: 0.6419 | G Loss: 14.7404 (GAN: 0.9046, L1: 5.5560, Perceptual: 8.2798)\n",
      "Epoch [2/50], Batch [220/250] | D Loss: 0.6419 | G Loss: 14.7404 (GAN: 0.9046, L1: 5.5560, Perceptual: 8.2798)\n",
      "Epoch [2/50], Batch [230/250] | D Loss: 0.7501 | G Loss: 13.4271 (GAN: 0.8667, L1: 4.5666, Perceptual: 7.9938)\n",
      "Epoch [2/50], Batch [230/250] | D Loss: 0.7501 | G Loss: 13.4271 (GAN: 0.8667, L1: 4.5666, Perceptual: 7.9938)\n",
      "Epoch [2/50], Batch [240/250] | D Loss: 0.5910 | G Loss: 12.7224 (GAN: 0.8014, L1: 4.0708, Perceptual: 7.8502)\n",
      "Epoch [2/50], Batch [240/250] | D Loss: 0.5910 | G Loss: 12.7224 (GAN: 0.8014, L1: 4.0708, Perceptual: 7.8502)\n",
      "Epoch [2/50], Batch [250/250] | D Loss: 0.7739 | G Loss: 13.2471 (GAN: 0.9030, L1: 4.2903, Perceptual: 8.0538)\n",
      "Epoch 2 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [2/50], Batch [250/250] | D Loss: 0.7739 | G Loss: 13.2471 (GAN: 0.9030, L1: 4.2903, Perceptual: 8.0538)\n",
      "Epoch 2 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [3/50], Batch [10/250] | D Loss: 0.6380 | G Loss: 14.3132 (GAN: 1.1288, L1: 4.8873, Perceptual: 8.2971)\n",
      "Epoch [3/50], Batch [10/250] | D Loss: 0.6380 | G Loss: 14.3132 (GAN: 1.1288, L1: 4.8873, Perceptual: 8.2971)\n",
      "Epoch [3/50], Batch [20/250] | D Loss: 0.6365 | G Loss: 12.8728 (GAN: 0.5789, L1: 4.0442, Perceptual: 8.2497)\n",
      "Epoch [3/50], Batch [20/250] | D Loss: 0.6365 | G Loss: 12.8728 (GAN: 0.5789, L1: 4.0442, Perceptual: 8.2497)\n",
      "Epoch [3/50], Batch [30/250] | D Loss: 0.8131 | G Loss: 14.8717 (GAN: 0.7477, L1: 5.1998, Perceptual: 8.9242)\n",
      "Epoch [3/50], Batch [30/250] | D Loss: 0.8131 | G Loss: 14.8717 (GAN: 0.7477, L1: 5.1998, Perceptual: 8.9242)\n",
      "Epoch [3/50], Batch [40/250] | D Loss: 0.5868 | G Loss: 14.2924 (GAN: 0.8812, L1: 4.3762, Perceptual: 9.0351)\n",
      "Epoch [3/50], Batch [40/250] | D Loss: 0.5868 | G Loss: 14.2924 (GAN: 0.8812, L1: 4.3762, Perceptual: 9.0351)\n",
      "Epoch [3/50], Batch [50/250] | D Loss: 0.5501 | G Loss: 14.7033 (GAN: 1.0046, L1: 5.0776, Perceptual: 8.6211)\n",
      "Epoch [3/50], Batch [50/250] | D Loss: 0.5501 | G Loss: 14.7033 (GAN: 1.0046, L1: 5.0776, Perceptual: 8.6211)\n",
      "Epoch [3/50], Batch [60/250] | D Loss: 0.5954 | G Loss: 13.5050 (GAN: 0.8049, L1: 4.5978, Perceptual: 8.1023)\n",
      "Epoch [3/50], Batch [60/250] | D Loss: 0.5954 | G Loss: 13.5050 (GAN: 0.8049, L1: 4.5978, Perceptual: 8.1023)\n",
      "Epoch [3/50], Batch [70/250] | D Loss: 0.5030 | G Loss: 14.1127 (GAN: 0.7199, L1: 5.1240, Perceptual: 8.2688)\n",
      "Epoch [3/50], Batch [70/250] | D Loss: 0.5030 | G Loss: 14.1127 (GAN: 0.7199, L1: 5.1240, Perceptual: 8.2688)\n",
      "Epoch [3/50], Batch [80/250] | D Loss: 0.6384 | G Loss: 12.7791 (GAN: 0.8329, L1: 4.2209, Perceptual: 7.7253)\n",
      "Epoch [3/50], Batch [80/250] | D Loss: 0.6384 | G Loss: 12.7791 (GAN: 0.8329, L1: 4.2209, Perceptual: 7.7253)\n",
      "Epoch [3/50], Batch [90/250] | D Loss: 0.5239 | G Loss: 15.2765 (GAN: 1.3221, L1: 4.7843, Perceptual: 9.1701)\n",
      "Epoch [3/50], Batch [90/250] | D Loss: 0.5239 | G Loss: 15.2765 (GAN: 1.3221, L1: 4.7843, Perceptual: 9.1701)\n",
      "Epoch [3/50], Batch [100/250] | D Loss: 0.5417 | G Loss: 14.5248 (GAN: 0.7228, L1: 4.9539, Perceptual: 8.8481)\n",
      "Epoch [3/50], Batch [100/250] | D Loss: 0.5417 | G Loss: 14.5248 (GAN: 0.7228, L1: 4.9539, Perceptual: 8.8481)\n",
      "Epoch [3/50], Batch [110/250] | D Loss: 0.5682 | G Loss: 13.1176 (GAN: 1.2440, L1: 4.0731, Perceptual: 7.8006)\n",
      "Epoch [3/50], Batch [110/250] | D Loss: 0.5682 | G Loss: 13.1176 (GAN: 1.2440, L1: 4.0731, Perceptual: 7.8006)\n",
      "Epoch [3/50], Batch [120/250] | D Loss: 0.5088 | G Loss: 13.7661 (GAN: 1.0054, L1: 4.3204, Perceptual: 8.4403)\n",
      "Epoch [3/50], Batch [120/250] | D Loss: 0.5088 | G Loss: 13.7661 (GAN: 1.0054, L1: 4.3204, Perceptual: 8.4403)\n",
      "Epoch [3/50], Batch [130/250] | D Loss: 0.6915 | G Loss: 14.0036 (GAN: 0.8457, L1: 4.4835, Perceptual: 8.6745)\n",
      "Epoch [3/50], Batch [130/250] | D Loss: 0.6915 | G Loss: 14.0036 (GAN: 0.8457, L1: 4.4835, Perceptual: 8.6745)\n",
      "Epoch [3/50], Batch [140/250] | D Loss: 0.7789 | G Loss: 14.0778 (GAN: 0.8144, L1: 4.9674, Perceptual: 8.2960)\n",
      "Epoch [3/50], Batch [140/250] | D Loss: 0.7789 | G Loss: 14.0778 (GAN: 0.8144, L1: 4.9674, Perceptual: 8.2960)\n",
      "Epoch [3/50], Batch [150/250] | D Loss: 0.4895 | G Loss: 13.6905 (GAN: 0.8231, L1: 4.4343, Perceptual: 8.4331)\n",
      "Epoch [3/50], Batch [150/250] | D Loss: 0.4895 | G Loss: 13.6905 (GAN: 0.8231, L1: 4.4343, Perceptual: 8.4331)\n",
      "Epoch [3/50], Batch [160/250] | D Loss: 0.5858 | G Loss: 14.5656 (GAN: 1.0222, L1: 4.9299, Perceptual: 8.6135)\n",
      "Epoch [3/50], Batch [160/250] | D Loss: 0.5858 | G Loss: 14.5656 (GAN: 1.0222, L1: 4.9299, Perceptual: 8.6135)\n",
      "Epoch [3/50], Batch [170/250] | D Loss: 0.5955 | G Loss: 12.4932 (GAN: 0.4917, L1: 3.8774, Perceptual: 8.1241)\n",
      "Epoch [3/50], Batch [170/250] | D Loss: 0.5955 | G Loss: 12.4932 (GAN: 0.4917, L1: 3.8774, Perceptual: 8.1241)\n",
      "Epoch [3/50], Batch [180/250] | D Loss: 0.6562 | G Loss: 12.3866 (GAN: 0.6287, L1: 4.0169, Perceptual: 7.7410)\n",
      "Epoch [3/50], Batch [180/250] | D Loss: 0.6562 | G Loss: 12.3866 (GAN: 0.6287, L1: 4.0169, Perceptual: 7.7410)\n",
      "Epoch [3/50], Batch [190/250] | D Loss: 0.6898 | G Loss: 13.4329 (GAN: 0.9045, L1: 3.9512, Perceptual: 8.5772)\n",
      "Epoch [3/50], Batch [190/250] | D Loss: 0.6898 | G Loss: 13.4329 (GAN: 0.9045, L1: 3.9512, Perceptual: 8.5772)\n",
      "Epoch [3/50], Batch [200/250] | D Loss: 0.7167 | G Loss: 13.2830 (GAN: 0.9670, L1: 4.2690, Perceptual: 8.0470)\n",
      "Epoch [3/50], Batch [200/250] | D Loss: 0.7167 | G Loss: 13.2830 (GAN: 0.9670, L1: 4.2690, Perceptual: 8.0470)\n",
      "Epoch [3/50], Batch [210/250] | D Loss: 0.5724 | G Loss: 12.4061 (GAN: 0.7904, L1: 4.0865, Perceptual: 7.5292)\n",
      "Epoch [3/50], Batch [210/250] | D Loss: 0.5724 | G Loss: 12.4061 (GAN: 0.7904, L1: 4.0865, Perceptual: 7.5292)\n",
      "Epoch [3/50], Batch [220/250] | D Loss: 0.4670 | G Loss: 13.7399 (GAN: 1.0024, L1: 4.3627, Perceptual: 8.3749)\n",
      "Epoch [3/50], Batch [220/250] | D Loss: 0.4670 | G Loss: 13.7399 (GAN: 1.0024, L1: 4.3627, Perceptual: 8.3749)\n",
      "Epoch [3/50], Batch [230/250] | D Loss: 0.5982 | G Loss: 13.0417 (GAN: 0.8789, L1: 4.8281, Perceptual: 7.3347)\n",
      "Epoch [3/50], Batch [230/250] | D Loss: 0.5982 | G Loss: 13.0417 (GAN: 0.8789, L1: 4.8281, Perceptual: 7.3347)\n",
      "Epoch [3/50], Batch [240/250] | D Loss: 0.6188 | G Loss: 12.1339 (GAN: 0.7835, L1: 3.9639, Perceptual: 7.3865)\n",
      "Epoch [3/50], Batch [240/250] | D Loss: 0.6188 | G Loss: 12.1339 (GAN: 0.7835, L1: 3.9639, Perceptual: 7.3865)\n",
      "Epoch [3/50], Batch [250/250] | D Loss: 1.0339 | G Loss: 13.4592 (GAN: 0.8476, L1: 4.6246, Perceptual: 7.9870)\n",
      "Epoch 3 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [3/50], Batch [250/250] | D Loss: 1.0339 | G Loss: 13.4592 (GAN: 0.8476, L1: 4.6246, Perceptual: 7.9870)\n",
      "Epoch 3 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [4/50], Batch [10/250] | D Loss: 0.6235 | G Loss: 12.3018 (GAN: 0.9103, L1: 3.7812, Perceptual: 7.6103)\n",
      "Epoch [4/50], Batch [10/250] | D Loss: 0.6235 | G Loss: 12.3018 (GAN: 0.9103, L1: 3.7812, Perceptual: 7.6103)\n",
      "Epoch [4/50], Batch [20/250] | D Loss: 0.6161 | G Loss: 12.9800 (GAN: 0.7746, L1: 4.7083, Perceptual: 7.4972)\n",
      "Epoch [4/50], Batch [20/250] | D Loss: 0.6161 | G Loss: 12.9800 (GAN: 0.7746, L1: 4.7083, Perceptual: 7.4972)\n",
      "Epoch [4/50], Batch [30/250] | D Loss: 0.4725 | G Loss: 15.3142 (GAN: 1.1632, L1: 4.9915, Perceptual: 9.1595)\n",
      "Epoch [4/50], Batch [30/250] | D Loss: 0.4725 | G Loss: 15.3142 (GAN: 1.1632, L1: 4.9915, Perceptual: 9.1595)\n",
      "Epoch [4/50], Batch [40/250] | D Loss: 0.7584 | G Loss: 11.7417 (GAN: 0.6944, L1: 3.9457, Perceptual: 7.1016)\n",
      "Epoch [4/50], Batch [40/250] | D Loss: 0.7584 | G Loss: 11.7417 (GAN: 0.6944, L1: 3.9457, Perceptual: 7.1016)\n",
      "Epoch [4/50], Batch [50/250] | D Loss: 0.4976 | G Loss: 14.5760 (GAN: 0.9127, L1: 4.6226, Perceptual: 9.0407)\n",
      "Epoch [4/50], Batch [50/250] | D Loss: 0.4976 | G Loss: 14.5760 (GAN: 0.9127, L1: 4.6226, Perceptual: 9.0407)\n",
      "Epoch [4/50], Batch [60/250] | D Loss: 0.5101 | G Loss: 13.0720 (GAN: 0.9239, L1: 4.1529, Perceptual: 7.9952)\n",
      "Epoch [4/50], Batch [60/250] | D Loss: 0.5101 | G Loss: 13.0720 (GAN: 0.9239, L1: 4.1529, Perceptual: 7.9952)\n",
      "Epoch [4/50], Batch [70/250] | D Loss: 0.5865 | G Loss: 12.4443 (GAN: 0.6220, L1: 4.2432, Perceptual: 7.5790)\n",
      "Epoch [4/50], Batch [70/250] | D Loss: 0.5865 | G Loss: 12.4443 (GAN: 0.6220, L1: 4.2432, Perceptual: 7.5790)\n",
      "Epoch [4/50], Batch [80/250] | D Loss: 0.6212 | G Loss: 11.9412 (GAN: 0.7400, L1: 3.6383, Perceptual: 7.5629)\n",
      "Epoch [4/50], Batch [80/250] | D Loss: 0.6212 | G Loss: 11.9412 (GAN: 0.7400, L1: 3.6383, Perceptual: 7.5629)\n",
      "Epoch [4/50], Batch [90/250] | D Loss: 0.7971 | G Loss: 12.8495 (GAN: 0.7897, L1: 4.3357, Perceptual: 7.7241)\n",
      "Epoch [4/50], Batch [90/250] | D Loss: 0.7971 | G Loss: 12.8495 (GAN: 0.7897, L1: 4.3357, Perceptual: 7.7241)\n",
      "Epoch [4/50], Batch [100/250] | D Loss: 0.6458 | G Loss: 13.8241 (GAN: 1.2878, L1: 4.4671, Perceptual: 8.0692)\n",
      "Epoch [4/50], Batch [100/250] | D Loss: 0.6458 | G Loss: 13.8241 (GAN: 1.2878, L1: 4.4671, Perceptual: 8.0692)\n",
      "Epoch [4/50], Batch [110/250] | D Loss: 0.6368 | G Loss: 12.8574 (GAN: 1.0715, L1: 3.9448, Perceptual: 7.8410)\n",
      "Epoch [4/50], Batch [110/250] | D Loss: 0.6368 | G Loss: 12.8574 (GAN: 1.0715, L1: 3.9448, Perceptual: 7.8410)\n",
      "Epoch [4/50], Batch [120/250] | D Loss: 0.5149 | G Loss: 12.9603 (GAN: 0.3826, L1: 4.2589, Perceptual: 8.3188)\n",
      "Epoch [4/50], Batch [120/250] | D Loss: 0.5149 | G Loss: 12.9603 (GAN: 0.3826, L1: 4.2589, Perceptual: 8.3188)\n",
      "Epoch [4/50], Batch [130/250] | D Loss: 0.4994 | G Loss: 14.8845 (GAN: 1.0032, L1: 5.3461, Perceptual: 8.5352)\n",
      "Epoch [4/50], Batch [130/250] | D Loss: 0.4994 | G Loss: 14.8845 (GAN: 1.0032, L1: 5.3461, Perceptual: 8.5352)\n",
      "Epoch [4/50], Batch [140/250] | D Loss: 0.5143 | G Loss: 12.7866 (GAN: 1.0166, L1: 3.7005, Perceptual: 8.0695)\n",
      "Epoch [4/50], Batch [140/250] | D Loss: 0.5143 | G Loss: 12.7866 (GAN: 1.0166, L1: 3.7005, Perceptual: 8.0695)\n",
      "Epoch [4/50], Batch [150/250] | D Loss: 0.5637 | G Loss: 13.6962 (GAN: 1.1349, L1: 4.4182, Perceptual: 8.1431)\n",
      "Epoch [4/50], Batch [150/250] | D Loss: 0.5637 | G Loss: 13.6962 (GAN: 1.1349, L1: 4.4182, Perceptual: 8.1431)\n",
      "Epoch [4/50], Batch [160/250] | D Loss: 0.4371 | G Loss: 15.9381 (GAN: 1.4628, L1: 5.3051, Perceptual: 9.1701)\n",
      "Epoch [4/50], Batch [160/250] | D Loss: 0.4371 | G Loss: 15.9381 (GAN: 1.4628, L1: 5.3051, Perceptual: 9.1701)\n",
      "Epoch [4/50], Batch [170/250] | D Loss: 0.8012 | G Loss: 13.1862 (GAN: 0.9472, L1: 4.4124, Perceptual: 7.8266)\n",
      "Epoch [4/50], Batch [170/250] | D Loss: 0.8012 | G Loss: 13.1862 (GAN: 0.9472, L1: 4.4124, Perceptual: 7.8266)\n",
      "Epoch [4/50], Batch [180/250] | D Loss: 0.4974 | G Loss: 14.3617 (GAN: 1.0790, L1: 4.5333, Perceptual: 8.7493)\n",
      "Epoch [4/50], Batch [180/250] | D Loss: 0.4974 | G Loss: 14.3617 (GAN: 1.0790, L1: 4.5333, Perceptual: 8.7493)\n",
      "Epoch [4/50], Batch [190/250] | D Loss: 0.5049 | G Loss: 14.2637 (GAN: 0.9529, L1: 4.8190, Perceptual: 8.4918)\n",
      "Epoch [4/50], Batch [190/250] | D Loss: 0.5049 | G Loss: 14.2637 (GAN: 0.9529, L1: 4.8190, Perceptual: 8.4918)\n",
      "Epoch [4/50], Batch [200/250] | D Loss: 0.6577 | G Loss: 12.3246 (GAN: 0.7413, L1: 3.7496, Perceptual: 7.8338)\n",
      "Epoch [4/50], Batch [200/250] | D Loss: 0.6577 | G Loss: 12.3246 (GAN: 0.7413, L1: 3.7496, Perceptual: 7.8338)\n",
      "Epoch [4/50], Batch [210/250] | D Loss: 0.7955 | G Loss: 11.2235 (GAN: 0.5730, L1: 3.2889, Perceptual: 7.3615)\n",
      "Epoch [4/50], Batch [210/250] | D Loss: 0.7955 | G Loss: 11.2235 (GAN: 0.5730, L1: 3.2889, Perceptual: 7.3615)\n",
      "Epoch [4/50], Batch [220/250] | D Loss: 0.4861 | G Loss: 14.0055 (GAN: 0.9153, L1: 4.6900, Perceptual: 8.4002)\n",
      "Epoch [4/50], Batch [220/250] | D Loss: 0.4861 | G Loss: 14.0055 (GAN: 0.9153, L1: 4.6900, Perceptual: 8.4002)\n",
      "Epoch [4/50], Batch [230/250] | D Loss: 0.6209 | G Loss: 13.2905 (GAN: 0.7182, L1: 4.6797, Perceptual: 7.8926)\n",
      "Epoch [4/50], Batch [230/250] | D Loss: 0.6209 | G Loss: 13.2905 (GAN: 0.7182, L1: 4.6797, Perceptual: 7.8926)\n",
      "Epoch [4/50], Batch [240/250] | D Loss: 0.7374 | G Loss: 13.3101 (GAN: 0.9374, L1: 4.6379, Perceptual: 7.7349)\n",
      "Epoch [4/50], Batch [240/250] | D Loss: 0.7374 | G Loss: 13.3101 (GAN: 0.9374, L1: 4.6379, Perceptual: 7.7349)\n",
      "Epoch [4/50], Batch [250/250] | D Loss: 0.5061 | G Loss: 14.1593 (GAN: 0.9955, L1: 4.6767, Perceptual: 8.4871)\n",
      "Epoch 4 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [4/50], Batch [250/250] | D Loss: 0.5061 | G Loss: 14.1593 (GAN: 0.9955, L1: 4.6767, Perceptual: 8.4871)\n",
      "Epoch 4 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [5/50], Batch [10/250] | D Loss: 0.8485 | G Loss: 11.6405 (GAN: 0.8607, L1: 3.6264, Perceptual: 7.1534)\n",
      "Epoch [5/50], Batch [10/250] | D Loss: 0.8485 | G Loss: 11.6405 (GAN: 0.8607, L1: 3.6264, Perceptual: 7.1534)\n",
      "Epoch [5/50], Batch [20/250] | D Loss: 0.4856 | G Loss: 12.7162 (GAN: 0.8352, L1: 3.8532, Perceptual: 8.0278)\n",
      "Epoch [5/50], Batch [20/250] | D Loss: 0.4856 | G Loss: 12.7162 (GAN: 0.8352, L1: 3.8532, Perceptual: 8.0278)\n",
      "Epoch [5/50], Batch [30/250] | D Loss: 0.5788 | G Loss: 13.1849 (GAN: 0.8071, L1: 4.1302, Perceptual: 8.2476)\n",
      "Epoch [5/50], Batch [30/250] | D Loss: 0.5788 | G Loss: 13.1849 (GAN: 0.8071, L1: 4.1302, Perceptual: 8.2476)\n",
      "Epoch [5/50], Batch [40/250] | D Loss: 0.6735 | G Loss: 12.7499 (GAN: 0.7729, L1: 4.0411, Perceptual: 7.9359)\n",
      "Epoch [5/50], Batch [40/250] | D Loss: 0.6735 | G Loss: 12.7499 (GAN: 0.7729, L1: 4.0411, Perceptual: 7.9359)\n",
      "Epoch [5/50], Batch [50/250] | D Loss: 0.6159 | G Loss: 13.3248 (GAN: 0.9208, L1: 4.2615, Perceptual: 8.1425)\n",
      "Epoch [5/50], Batch [50/250] | D Loss: 0.6159 | G Loss: 13.3248 (GAN: 0.9208, L1: 4.2615, Perceptual: 8.1425)\n",
      "Epoch [5/50], Batch [60/250] | D Loss: 0.5418 | G Loss: 14.0876 (GAN: 0.8481, L1: 4.7654, Perceptual: 8.4742)\n",
      "Epoch [5/50], Batch [60/250] | D Loss: 0.5418 | G Loss: 14.0876 (GAN: 0.8481, L1: 4.7654, Perceptual: 8.4742)\n",
      "Epoch [5/50], Batch [70/250] | D Loss: 0.4592 | G Loss: 13.4754 (GAN: 0.9571, L1: 4.4039, Perceptual: 8.1145)\n",
      "Epoch [5/50], Batch [70/250] | D Loss: 0.4592 | G Loss: 13.4754 (GAN: 0.9571, L1: 4.4039, Perceptual: 8.1145)\n",
      "Epoch [5/50], Batch [80/250] | D Loss: 0.6230 | G Loss: 12.2492 (GAN: 0.9141, L1: 3.7181, Perceptual: 7.6169)\n",
      "Epoch [5/50], Batch [80/250] | D Loss: 0.6230 | G Loss: 12.2492 (GAN: 0.9141, L1: 3.7181, Perceptual: 7.6169)\n",
      "Epoch [5/50], Batch [90/250] | D Loss: 0.6231 | G Loss: 12.9485 (GAN: 0.9197, L1: 3.8461, Perceptual: 8.1828)\n",
      "Epoch [5/50], Batch [90/250] | D Loss: 0.6231 | G Loss: 12.9485 (GAN: 0.9197, L1: 3.8461, Perceptual: 8.1828)\n",
      "Epoch [5/50], Batch [100/250] | D Loss: 0.8023 | G Loss: 12.7888 (GAN: 0.9381, L1: 3.8512, Perceptual: 7.9995)\n",
      "Epoch [5/50], Batch [100/250] | D Loss: 0.8023 | G Loss: 12.7888 (GAN: 0.9381, L1: 3.8512, Perceptual: 7.9995)\n",
      "Epoch [5/50], Batch [110/250] | D Loss: 0.7883 | G Loss: 12.9618 (GAN: 1.2693, L1: 3.9788, Perceptual: 7.7137)\n",
      "Epoch [5/50], Batch [110/250] | D Loss: 0.7883 | G Loss: 12.9618 (GAN: 1.2693, L1: 3.9788, Perceptual: 7.7137)\n",
      "Epoch [5/50], Batch [120/250] | D Loss: 0.6671 | G Loss: 12.8491 (GAN: 0.7670, L1: 3.9804, Perceptual: 8.1017)\n",
      "Epoch [5/50], Batch [120/250] | D Loss: 0.6671 | G Loss: 12.8491 (GAN: 0.7670, L1: 3.9804, Perceptual: 8.1017)\n",
      "Epoch [5/50], Batch [130/250] | D Loss: 0.5773 | G Loss: 13.1099 (GAN: 0.8425, L1: 4.0700, Perceptual: 8.1974)\n",
      "Epoch [5/50], Batch [130/250] | D Loss: 0.5773 | G Loss: 13.1099 (GAN: 0.8425, L1: 4.0700, Perceptual: 8.1974)\n",
      "Epoch [5/50], Batch [140/250] | D Loss: 0.7657 | G Loss: 12.0333 (GAN: 0.7237, L1: 3.7418, Perceptual: 7.5678)\n",
      "Epoch [5/50], Batch [140/250] | D Loss: 0.7657 | G Loss: 12.0333 (GAN: 0.7237, L1: 3.7418, Perceptual: 7.5678)\n",
      "Epoch [5/50], Batch [150/250] | D Loss: 0.8078 | G Loss: 11.6162 (GAN: 0.8738, L1: 3.5520, Perceptual: 7.1903)\n",
      "Epoch [5/50], Batch [150/250] | D Loss: 0.8078 | G Loss: 11.6162 (GAN: 0.8738, L1: 3.5520, Perceptual: 7.1903)\n",
      "Epoch [5/50], Batch [160/250] | D Loss: 0.5432 | G Loss: 12.5853 (GAN: 0.8600, L1: 4.1616, Perceptual: 7.5636)\n",
      "Epoch [5/50], Batch [160/250] | D Loss: 0.5432 | G Loss: 12.5853 (GAN: 0.8600, L1: 4.1616, Perceptual: 7.5636)\n",
      "Epoch [5/50], Batch [170/250] | D Loss: 0.6566 | G Loss: 11.7112 (GAN: 0.6081, L1: 3.8476, Perceptual: 7.2556)\n",
      "Epoch [5/50], Batch [170/250] | D Loss: 0.6566 | G Loss: 11.7112 (GAN: 0.6081, L1: 3.8476, Perceptual: 7.2556)\n",
      "Epoch [5/50], Batch [180/250] | D Loss: 0.4005 | G Loss: 13.9459 (GAN: 0.8402, L1: 4.4681, Perceptual: 8.6377)\n",
      "Epoch [5/50], Batch [180/250] | D Loss: 0.4005 | G Loss: 13.9459 (GAN: 0.8402, L1: 4.4681, Perceptual: 8.6377)\n",
      "Epoch [5/50], Batch [190/250] | D Loss: 0.4497 | G Loss: 12.4569 (GAN: 0.7915, L1: 3.8527, Perceptual: 7.8128)\n",
      "Epoch [5/50], Batch [190/250] | D Loss: 0.4497 | G Loss: 12.4569 (GAN: 0.7915, L1: 3.8527, Perceptual: 7.8128)\n",
      "Epoch [5/50], Batch [200/250] | D Loss: 0.6706 | G Loss: 13.4545 (GAN: 0.9616, L1: 4.4903, Perceptual: 8.0026)\n",
      "Epoch [5/50], Batch [200/250] | D Loss: 0.6706 | G Loss: 13.4545 (GAN: 0.9616, L1: 4.4903, Perceptual: 8.0026)\n",
      "Epoch [5/50], Batch [210/250] | D Loss: 0.8522 | G Loss: 12.0755 (GAN: 0.9000, L1: 3.7373, Perceptual: 7.4382)\n",
      "Epoch [5/50], Batch [210/250] | D Loss: 0.8522 | G Loss: 12.0755 (GAN: 0.9000, L1: 3.7373, Perceptual: 7.4382)\n",
      "Epoch [5/50], Batch [220/250] | D Loss: 0.7408 | G Loss: 11.3845 (GAN: 0.5768, L1: 3.5164, Perceptual: 7.2912)\n",
      "Epoch [5/50], Batch [220/250] | D Loss: 0.7408 | G Loss: 11.3845 (GAN: 0.5768, L1: 3.5164, Perceptual: 7.2912)\n",
      "Epoch [5/50], Batch [230/250] | D Loss: 0.7004 | G Loss: 12.2995 (GAN: 0.6978, L1: 3.6391, Perceptual: 7.9626)\n",
      "Epoch [5/50], Batch [230/250] | D Loss: 0.7004 | G Loss: 12.2995 (GAN: 0.6978, L1: 3.6391, Perceptual: 7.9626)\n",
      "Epoch [5/50], Batch [240/250] | D Loss: 0.7603 | G Loss: 12.2854 (GAN: 0.8629, L1: 3.7291, Perceptual: 7.6934)\n",
      "Epoch [5/50], Batch [240/250] | D Loss: 0.7603 | G Loss: 12.2854 (GAN: 0.8629, L1: 3.7291, Perceptual: 7.6934)\n",
      "Epoch [5/50], Batch [250/250] | D Loss: 0.6652 | G Loss: 11.6527 (GAN: 0.7425, L1: 3.6749, Perceptual: 7.2353)\n",
      "Epoch 5 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 5 ---\n",
      "Saving samples and checkpoint for epoch 5...\n",
      "Epoch [5/50], Batch [250/250] | D Loss: 0.6652 | G Loss: 11.6527 (GAN: 0.7425, L1: 3.6749, Perceptual: 7.2353)\n",
      "Epoch 5 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 5 ---\n",
      "Saving samples and checkpoint for epoch 5...\n",
      "Successfully saved image to: output/generated_images\\epoch_5_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_5_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [6/50], Batch [10/250] | D Loss: 0.5612 | G Loss: 11.8565 (GAN: 0.6535, L1: 3.5082, Perceptual: 7.6948)\n",
      "Epoch [6/50], Batch [10/250] | D Loss: 0.5612 | G Loss: 11.8565 (GAN: 0.6535, L1: 3.5082, Perceptual: 7.6948)\n",
      "Epoch [6/50], Batch [20/250] | D Loss: 0.6615 | G Loss: 12.3746 (GAN: 0.9041, L1: 3.8211, Perceptual: 7.6494)\n",
      "Epoch [6/50], Batch [20/250] | D Loss: 0.6615 | G Loss: 12.3746 (GAN: 0.9041, L1: 3.8211, Perceptual: 7.6494)\n",
      "Epoch [6/50], Batch [30/250] | D Loss: 0.7419 | G Loss: 11.9911 (GAN: 0.8691, L1: 3.7710, Perceptual: 7.3510)\n",
      "Epoch [6/50], Batch [30/250] | D Loss: 0.7419 | G Loss: 11.9911 (GAN: 0.8691, L1: 3.7710, Perceptual: 7.3510)\n",
      "Epoch [6/50], Batch [40/250] | D Loss: 0.4748 | G Loss: 14.6490 (GAN: 1.1226, L1: 5.0152, Perceptual: 8.5112)\n",
      "Epoch [6/50], Batch [40/250] | D Loss: 0.4748 | G Loss: 14.6490 (GAN: 1.1226, L1: 5.0152, Perceptual: 8.5112)\n",
      "Epoch [6/50], Batch [50/250] | D Loss: 0.7860 | G Loss: 11.3329 (GAN: 1.0019, L1: 3.3714, Perceptual: 6.9595)\n",
      "Epoch [6/50], Batch [50/250] | D Loss: 0.7860 | G Loss: 11.3329 (GAN: 1.0019, L1: 3.3714, Perceptual: 6.9595)\n",
      "Epoch [6/50], Batch [60/250] | D Loss: 0.7218 | G Loss: 13.8832 (GAN: 0.8758, L1: 5.1786, Perceptual: 7.8289)\n",
      "Epoch [6/50], Batch [60/250] | D Loss: 0.7218 | G Loss: 13.8832 (GAN: 0.8758, L1: 5.1786, Perceptual: 7.8289)\n",
      "Epoch [6/50], Batch [70/250] | D Loss: 0.5813 | G Loss: 11.6662 (GAN: 0.6632, L1: 3.3195, Perceptual: 7.6834)\n",
      "Epoch [6/50], Batch [70/250] | D Loss: 0.5813 | G Loss: 11.6662 (GAN: 0.6632, L1: 3.3195, Perceptual: 7.6834)\n",
      "Epoch [6/50], Batch [80/250] | D Loss: 0.6910 | G Loss: 12.2332 (GAN: 0.8721, L1: 3.6506, Perceptual: 7.7106)\n",
      "Epoch [6/50], Batch [80/250] | D Loss: 0.6910 | G Loss: 12.2332 (GAN: 0.8721, L1: 3.6506, Perceptual: 7.7106)\n",
      "Epoch [6/50], Batch [90/250] | D Loss: 0.4949 | G Loss: 13.3103 (GAN: 0.8152, L1: 4.2235, Perceptual: 8.2716)\n",
      "Epoch [6/50], Batch [90/250] | D Loss: 0.4949 | G Loss: 13.3103 (GAN: 0.8152, L1: 4.2235, Perceptual: 8.2716)\n",
      "Epoch [6/50], Batch [100/250] | D Loss: 0.7282 | G Loss: 13.5091 (GAN: 0.8983, L1: 4.2838, Perceptual: 8.3269)\n",
      "Epoch [6/50], Batch [100/250] | D Loss: 0.7282 | G Loss: 13.5091 (GAN: 0.8983, L1: 4.2838, Perceptual: 8.3269)\n",
      "Epoch [6/50], Batch [110/250] | D Loss: 0.4428 | G Loss: 14.1614 (GAN: 0.8247, L1: 4.7431, Perceptual: 8.5935)\n",
      "Epoch [6/50], Batch [110/250] | D Loss: 0.4428 | G Loss: 14.1614 (GAN: 0.8247, L1: 4.7431, Perceptual: 8.5935)\n",
      "Epoch [6/50], Batch [120/250] | D Loss: 0.9208 | G Loss: 13.1496 (GAN: 0.9112, L1: 4.3035, Perceptual: 7.9349)\n",
      "Epoch [6/50], Batch [120/250] | D Loss: 0.9208 | G Loss: 13.1496 (GAN: 0.9112, L1: 4.3035, Perceptual: 7.9349)\n",
      "Epoch [6/50], Batch [130/250] | D Loss: 0.6982 | G Loss: 12.4415 (GAN: 0.7972, L1: 4.0006, Perceptual: 7.6438)\n",
      "Epoch [6/50], Batch [130/250] | D Loss: 0.6982 | G Loss: 12.4415 (GAN: 0.7972, L1: 4.0006, Perceptual: 7.6438)\n",
      "Epoch [6/50], Batch [140/250] | D Loss: 0.5548 | G Loss: 12.0172 (GAN: 0.6273, L1: 3.5608, Perceptual: 7.8291)\n",
      "Epoch [6/50], Batch [140/250] | D Loss: 0.5548 | G Loss: 12.0172 (GAN: 0.6273, L1: 3.5608, Perceptual: 7.8291)\n",
      "Epoch [6/50], Batch [150/250] | D Loss: 0.5154 | G Loss: 12.4538 (GAN: 0.6560, L1: 4.1851, Perceptual: 7.6126)\n",
      "Epoch [6/50], Batch [150/250] | D Loss: 0.5154 | G Loss: 12.4538 (GAN: 0.6560, L1: 4.1851, Perceptual: 7.6126)\n",
      "Epoch [6/50], Batch [160/250] | D Loss: 0.6799 | G Loss: 11.5466 (GAN: 0.7005, L1: 3.7030, Perceptual: 7.1431)\n",
      "Epoch [6/50], Batch [160/250] | D Loss: 0.6799 | G Loss: 11.5466 (GAN: 0.7005, L1: 3.7030, Perceptual: 7.1431)\n",
      "Epoch [6/50], Batch [170/250] | D Loss: 0.6512 | G Loss: 12.5256 (GAN: 0.6561, L1: 4.1630, Perceptual: 7.7065)\n",
      "Epoch [6/50], Batch [170/250] | D Loss: 0.6512 | G Loss: 12.5256 (GAN: 0.6561, L1: 4.1630, Perceptual: 7.7065)\n",
      "Epoch [6/50], Batch [180/250] | D Loss: 0.5159 | G Loss: 11.4978 (GAN: 0.9273, L1: 3.8492, Perceptual: 6.7214)\n",
      "Epoch [6/50], Batch [180/250] | D Loss: 0.5159 | G Loss: 11.4978 (GAN: 0.9273, L1: 3.8492, Perceptual: 6.7214)\n",
      "Epoch [6/50], Batch [190/250] | D Loss: 0.6993 | G Loss: 13.6648 (GAN: 0.6891, L1: 4.2679, Perceptual: 8.7078)\n",
      "Epoch [6/50], Batch [190/250] | D Loss: 0.6993 | G Loss: 13.6648 (GAN: 0.6891, L1: 4.2679, Perceptual: 8.7078)\n",
      "Epoch [6/50], Batch [200/250] | D Loss: 0.6141 | G Loss: 12.7940 (GAN: 0.7074, L1: 4.0415, Perceptual: 8.0452)\n",
      "Epoch [6/50], Batch [200/250] | D Loss: 0.6141 | G Loss: 12.7940 (GAN: 0.7074, L1: 4.0415, Perceptual: 8.0452)\n",
      "Epoch [6/50], Batch [210/250] | D Loss: 0.7200 | G Loss: 13.1278 (GAN: 0.6907, L1: 4.4210, Perceptual: 8.0161)\n",
      "Epoch [6/50], Batch [210/250] | D Loss: 0.7200 | G Loss: 13.1278 (GAN: 0.6907, L1: 4.4210, Perceptual: 8.0161)\n",
      "Epoch [6/50], Batch [220/250] | D Loss: 0.6106 | G Loss: 12.1572 (GAN: 0.8297, L1: 3.7546, Perceptual: 7.5729)\n",
      "Epoch [6/50], Batch [220/250] | D Loss: 0.6106 | G Loss: 12.1572 (GAN: 0.8297, L1: 3.7546, Perceptual: 7.5729)\n",
      "Epoch [6/50], Batch [230/250] | D Loss: 0.5857 | G Loss: 12.4067 (GAN: 0.8775, L1: 4.6355, Perceptual: 6.8937)\n",
      "Epoch [6/50], Batch [230/250] | D Loss: 0.5857 | G Loss: 12.4067 (GAN: 0.8775, L1: 4.6355, Perceptual: 6.8937)\n",
      "Epoch [6/50], Batch [240/250] | D Loss: 0.6218 | G Loss: 13.4200 (GAN: 0.9910, L1: 4.0730, Perceptual: 8.3561)\n",
      "Epoch [6/50], Batch [240/250] | D Loss: 0.6218 | G Loss: 13.4200 (GAN: 0.9910, L1: 4.0730, Perceptual: 8.3561)\n",
      "Epoch [6/50], Batch [250/250] | D Loss: 0.5433 | G Loss: 13.0566 (GAN: 0.9755, L1: 4.2365, Perceptual: 7.8446)\n",
      "Epoch 6 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [6/50], Batch [250/250] | D Loss: 0.5433 | G Loss: 13.0566 (GAN: 0.9755, L1: 4.2365, Perceptual: 7.8446)\n",
      "Epoch 6 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [7/50], Batch [10/250] | D Loss: 0.7731 | G Loss: 11.1435 (GAN: 0.6939, L1: 3.6169, Perceptual: 6.8327)\n",
      "Epoch [7/50], Batch [10/250] | D Loss: 0.7731 | G Loss: 11.1435 (GAN: 0.6939, L1: 3.6169, Perceptual: 6.8327)\n",
      "Epoch [7/50], Batch [20/250] | D Loss: 0.4704 | G Loss: 13.3748 (GAN: 0.8534, L1: 4.5043, Perceptual: 8.0171)\n",
      "Epoch [7/50], Batch [20/250] | D Loss: 0.4704 | G Loss: 13.3748 (GAN: 0.8534, L1: 4.5043, Perceptual: 8.0171)\n",
      "Epoch [7/50], Batch [30/250] | D Loss: 0.4929 | G Loss: 13.4426 (GAN: 0.9405, L1: 4.3589, Perceptual: 8.1432)\n",
      "Epoch [7/50], Batch [30/250] | D Loss: 0.4929 | G Loss: 13.4426 (GAN: 0.9405, L1: 4.3589, Perceptual: 8.1432)\n",
      "Epoch [7/50], Batch [40/250] | D Loss: 0.3984 | G Loss: 14.5563 (GAN: 0.8604, L1: 4.5850, Perceptual: 9.1110)\n",
      "Epoch [7/50], Batch [40/250] | D Loss: 0.3984 | G Loss: 14.5563 (GAN: 0.8604, L1: 4.5850, Perceptual: 9.1110)\n",
      "Epoch [7/50], Batch [50/250] | D Loss: 0.5844 | G Loss: 12.4146 (GAN: 0.9287, L1: 3.6030, Perceptual: 7.8829)\n",
      "Epoch [7/50], Batch [50/250] | D Loss: 0.5844 | G Loss: 12.4146 (GAN: 0.9287, L1: 3.6030, Perceptual: 7.8829)\n",
      "Epoch [7/50], Batch [60/250] | D Loss: 0.6016 | G Loss: 11.7102 (GAN: 0.7923, L1: 3.5010, Perceptual: 7.4169)\n",
      "Epoch [7/50], Batch [60/250] | D Loss: 0.6016 | G Loss: 11.7102 (GAN: 0.7923, L1: 3.5010, Perceptual: 7.4169)\n",
      "Epoch [7/50], Batch [70/250] | D Loss: 0.5534 | G Loss: 13.3908 (GAN: 0.6945, L1: 4.7203, Perceptual: 7.9760)\n",
      "Epoch [7/50], Batch [70/250] | D Loss: 0.5534 | G Loss: 13.3908 (GAN: 0.6945, L1: 4.7203, Perceptual: 7.9760)\n",
      "Epoch [7/50], Batch [80/250] | D Loss: 0.4326 | G Loss: 13.5043 (GAN: 0.9929, L1: 4.3692, Perceptual: 8.1421)\n",
      "Epoch [7/50], Batch [80/250] | D Loss: 0.4326 | G Loss: 13.5043 (GAN: 0.9929, L1: 4.3692, Perceptual: 8.1421)\n",
      "Epoch [7/50], Batch [90/250] | D Loss: 0.7184 | G Loss: 12.0083 (GAN: 0.6800, L1: 3.7964, Perceptual: 7.5319)\n",
      "Epoch [7/50], Batch [90/250] | D Loss: 0.7184 | G Loss: 12.0083 (GAN: 0.6800, L1: 3.7964, Perceptual: 7.5319)\n",
      "Epoch [7/50], Batch [100/250] | D Loss: 0.5265 | G Loss: 13.6639 (GAN: 0.8938, L1: 4.6648, Perceptual: 8.1053)\n",
      "Epoch [7/50], Batch [100/250] | D Loss: 0.5265 | G Loss: 13.6639 (GAN: 0.8938, L1: 4.6648, Perceptual: 8.1053)\n",
      "Epoch [7/50], Batch [110/250] | D Loss: 0.6518 | G Loss: 12.5592 (GAN: 0.7435, L1: 4.2265, Perceptual: 7.5892)\n",
      "Epoch [7/50], Batch [110/250] | D Loss: 0.6518 | G Loss: 12.5592 (GAN: 0.7435, L1: 4.2265, Perceptual: 7.5892)\n",
      "Epoch [7/50], Batch [120/250] | D Loss: 0.7313 | G Loss: 12.0315 (GAN: 0.8501, L1: 3.4167, Perceptual: 7.7647)\n",
      "Epoch [7/50], Batch [120/250] | D Loss: 0.7313 | G Loss: 12.0315 (GAN: 0.8501, L1: 3.4167, Perceptual: 7.7647)\n",
      "Epoch [7/50], Batch [130/250] | D Loss: 0.5473 | G Loss: 12.7298 (GAN: 0.6490, L1: 4.1230, Perceptual: 7.9577)\n",
      "Epoch [7/50], Batch [130/250] | D Loss: 0.5473 | G Loss: 12.7298 (GAN: 0.6490, L1: 4.1230, Perceptual: 7.9577)\n",
      "Epoch [7/50], Batch [140/250] | D Loss: 0.6650 | G Loss: 13.2168 (GAN: 0.9235, L1: 4.3070, Perceptual: 7.9862)\n",
      "Epoch [7/50], Batch [140/250] | D Loss: 0.6650 | G Loss: 13.2168 (GAN: 0.9235, L1: 4.3070, Perceptual: 7.9862)\n",
      "Epoch [7/50], Batch [150/250] | D Loss: 0.6487 | G Loss: 13.0747 (GAN: 0.9525, L1: 3.9134, Perceptual: 8.2088)\n",
      "Epoch [7/50], Batch [150/250] | D Loss: 0.6487 | G Loss: 13.0747 (GAN: 0.9525, L1: 3.9134, Perceptual: 8.2088)\n",
      "Epoch [7/50], Batch [160/250] | D Loss: 0.9248 | G Loss: 11.6528 (GAN: 1.0259, L1: 3.5888, Perceptual: 7.0381)\n",
      "Epoch [7/50], Batch [160/250] | D Loss: 0.9248 | G Loss: 11.6528 (GAN: 1.0259, L1: 3.5888, Perceptual: 7.0381)\n",
      "Epoch [7/50], Batch [170/250] | D Loss: 0.7121 | G Loss: 12.7208 (GAN: 0.6934, L1: 4.4970, Perceptual: 7.5304)\n",
      "Epoch [7/50], Batch [170/250] | D Loss: 0.7121 | G Loss: 12.7208 (GAN: 0.6934, L1: 4.4970, Perceptual: 7.5304)\n",
      "Epoch [7/50], Batch [180/250] | D Loss: 0.6068 | G Loss: 12.3927 (GAN: 0.7249, L1: 3.7266, Perceptual: 7.9412)\n",
      "Epoch [7/50], Batch [180/250] | D Loss: 0.6068 | G Loss: 12.3927 (GAN: 0.7249, L1: 3.7266, Perceptual: 7.9412)\n",
      "Epoch [7/50], Batch [190/250] | D Loss: 0.7737 | G Loss: 11.5456 (GAN: 0.6560, L1: 3.4253, Perceptual: 7.4643)\n",
      "Epoch [7/50], Batch [190/250] | D Loss: 0.7737 | G Loss: 11.5456 (GAN: 0.6560, L1: 3.4253, Perceptual: 7.4643)\n",
      "Epoch [7/50], Batch [200/250] | D Loss: 0.6071 | G Loss: 12.6494 (GAN: 0.7974, L1: 4.0941, Perceptual: 7.7579)\n",
      "Epoch [7/50], Batch [200/250] | D Loss: 0.6071 | G Loss: 12.6494 (GAN: 0.7974, L1: 4.0941, Perceptual: 7.7579)\n",
      "Epoch [7/50], Batch [210/250] | D Loss: 0.4664 | G Loss: 12.7729 (GAN: 0.7372, L1: 4.0773, Perceptual: 7.9584)\n",
      "Epoch [7/50], Batch [210/250] | D Loss: 0.4664 | G Loss: 12.7729 (GAN: 0.7372, L1: 4.0773, Perceptual: 7.9584)\n",
      "Epoch [7/50], Batch [220/250] | D Loss: 0.5229 | G Loss: 12.5001 (GAN: 0.6792, L1: 3.9668, Perceptual: 7.8541)\n",
      "Epoch [7/50], Batch [220/250] | D Loss: 0.5229 | G Loss: 12.5001 (GAN: 0.6792, L1: 3.9668, Perceptual: 7.8541)\n",
      "Epoch [7/50], Batch [230/250] | D Loss: 0.5847 | G Loss: 12.1662 (GAN: 0.7337, L1: 3.6772, Perceptual: 7.7553)\n",
      "Epoch [7/50], Batch [230/250] | D Loss: 0.5847 | G Loss: 12.1662 (GAN: 0.7337, L1: 3.6772, Perceptual: 7.7553)\n",
      "Epoch [7/50], Batch [240/250] | D Loss: 0.7649 | G Loss: 13.4559 (GAN: 1.1720, L1: 4.6987, Perceptual: 7.5852)\n",
      "Epoch [7/50], Batch [240/250] | D Loss: 0.7649 | G Loss: 13.4559 (GAN: 1.1720, L1: 4.6987, Perceptual: 7.5852)\n",
      "Epoch [7/50], Batch [250/250] | D Loss: 0.5087 | G Loss: 13.9800 (GAN: 0.9089, L1: 4.2347, Perceptual: 8.8365)\n",
      "Epoch 7 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [7/50], Batch [250/250] | D Loss: 0.5087 | G Loss: 13.9800 (GAN: 0.9089, L1: 4.2347, Perceptual: 8.8365)\n",
      "Epoch 7 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [8/50], Batch [10/250] | D Loss: 0.6603 | G Loss: 11.0660 (GAN: 0.9344, L1: 3.3563, Perceptual: 6.7753)\n",
      "Epoch [8/50], Batch [10/250] | D Loss: 0.6603 | G Loss: 11.0660 (GAN: 0.9344, L1: 3.3563, Perceptual: 6.7753)\n",
      "Epoch [8/50], Batch [20/250] | D Loss: 0.4397 | G Loss: 12.3106 (GAN: 1.0104, L1: 3.8654, Perceptual: 7.4348)\n",
      "Epoch [8/50], Batch [20/250] | D Loss: 0.4397 | G Loss: 12.3106 (GAN: 1.0104, L1: 3.8654, Perceptual: 7.4348)\n",
      "Epoch [8/50], Batch [30/250] | D Loss: 0.7447 | G Loss: 11.1779 (GAN: 0.8676, L1: 3.3396, Perceptual: 6.9707)\n",
      "Epoch [8/50], Batch [30/250] | D Loss: 0.7447 | G Loss: 11.1779 (GAN: 0.8676, L1: 3.3396, Perceptual: 6.9707)\n",
      "Epoch [8/50], Batch [40/250] | D Loss: 0.7319 | G Loss: 11.8611 (GAN: 0.8913, L1: 3.6159, Perceptual: 7.3540)\n",
      "Epoch [8/50], Batch [40/250] | D Loss: 0.7319 | G Loss: 11.8611 (GAN: 0.8913, L1: 3.6159, Perceptual: 7.3540)\n",
      "Epoch [8/50], Batch [50/250] | D Loss: 0.6045 | G Loss: 11.7548 (GAN: 0.6937, L1: 3.8184, Perceptual: 7.2427)\n",
      "Epoch [8/50], Batch [50/250] | D Loss: 0.6045 | G Loss: 11.7548 (GAN: 0.6937, L1: 3.8184, Perceptual: 7.2427)\n",
      "Epoch [8/50], Batch [60/250] | D Loss: 0.6112 | G Loss: 12.6884 (GAN: 0.9738, L1: 3.6270, Perceptual: 8.0876)\n",
      "Epoch [8/50], Batch [60/250] | D Loss: 0.6112 | G Loss: 12.6884 (GAN: 0.9738, L1: 3.6270, Perceptual: 8.0876)\n",
      "Epoch [8/50], Batch [70/250] | D Loss: 0.5204 | G Loss: 13.4123 (GAN: 0.8128, L1: 4.4662, Perceptual: 8.1333)\n",
      "Epoch [8/50], Batch [70/250] | D Loss: 0.5204 | G Loss: 13.4123 (GAN: 0.8128, L1: 4.4662, Perceptual: 8.1333)\n",
      "Epoch [8/50], Batch [80/250] | D Loss: 0.6233 | G Loss: 11.7419 (GAN: 0.7329, L1: 3.7139, Perceptual: 7.2951)\n",
      "Epoch [8/50], Batch [80/250] | D Loss: 0.6233 | G Loss: 11.7419 (GAN: 0.7329, L1: 3.7139, Perceptual: 7.2951)\n",
      "Epoch [8/50], Batch [90/250] | D Loss: 0.6726 | G Loss: 12.1480 (GAN: 0.7804, L1: 3.4996, Perceptual: 7.8680)\n",
      "Epoch [8/50], Batch [90/250] | D Loss: 0.6726 | G Loss: 12.1480 (GAN: 0.7804, L1: 3.4996, Perceptual: 7.8680)\n",
      "Epoch [8/50], Batch [100/250] | D Loss: 0.5064 | G Loss: 14.5742 (GAN: 0.6581, L1: 5.1212, Perceptual: 8.7950)\n",
      "Epoch [8/50], Batch [100/250] | D Loss: 0.5064 | G Loss: 14.5742 (GAN: 0.6581, L1: 5.1212, Perceptual: 8.7950)\n",
      "Epoch [8/50], Batch [110/250] | D Loss: 0.6002 | G Loss: 13.2112 (GAN: 0.7568, L1: 4.2785, Perceptual: 8.1759)\n",
      "Epoch [8/50], Batch [110/250] | D Loss: 0.6002 | G Loss: 13.2112 (GAN: 0.7568, L1: 4.2785, Perceptual: 8.1759)\n",
      "Epoch [8/50], Batch [120/250] | D Loss: 0.9659 | G Loss: 12.3243 (GAN: 0.8686, L1: 3.7206, Perceptual: 7.7352)\n",
      "Epoch [8/50], Batch [120/250] | D Loss: 0.9659 | G Loss: 12.3243 (GAN: 0.8686, L1: 3.7206, Perceptual: 7.7352)\n",
      "Epoch [8/50], Batch [130/250] | D Loss: 0.5734 | G Loss: 11.0761 (GAN: 0.6056, L1: 3.4137, Perceptual: 7.0568)\n",
      "Epoch [8/50], Batch [130/250] | D Loss: 0.5734 | G Loss: 11.0761 (GAN: 0.6056, L1: 3.4137, Perceptual: 7.0568)\n",
      "Epoch [8/50], Batch [140/250] | D Loss: 0.7679 | G Loss: 10.9669 (GAN: 0.4331, L1: 3.2927, Perceptual: 7.2412)\n",
      "Epoch [8/50], Batch [140/250] | D Loss: 0.7679 | G Loss: 10.9669 (GAN: 0.4331, L1: 3.2927, Perceptual: 7.2412)\n",
      "Epoch [8/50], Batch [150/250] | D Loss: 0.6114 | G Loss: 12.1109 (GAN: 0.7382, L1: 3.7376, Perceptual: 7.6351)\n",
      "Epoch [8/50], Batch [150/250] | D Loss: 0.6114 | G Loss: 12.1109 (GAN: 0.7382, L1: 3.7376, Perceptual: 7.6351)\n",
      "Epoch [8/50], Batch [160/250] | D Loss: 0.4507 | G Loss: 14.2094 (GAN: 0.9205, L1: 4.6012, Perceptual: 8.6878)\n",
      "Epoch [8/50], Batch [160/250] | D Loss: 0.4507 | G Loss: 14.2094 (GAN: 0.9205, L1: 4.6012, Perceptual: 8.6878)\n",
      "Epoch [8/50], Batch [170/250] | D Loss: 0.6644 | G Loss: 12.3722 (GAN: 0.6385, L1: 3.8105, Perceptual: 7.9232)\n",
      "Epoch [8/50], Batch [170/250] | D Loss: 0.6644 | G Loss: 12.3722 (GAN: 0.6385, L1: 3.8105, Perceptual: 7.9232)\n",
      "Epoch [8/50], Batch [180/250] | D Loss: 0.6743 | G Loss: 13.1804 (GAN: 0.9158, L1: 4.0813, Perceptual: 8.1833)\n",
      "Epoch [8/50], Batch [180/250] | D Loss: 0.6743 | G Loss: 13.1804 (GAN: 0.9158, L1: 4.0813, Perceptual: 8.1833)\n",
      "Epoch [8/50], Batch [190/250] | D Loss: 0.8106 | G Loss: 12.2815 (GAN: 1.1982, L1: 3.6403, Perceptual: 7.4429)\n",
      "Epoch [8/50], Batch [190/250] | D Loss: 0.8106 | G Loss: 12.2815 (GAN: 1.1982, L1: 3.6403, Perceptual: 7.4429)\n",
      "Epoch [8/50], Batch [200/250] | D Loss: 0.6947 | G Loss: 12.6396 (GAN: 0.8063, L1: 3.7037, Perceptual: 8.1296)\n",
      "Epoch [8/50], Batch [200/250] | D Loss: 0.6947 | G Loss: 12.6396 (GAN: 0.8063, L1: 3.7037, Perceptual: 8.1296)\n",
      "Epoch [8/50], Batch [210/250] | D Loss: 0.5883 | G Loss: 13.3391 (GAN: 0.7725, L1: 3.9278, Perceptual: 8.6388)\n",
      "Epoch [8/50], Batch [210/250] | D Loss: 0.5883 | G Loss: 13.3391 (GAN: 0.7725, L1: 3.9278, Perceptual: 8.6388)\n",
      "Epoch [8/50], Batch [220/250] | D Loss: 0.7059 | G Loss: 12.2503 (GAN: 0.8286, L1: 3.9582, Perceptual: 7.4635)\n",
      "Epoch [8/50], Batch [220/250] | D Loss: 0.7059 | G Loss: 12.2503 (GAN: 0.8286, L1: 3.9582, Perceptual: 7.4635)\n",
      "Epoch [8/50], Batch [230/250] | D Loss: 0.6937 | G Loss: 12.8742 (GAN: 0.7580, L1: 4.1516, Perceptual: 7.9646)\n",
      "Epoch [8/50], Batch [230/250] | D Loss: 0.6937 | G Loss: 12.8742 (GAN: 0.7580, L1: 4.1516, Perceptual: 7.9646)\n",
      "Epoch [8/50], Batch [240/250] | D Loss: 0.5093 | G Loss: 13.1446 (GAN: 0.8048, L1: 4.2013, Perceptual: 8.1385)\n",
      "Epoch [8/50], Batch [240/250] | D Loss: 0.5093 | G Loss: 13.1446 (GAN: 0.8048, L1: 4.2013, Perceptual: 8.1385)\n",
      "Epoch [8/50], Batch [250/250] | D Loss: 0.5066 | G Loss: 12.9050 (GAN: 0.8867, L1: 3.9360, Perceptual: 8.0824)\n",
      "Epoch 8 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [8/50], Batch [250/250] | D Loss: 0.5066 | G Loss: 12.9050 (GAN: 0.8867, L1: 3.9360, Perceptual: 8.0824)\n",
      "Epoch 8 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [9/50], Batch [10/250] | D Loss: 0.5295 | G Loss: 12.8982 (GAN: 0.7378, L1: 4.3211, Perceptual: 7.8393)\n",
      "Epoch [9/50], Batch [10/250] | D Loss: 0.5295 | G Loss: 12.8982 (GAN: 0.7378, L1: 4.3211, Perceptual: 7.8393)\n",
      "Epoch [9/50], Batch [20/250] | D Loss: 0.6824 | G Loss: 10.9556 (GAN: 0.6255, L1: 3.3005, Perceptual: 7.0295)\n",
      "Epoch [9/50], Batch [20/250] | D Loss: 0.6824 | G Loss: 10.9556 (GAN: 0.6255, L1: 3.3005, Perceptual: 7.0295)\n",
      "Epoch [9/50], Batch [30/250] | D Loss: 0.5631 | G Loss: 12.7764 (GAN: 0.6959, L1: 4.2654, Perceptual: 7.8151)\n",
      "Epoch [9/50], Batch [30/250] | D Loss: 0.5631 | G Loss: 12.7764 (GAN: 0.6959, L1: 4.2654, Perceptual: 7.8151)\n",
      "Epoch [9/50], Batch [40/250] | D Loss: 0.7938 | G Loss: 12.7052 (GAN: 0.8156, L1: 4.0705, Perceptual: 7.8191)\n",
      "Epoch [9/50], Batch [40/250] | D Loss: 0.7938 | G Loss: 12.7052 (GAN: 0.8156, L1: 4.0705, Perceptual: 7.8191)\n",
      "Epoch [9/50], Batch [50/250] | D Loss: 0.4327 | G Loss: 12.7711 (GAN: 0.8863, L1: 3.8712, Perceptual: 8.0135)\n",
      "Epoch [9/50], Batch [50/250] | D Loss: 0.4327 | G Loss: 12.7711 (GAN: 0.8863, L1: 3.8712, Perceptual: 8.0135)\n",
      "Epoch [9/50], Batch [60/250] | D Loss: 0.5527 | G Loss: 13.5431 (GAN: 1.0242, L1: 4.4626, Perceptual: 8.0563)\n",
      "Epoch [9/50], Batch [60/250] | D Loss: 0.5527 | G Loss: 13.5431 (GAN: 1.0242, L1: 4.4626, Perceptual: 8.0563)\n",
      "Epoch [9/50], Batch [70/250] | D Loss: 0.7159 | G Loss: 11.5281 (GAN: 0.6391, L1: 3.3813, Perceptual: 7.5078)\n",
      "Epoch [9/50], Batch [70/250] | D Loss: 0.7159 | G Loss: 11.5281 (GAN: 0.6391, L1: 3.3813, Perceptual: 7.5078)\n",
      "Epoch [9/50], Batch [80/250] | D Loss: 0.6740 | G Loss: 11.1917 (GAN: 0.7262, L1: 3.3330, Perceptual: 7.1326)\n",
      "Epoch [9/50], Batch [80/250] | D Loss: 0.6740 | G Loss: 11.1917 (GAN: 0.7262, L1: 3.3330, Perceptual: 7.1326)\n",
      "Epoch [9/50], Batch [90/250] | D Loss: 0.6324 | G Loss: 12.2508 (GAN: 0.9340, L1: 4.0080, Perceptual: 7.3089)\n",
      "Epoch [9/50], Batch [90/250] | D Loss: 0.6324 | G Loss: 12.2508 (GAN: 0.9340, L1: 4.0080, Perceptual: 7.3089)\n",
      "Epoch [9/50], Batch [100/250] | D Loss: 0.5986 | G Loss: 12.4018 (GAN: 0.9276, L1: 3.9708, Perceptual: 7.5034)\n",
      "Epoch [9/50], Batch [100/250] | D Loss: 0.5986 | G Loss: 12.4018 (GAN: 0.9276, L1: 3.9708, Perceptual: 7.5034)\n",
      "Epoch [9/50], Batch [110/250] | D Loss: 0.5189 | G Loss: 13.6463 (GAN: 1.0513, L1: 4.5826, Perceptual: 8.0123)\n",
      "Epoch [9/50], Batch [110/250] | D Loss: 0.5189 | G Loss: 13.6463 (GAN: 1.0513, L1: 4.5826, Perceptual: 8.0123)\n",
      "Epoch [9/50], Batch [120/250] | D Loss: 0.5424 | G Loss: 12.6086 (GAN: 0.7469, L1: 4.6063, Perceptual: 7.2553)\n",
      "Epoch [9/50], Batch [120/250] | D Loss: 0.5424 | G Loss: 12.6086 (GAN: 0.7469, L1: 4.6063, Perceptual: 7.2553)\n",
      "Epoch [9/50], Batch [130/250] | D Loss: 0.6117 | G Loss: 11.6456 (GAN: 0.8524, L1: 3.4719, Perceptual: 7.3213)\n",
      "Epoch [9/50], Batch [130/250] | D Loss: 0.6117 | G Loss: 11.6456 (GAN: 0.8524, L1: 3.4719, Perceptual: 7.3213)\n",
      "Epoch [9/50], Batch [140/250] | D Loss: 0.8508 | G Loss: 10.5677 (GAN: 0.7499, L1: 3.1820, Perceptual: 6.6358)\n",
      "Epoch [9/50], Batch [140/250] | D Loss: 0.8508 | G Loss: 10.5677 (GAN: 0.7499, L1: 3.1820, Perceptual: 6.6358)\n",
      "Epoch [9/50], Batch [150/250] | D Loss: 0.6411 | G Loss: 12.2105 (GAN: 0.6750, L1: 3.9161, Perceptual: 7.6194)\n",
      "Epoch [9/50], Batch [150/250] | D Loss: 0.6411 | G Loss: 12.2105 (GAN: 0.6750, L1: 3.9161, Perceptual: 7.6194)\n",
      "Epoch [9/50], Batch [160/250] | D Loss: 0.7270 | G Loss: 12.2404 (GAN: 1.0077, L1: 3.8026, Perceptual: 7.4300)\n",
      "Epoch [9/50], Batch [160/250] | D Loss: 0.7270 | G Loss: 12.2404 (GAN: 1.0077, L1: 3.8026, Perceptual: 7.4300)\n",
      "Epoch [9/50], Batch [170/250] | D Loss: 0.6370 | G Loss: 12.3498 (GAN: 0.7724, L1: 3.6887, Perceptual: 7.8887)\n",
      "Epoch [9/50], Batch [170/250] | D Loss: 0.6370 | G Loss: 12.3498 (GAN: 0.7724, L1: 3.6887, Perceptual: 7.8887)\n",
      "Epoch [9/50], Batch [180/250] | D Loss: 0.6209 | G Loss: 12.9124 (GAN: 0.8366, L1: 4.3499, Perceptual: 7.7259)\n",
      "Epoch [9/50], Batch [180/250] | D Loss: 0.6209 | G Loss: 12.9124 (GAN: 0.8366, L1: 4.3499, Perceptual: 7.7259)\n",
      "Epoch [9/50], Batch [190/250] | D Loss: 0.6166 | G Loss: 11.7124 (GAN: 0.8225, L1: 3.6836, Perceptual: 7.2062)\n",
      "Epoch [9/50], Batch [190/250] | D Loss: 0.6166 | G Loss: 11.7124 (GAN: 0.8225, L1: 3.6836, Perceptual: 7.2062)\n",
      "Epoch [9/50], Batch [200/250] | D Loss: 0.3854 | G Loss: 14.0672 (GAN: 0.6574, L1: 4.6944, Perceptual: 8.7153)\n",
      "Epoch [9/50], Batch [200/250] | D Loss: 0.3854 | G Loss: 14.0672 (GAN: 0.6574, L1: 4.6944, Perceptual: 8.7153)\n",
      "Epoch [9/50], Batch [210/250] | D Loss: 0.7839 | G Loss: 11.3708 (GAN: 0.4928, L1: 3.7484, Perceptual: 7.1296)\n",
      "Epoch [9/50], Batch [210/250] | D Loss: 0.7839 | G Loss: 11.3708 (GAN: 0.4928, L1: 3.7484, Perceptual: 7.1296)\n",
      "Epoch [9/50], Batch [220/250] | D Loss: 0.6596 | G Loss: 13.1951 (GAN: 1.2761, L1: 3.8119, Perceptual: 8.1071)\n",
      "Epoch [9/50], Batch [220/250] | D Loss: 0.6596 | G Loss: 13.1951 (GAN: 1.2761, L1: 3.8119, Perceptual: 8.1071)\n",
      "Epoch [9/50], Batch [230/250] | D Loss: 0.7713 | G Loss: 12.1829 (GAN: 0.8144, L1: 3.7540, Perceptual: 7.6145)\n",
      "Epoch [9/50], Batch [230/250] | D Loss: 0.7713 | G Loss: 12.1829 (GAN: 0.8144, L1: 3.7540, Perceptual: 7.6145)\n",
      "Epoch [9/50], Batch [240/250] | D Loss: 0.7971 | G Loss: 11.9692 (GAN: 0.8871, L1: 3.4993, Perceptual: 7.5828)\n",
      "Epoch [9/50], Batch [240/250] | D Loss: 0.7971 | G Loss: 11.9692 (GAN: 0.8871, L1: 3.4993, Perceptual: 7.5828)\n",
      "Epoch [9/50], Batch [250/250] | D Loss: 0.6967 | G Loss: 11.3113 (GAN: 0.5955, L1: 3.2252, Perceptual: 7.4906)\n",
      "Epoch 9 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [9/50], Batch [250/250] | D Loss: 0.6967 | G Loss: 11.3113 (GAN: 0.5955, L1: 3.2252, Perceptual: 7.4906)\n",
      "Epoch 9 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [10/50], Batch [10/250] | D Loss: 0.7842 | G Loss: 11.6388 (GAN: 0.9426, L1: 3.5721, Perceptual: 7.1241)\n",
      "Epoch [10/50], Batch [10/250] | D Loss: 0.7842 | G Loss: 11.6388 (GAN: 0.9426, L1: 3.5721, Perceptual: 7.1241)\n",
      "Epoch [10/50], Batch [20/250] | D Loss: 0.5170 | G Loss: 12.7712 (GAN: 0.7320, L1: 4.0296, Perceptual: 8.0096)\n",
      "Epoch [10/50], Batch [20/250] | D Loss: 0.5170 | G Loss: 12.7712 (GAN: 0.7320, L1: 4.0296, Perceptual: 8.0096)\n",
      "Epoch [10/50], Batch [30/250] | D Loss: 0.7033 | G Loss: 11.5811 (GAN: 0.8780, L1: 3.4477, Perceptual: 7.2554)\n",
      "Epoch [10/50], Batch [30/250] | D Loss: 0.7033 | G Loss: 11.5811 (GAN: 0.8780, L1: 3.4477, Perceptual: 7.2554)\n",
      "Epoch [10/50], Batch [40/250] | D Loss: 0.5637 | G Loss: 13.4558 (GAN: 0.7605, L1: 4.3016, Perceptual: 8.3937)\n",
      "Epoch [10/50], Batch [40/250] | D Loss: 0.5637 | G Loss: 13.4558 (GAN: 0.7605, L1: 4.3016, Perceptual: 8.3937)\n",
      "Epoch [10/50], Batch [50/250] | D Loss: 0.6272 | G Loss: 11.4753 (GAN: 0.7356, L1: 3.3086, Perceptual: 7.4311)\n",
      "Epoch [10/50], Batch [50/250] | D Loss: 0.6272 | G Loss: 11.4753 (GAN: 0.7356, L1: 3.3086, Perceptual: 7.4311)\n",
      "Epoch [10/50], Batch [60/250] | D Loss: 0.6647 | G Loss: 11.3850 (GAN: 0.7297, L1: 3.3766, Perceptual: 7.2787)\n",
      "Epoch [10/50], Batch [60/250] | D Loss: 0.6647 | G Loss: 11.3850 (GAN: 0.7297, L1: 3.3766, Perceptual: 7.2787)\n",
      "Epoch [10/50], Batch [70/250] | D Loss: 0.6222 | G Loss: 12.6680 (GAN: 0.7120, L1: 4.2131, Perceptual: 7.7429)\n",
      "Epoch [10/50], Batch [70/250] | D Loss: 0.6222 | G Loss: 12.6680 (GAN: 0.7120, L1: 4.2131, Perceptual: 7.7429)\n",
      "Epoch [10/50], Batch [80/250] | D Loss: 0.5681 | G Loss: 12.1203 (GAN: 0.7934, L1: 3.5554, Perceptual: 7.7715)\n",
      "Epoch [10/50], Batch [80/250] | D Loss: 0.5681 | G Loss: 12.1203 (GAN: 0.7934, L1: 3.5554, Perceptual: 7.7715)\n",
      "Epoch [10/50], Batch [90/250] | D Loss: 0.6754 | G Loss: 12.2317 (GAN: 0.9461, L1: 4.2206, Perceptual: 7.0650)\n",
      "Epoch [10/50], Batch [90/250] | D Loss: 0.6754 | G Loss: 12.2317 (GAN: 0.9461, L1: 4.2206, Perceptual: 7.0650)\n",
      "Epoch [10/50], Batch [100/250] | D Loss: 0.6837 | G Loss: 12.1452 (GAN: 0.5741, L1: 3.8675, Perceptual: 7.7036)\n",
      "Epoch [10/50], Batch [100/250] | D Loss: 0.6837 | G Loss: 12.1452 (GAN: 0.5741, L1: 3.8675, Perceptual: 7.7036)\n",
      "Epoch [10/50], Batch [110/250] | D Loss: 0.6238 | G Loss: 13.7514 (GAN: 0.6475, L1: 4.7583, Perceptual: 8.3456)\n",
      "Epoch [10/50], Batch [110/250] | D Loss: 0.6238 | G Loss: 13.7514 (GAN: 0.6475, L1: 4.7583, Perceptual: 8.3456)\n",
      "Epoch [10/50], Batch [120/250] | D Loss: 0.6937 | G Loss: 11.3420 (GAN: 0.7447, L1: 3.6171, Perceptual: 6.9802)\n",
      "Epoch [10/50], Batch [120/250] | D Loss: 0.6937 | G Loss: 11.3420 (GAN: 0.7447, L1: 3.6171, Perceptual: 6.9802)\n",
      "Epoch [10/50], Batch [130/250] | D Loss: 0.6475 | G Loss: 11.7934 (GAN: 0.6366, L1: 3.5541, Perceptual: 7.6027)\n",
      "Epoch [10/50], Batch [130/250] | D Loss: 0.6475 | G Loss: 11.7934 (GAN: 0.6366, L1: 3.5541, Perceptual: 7.6027)\n",
      "Epoch [10/50], Batch [140/250] | D Loss: 0.4988 | G Loss: 12.1810 (GAN: 0.5441, L1: 3.8726, Perceptual: 7.7644)\n",
      "Epoch [10/50], Batch [140/250] | D Loss: 0.4988 | G Loss: 12.1810 (GAN: 0.5441, L1: 3.8726, Perceptual: 7.7644)\n",
      "Epoch [10/50], Batch [150/250] | D Loss: 0.9095 | G Loss: 11.8567 (GAN: 0.8090, L1: 3.4515, Perceptual: 7.5961)\n",
      "Epoch [10/50], Batch [150/250] | D Loss: 0.9095 | G Loss: 11.8567 (GAN: 0.8090, L1: 3.4515, Perceptual: 7.5961)\n",
      "Epoch [10/50], Batch [160/250] | D Loss: 0.5764 | G Loss: 12.5956 (GAN: 0.8881, L1: 3.5743, Perceptual: 8.1332)\n",
      "Epoch [10/50], Batch [160/250] | D Loss: 0.5764 | G Loss: 12.5956 (GAN: 0.8881, L1: 3.5743, Perceptual: 8.1332)\n",
      "Epoch [10/50], Batch [170/250] | D Loss: 0.4890 | G Loss: 13.1390 (GAN: 0.5459, L1: 4.6720, Perceptual: 7.9211)\n",
      "Epoch [10/50], Batch [170/250] | D Loss: 0.4890 | G Loss: 13.1390 (GAN: 0.5459, L1: 4.6720, Perceptual: 7.9211)\n",
      "Epoch [10/50], Batch [180/250] | D Loss: 0.5910 | G Loss: 13.1074 (GAN: 0.6153, L1: 4.6115, Perceptual: 7.8807)\n",
      "Epoch [10/50], Batch [180/250] | D Loss: 0.5910 | G Loss: 13.1074 (GAN: 0.6153, L1: 4.6115, Perceptual: 7.8807)\n",
      "Epoch [10/50], Batch [190/250] | D Loss: 0.6905 | G Loss: 12.9092 (GAN: 0.7644, L1: 4.2023, Perceptual: 7.9426)\n",
      "Epoch [10/50], Batch [190/250] | D Loss: 0.6905 | G Loss: 12.9092 (GAN: 0.7644, L1: 4.2023, Perceptual: 7.9426)\n",
      "Epoch [10/50], Batch [200/250] | D Loss: 0.7915 | G Loss: 13.6878 (GAN: 0.9231, L1: 4.2888, Perceptual: 8.4759)\n",
      "Epoch [10/50], Batch [200/250] | D Loss: 0.7915 | G Loss: 13.6878 (GAN: 0.9231, L1: 4.2888, Perceptual: 8.4759)\n",
      "Epoch [10/50], Batch [210/250] | D Loss: 0.5772 | G Loss: 13.2399 (GAN: 0.7962, L1: 4.3424, Perceptual: 8.1012)\n",
      "Epoch [10/50], Batch [210/250] | D Loss: 0.5772 | G Loss: 13.2399 (GAN: 0.7962, L1: 4.3424, Perceptual: 8.1012)\n",
      "Epoch [10/50], Batch [220/250] | D Loss: 0.6067 | G Loss: 11.7560 (GAN: 0.6243, L1: 3.8389, Perceptual: 7.2928)\n",
      "Epoch [10/50], Batch [220/250] | D Loss: 0.6067 | G Loss: 11.7560 (GAN: 0.6243, L1: 3.8389, Perceptual: 7.2928)\n",
      "Epoch [10/50], Batch [230/250] | D Loss: 0.3402 | G Loss: 13.9685 (GAN: 0.9243, L1: 4.4734, Perceptual: 8.5708)\n",
      "Epoch [10/50], Batch [230/250] | D Loss: 0.3402 | G Loss: 13.9685 (GAN: 0.9243, L1: 4.4734, Perceptual: 8.5708)\n",
      "Epoch [10/50], Batch [240/250] | D Loss: 0.6128 | G Loss: 12.3303 (GAN: 0.9184, L1: 3.9460, Perceptual: 7.4658)\n",
      "Epoch [10/50], Batch [240/250] | D Loss: 0.6128 | G Loss: 12.3303 (GAN: 0.9184, L1: 3.9460, Perceptual: 7.4658)\n",
      "Epoch [10/50], Batch [250/250] | D Loss: 0.6248 | G Loss: 12.6404 (GAN: 0.8663, L1: 4.1244, Perceptual: 7.6497)\n",
      "Epoch 10 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 10 ---\n",
      "Saving samples and checkpoint for epoch 10...\n",
      "Epoch [10/50], Batch [250/250] | D Loss: 0.6248 | G Loss: 12.6404 (GAN: 0.8663, L1: 4.1244, Perceptual: 7.6497)\n",
      "Epoch 10 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 10 ---\n",
      "Saving samples and checkpoint for epoch 10...\n",
      "Successfully saved image to: output/generated_images\\epoch_10_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_10_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [11/50], Batch [10/250] | D Loss: 0.6506 | G Loss: 11.8038 (GAN: 0.9069, L1: 3.5495, Perceptual: 7.3474)\n",
      "Epoch [11/50], Batch [10/250] | D Loss: 0.6506 | G Loss: 11.8038 (GAN: 0.9069, L1: 3.5495, Perceptual: 7.3474)\n",
      "Epoch [11/50], Batch [20/250] | D Loss: 0.5381 | G Loss: 13.0527 (GAN: 0.8025, L1: 4.2060, Perceptual: 8.0442)\n",
      "Epoch [11/50], Batch [20/250] | D Loss: 0.5381 | G Loss: 13.0527 (GAN: 0.8025, L1: 4.2060, Perceptual: 8.0442)\n",
      "Epoch [11/50], Batch [30/250] | D Loss: 0.6036 | G Loss: 12.8200 (GAN: 0.7080, L1: 4.3045, Perceptual: 7.8074)\n",
      "Epoch [11/50], Batch [30/250] | D Loss: 0.6036 | G Loss: 12.8200 (GAN: 0.7080, L1: 4.3045, Perceptual: 7.8074)\n",
      "Epoch [11/50], Batch [40/250] | D Loss: 0.5582 | G Loss: 12.4161 (GAN: 0.8737, L1: 3.6083, Perceptual: 7.9341)\n",
      "Epoch [11/50], Batch [40/250] | D Loss: 0.5582 | G Loss: 12.4161 (GAN: 0.8737, L1: 3.6083, Perceptual: 7.9341)\n",
      "Epoch [11/50], Batch [50/250] | D Loss: 0.7515 | G Loss: 12.5688 (GAN: 1.0684, L1: 3.9125, Perceptual: 7.5879)\n",
      "Epoch [11/50], Batch [50/250] | D Loss: 0.7515 | G Loss: 12.5688 (GAN: 1.0684, L1: 3.9125, Perceptual: 7.5879)\n",
      "Epoch [11/50], Batch [60/250] | D Loss: 0.6148 | G Loss: 12.3501 (GAN: 0.9540, L1: 3.7013, Perceptual: 7.6947)\n",
      "Epoch [11/50], Batch [60/250] | D Loss: 0.6148 | G Loss: 12.3501 (GAN: 0.9540, L1: 3.7013, Perceptual: 7.6947)\n",
      "Epoch [11/50], Batch [70/250] | D Loss: 0.6978 | G Loss: 11.9510 (GAN: 0.7755, L1: 3.9011, Perceptual: 7.2743)\n",
      "Epoch [11/50], Batch [70/250] | D Loss: 0.6978 | G Loss: 11.9510 (GAN: 0.7755, L1: 3.9011, Perceptual: 7.2743)\n",
      "Epoch [11/50], Batch [80/250] | D Loss: 0.5701 | G Loss: 13.1890 (GAN: 0.8076, L1: 4.3160, Perceptual: 8.0654)\n",
      "Epoch [11/50], Batch [80/250] | D Loss: 0.5701 | G Loss: 13.1890 (GAN: 0.8076, L1: 4.3160, Perceptual: 8.0654)\n",
      "Epoch [11/50], Batch [90/250] | D Loss: 0.7518 | G Loss: 11.6548 (GAN: 0.7228, L1: 3.4366, Perceptual: 7.4955)\n",
      "Epoch [11/50], Batch [90/250] | D Loss: 0.7518 | G Loss: 11.6548 (GAN: 0.7228, L1: 3.4366, Perceptual: 7.4955)\n",
      "Epoch [11/50], Batch [100/250] | D Loss: 0.5832 | G Loss: 12.1579 (GAN: 0.8043, L1: 3.6614, Perceptual: 7.6922)\n",
      "Epoch [11/50], Batch [100/250] | D Loss: 0.5832 | G Loss: 12.1579 (GAN: 0.8043, L1: 3.6614, Perceptual: 7.6922)\n",
      "Epoch [11/50], Batch [110/250] | D Loss: 0.6827 | G Loss: 12.4697 (GAN: 1.0882, L1: 3.9026, Perceptual: 7.4788)\n",
      "Epoch [11/50], Batch [110/250] | D Loss: 0.6827 | G Loss: 12.4697 (GAN: 1.0882, L1: 3.9026, Perceptual: 7.4788)\n",
      "Epoch [11/50], Batch [120/250] | D Loss: 0.7467 | G Loss: 10.5633 (GAN: 0.7176, L1: 3.0849, Perceptual: 6.7609)\n",
      "Epoch [11/50], Batch [120/250] | D Loss: 0.7467 | G Loss: 10.5633 (GAN: 0.7176, L1: 3.0849, Perceptual: 6.7609)\n",
      "Epoch [11/50], Batch [130/250] | D Loss: 0.6119 | G Loss: 11.4412 (GAN: 0.7202, L1: 3.5254, Perceptual: 7.1956)\n",
      "Epoch [11/50], Batch [130/250] | D Loss: 0.6119 | G Loss: 11.4412 (GAN: 0.7202, L1: 3.5254, Perceptual: 7.1956)\n",
      "Epoch [11/50], Batch [140/250] | D Loss: 0.5897 | G Loss: 11.6434 (GAN: 0.5733, L1: 3.8588, Perceptual: 7.2113)\n",
      "Epoch [11/50], Batch [140/250] | D Loss: 0.5897 | G Loss: 11.6434 (GAN: 0.5733, L1: 3.8588, Perceptual: 7.2113)\n",
      "Epoch [11/50], Batch [150/250] | D Loss: 0.6753 | G Loss: 10.8136 (GAN: 0.7765, L1: 3.1339, Perceptual: 6.9032)\n",
      "Epoch [11/50], Batch [150/250] | D Loss: 0.6753 | G Loss: 10.8136 (GAN: 0.7765, L1: 3.1339, Perceptual: 6.9032)\n",
      "Epoch [11/50], Batch [160/250] | D Loss: 0.7950 | G Loss: 11.2080 (GAN: 0.8937, L1: 3.2439, Perceptual: 7.0705)\n",
      "Epoch [11/50], Batch [160/250] | D Loss: 0.7950 | G Loss: 11.2080 (GAN: 0.8937, L1: 3.2439, Perceptual: 7.0705)\n",
      "Epoch [11/50], Batch [170/250] | D Loss: 0.6177 | G Loss: 10.8704 (GAN: 0.7139, L1: 3.1254, Perceptual: 7.0311)\n",
      "Epoch [11/50], Batch [170/250] | D Loss: 0.6177 | G Loss: 10.8704 (GAN: 0.7139, L1: 3.1254, Perceptual: 7.0311)\n",
      "Epoch [11/50], Batch [180/250] | D Loss: 0.6239 | G Loss: 11.3215 (GAN: 0.7205, L1: 3.1158, Perceptual: 7.4853)\n",
      "Epoch [11/50], Batch [180/250] | D Loss: 0.6239 | G Loss: 11.3215 (GAN: 0.7205, L1: 3.1158, Perceptual: 7.4853)\n",
      "Epoch [11/50], Batch [190/250] | D Loss: 0.4396 | G Loss: 13.0429 (GAN: 0.7214, L1: 4.0550, Perceptual: 8.2665)\n",
      "Epoch [11/50], Batch [190/250] | D Loss: 0.4396 | G Loss: 13.0429 (GAN: 0.7214, L1: 4.0550, Perceptual: 8.2665)\n",
      "Epoch [11/50], Batch [200/250] | D Loss: 0.6085 | G Loss: 12.3493 (GAN: 0.5916, L1: 4.0138, Perceptual: 7.7440)\n",
      "Epoch [11/50], Batch [200/250] | D Loss: 0.6085 | G Loss: 12.3493 (GAN: 0.5916, L1: 4.0138, Perceptual: 7.7440)\n",
      "Epoch [11/50], Batch [210/250] | D Loss: 0.6141 | G Loss: 13.0574 (GAN: 0.7604, L1: 4.1630, Perceptual: 8.1339)\n",
      "Epoch [11/50], Batch [210/250] | D Loss: 0.6141 | G Loss: 13.0574 (GAN: 0.7604, L1: 4.1630, Perceptual: 8.1339)\n",
      "Epoch [11/50], Batch [220/250] | D Loss: 0.5980 | G Loss: 12.0252 (GAN: 0.9546, L1: 3.6561, Perceptual: 7.4145)\n",
      "Epoch [11/50], Batch [220/250] | D Loss: 0.5980 | G Loss: 12.0252 (GAN: 0.9546, L1: 3.6561, Perceptual: 7.4145)\n",
      "Epoch [11/50], Batch [230/250] | D Loss: 0.7406 | G Loss: 11.5759 (GAN: 1.0215, L1: 3.3836, Perceptual: 7.1708)\n",
      "Epoch [11/50], Batch [230/250] | D Loss: 0.7406 | G Loss: 11.5759 (GAN: 1.0215, L1: 3.3836, Perceptual: 7.1708)\n",
      "Epoch [11/50], Batch [240/250] | D Loss: 0.7531 | G Loss: 11.3333 (GAN: 0.6907, L1: 3.1034, Perceptual: 7.5391)\n",
      "Epoch [11/50], Batch [240/250] | D Loss: 0.7531 | G Loss: 11.3333 (GAN: 0.6907, L1: 3.1034, Perceptual: 7.5391)\n",
      "Epoch [11/50], Batch [250/250] | D Loss: 0.7354 | G Loss: 11.9272 (GAN: 0.9655, L1: 3.5068, Perceptual: 7.4549)\n",
      "Epoch 11 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [11/50], Batch [250/250] | D Loss: 0.7354 | G Loss: 11.9272 (GAN: 0.9655, L1: 3.5068, Perceptual: 7.4549)\n",
      "Epoch 11 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [12/50], Batch [10/250] | D Loss: 0.5771 | G Loss: 11.3058 (GAN: 0.7675, L1: 3.2869, Perceptual: 7.2513)\n",
      "Epoch [12/50], Batch [10/250] | D Loss: 0.5771 | G Loss: 11.3058 (GAN: 0.7675, L1: 3.2869, Perceptual: 7.2513)\n",
      "Epoch [12/50], Batch [20/250] | D Loss: 0.7938 | G Loss: 11.8515 (GAN: 1.0641, L1: 3.3153, Perceptual: 7.4721)\n",
      "Epoch [12/50], Batch [20/250] | D Loss: 0.7938 | G Loss: 11.8515 (GAN: 1.0641, L1: 3.3153, Perceptual: 7.4721)\n",
      "Epoch [12/50], Batch [30/250] | D Loss: 0.5840 | G Loss: 12.5292 (GAN: 0.9113, L1: 3.7997, Perceptual: 7.8182)\n",
      "Epoch [12/50], Batch [30/250] | D Loss: 0.5840 | G Loss: 12.5292 (GAN: 0.9113, L1: 3.7997, Perceptual: 7.8182)\n",
      "Epoch [12/50], Batch [40/250] | D Loss: 0.6960 | G Loss: 11.7541 (GAN: 0.7798, L1: 3.4395, Perceptual: 7.5349)\n",
      "Epoch [12/50], Batch [40/250] | D Loss: 0.6960 | G Loss: 11.7541 (GAN: 0.7798, L1: 3.4395, Perceptual: 7.5349)\n",
      "Epoch [12/50], Batch [50/250] | D Loss: 0.7394 | G Loss: 12.9051 (GAN: 0.8862, L1: 4.1618, Perceptual: 7.8570)\n",
      "Epoch [12/50], Batch [50/250] | D Loss: 0.7394 | G Loss: 12.9051 (GAN: 0.8862, L1: 4.1618, Perceptual: 7.8570)\n",
      "Epoch [12/50], Batch [60/250] | D Loss: 0.5957 | G Loss: 12.3865 (GAN: 0.5775, L1: 4.1219, Perceptual: 7.6871)\n",
      "Epoch [12/50], Batch [60/250] | D Loss: 0.5957 | G Loss: 12.3865 (GAN: 0.5775, L1: 4.1219, Perceptual: 7.6871)\n",
      "Epoch [12/50], Batch [70/250] | D Loss: 0.6968 | G Loss: 12.2789 (GAN: 0.5509, L1: 3.8421, Perceptual: 7.8859)\n",
      "Epoch [12/50], Batch [70/250] | D Loss: 0.6968 | G Loss: 12.2789 (GAN: 0.5509, L1: 3.8421, Perceptual: 7.8859)\n",
      "Epoch [12/50], Batch [80/250] | D Loss: 0.6979 | G Loss: 13.4701 (GAN: 0.9187, L1: 4.5187, Perceptual: 8.0327)\n",
      "Epoch [12/50], Batch [80/250] | D Loss: 0.6979 | G Loss: 13.4701 (GAN: 0.9187, L1: 4.5187, Perceptual: 8.0327)\n",
      "Epoch [12/50], Batch [90/250] | D Loss: 0.6036 | G Loss: 11.5565 (GAN: 0.7385, L1: 3.6201, Perceptual: 7.1979)\n",
      "Epoch [12/50], Batch [90/250] | D Loss: 0.6036 | G Loss: 11.5565 (GAN: 0.7385, L1: 3.6201, Perceptual: 7.1979)\n",
      "Epoch [12/50], Batch [100/250] | D Loss: 0.6994 | G Loss: 11.3099 (GAN: 0.6664, L1: 3.5831, Perceptual: 7.0603)\n",
      "Epoch [12/50], Batch [100/250] | D Loss: 0.6994 | G Loss: 11.3099 (GAN: 0.6664, L1: 3.5831, Perceptual: 7.0603)\n",
      "Epoch [12/50], Batch [110/250] | D Loss: 0.4317 | G Loss: 14.2863 (GAN: 1.1147, L1: 5.2778, Perceptual: 7.8938)\n",
      "Epoch [12/50], Batch [110/250] | D Loss: 0.4317 | G Loss: 14.2863 (GAN: 1.1147, L1: 5.2778, Perceptual: 7.8938)\n",
      "Epoch [12/50], Batch [120/250] | D Loss: 0.6366 | G Loss: 12.6301 (GAN: 0.7534, L1: 4.0936, Perceptual: 7.7831)\n",
      "Epoch [12/50], Batch [120/250] | D Loss: 0.6366 | G Loss: 12.6301 (GAN: 0.7534, L1: 4.0936, Perceptual: 7.7831)\n",
      "Epoch [12/50], Batch [130/250] | D Loss: 0.7727 | G Loss: 10.7192 (GAN: 0.9848, L1: 3.0606, Perceptual: 6.6738)\n",
      "Epoch [12/50], Batch [130/250] | D Loss: 0.7727 | G Loss: 10.7192 (GAN: 0.9848, L1: 3.0606, Perceptual: 6.6738)\n",
      "Epoch [12/50], Batch [140/250] | D Loss: 0.4422 | G Loss: 11.6023 (GAN: 0.7949, L1: 3.4842, Perceptual: 7.3231)\n",
      "Epoch [12/50], Batch [140/250] | D Loss: 0.4422 | G Loss: 11.6023 (GAN: 0.7949, L1: 3.4842, Perceptual: 7.3231)\n",
      "Epoch [12/50], Batch [150/250] | D Loss: 0.7991 | G Loss: 13.0794 (GAN: 0.9523, L1: 3.9754, Perceptual: 8.1517)\n",
      "Epoch [12/50], Batch [150/250] | D Loss: 0.7991 | G Loss: 13.0794 (GAN: 0.9523, L1: 3.9754, Perceptual: 8.1517)\n",
      "Epoch [12/50], Batch [160/250] | D Loss: 0.6487 | G Loss: 11.0312 (GAN: 0.8245, L1: 3.1940, Perceptual: 7.0127)\n",
      "Epoch [12/50], Batch [160/250] | D Loss: 0.6487 | G Loss: 11.0312 (GAN: 0.8245, L1: 3.1940, Perceptual: 7.0127)\n",
      "Epoch [12/50], Batch [170/250] | D Loss: 0.5699 | G Loss: 12.1806 (GAN: 0.7150, L1: 3.8981, Perceptual: 7.5675)\n",
      "Epoch [12/50], Batch [170/250] | D Loss: 0.5699 | G Loss: 12.1806 (GAN: 0.7150, L1: 3.8981, Perceptual: 7.5675)\n",
      "Epoch [12/50], Batch [180/250] | D Loss: 0.5655 | G Loss: 12.3412 (GAN: 0.7252, L1: 3.7942, Perceptual: 7.8217)\n",
      "Epoch [12/50], Batch [180/250] | D Loss: 0.5655 | G Loss: 12.3412 (GAN: 0.7252, L1: 3.7942, Perceptual: 7.8217)\n",
      "Epoch [12/50], Batch [190/250] | D Loss: 0.4380 | G Loss: 12.9001 (GAN: 0.7853, L1: 4.1007, Perceptual: 8.0140)\n",
      "Epoch [12/50], Batch [190/250] | D Loss: 0.4380 | G Loss: 12.9001 (GAN: 0.7853, L1: 4.1007, Perceptual: 8.0140)\n",
      "Epoch [12/50], Batch [200/250] | D Loss: 0.3383 | G Loss: 12.9289 (GAN: 0.7127, L1: 3.8986, Perceptual: 8.3177)\n",
      "Epoch [12/50], Batch [200/250] | D Loss: 0.3383 | G Loss: 12.9289 (GAN: 0.7127, L1: 3.8986, Perceptual: 8.3177)\n",
      "Epoch [12/50], Batch [210/250] | D Loss: 0.6511 | G Loss: 13.2098 (GAN: 1.2121, L1: 4.2055, Perceptual: 7.7922)\n",
      "Epoch [12/50], Batch [210/250] | D Loss: 0.6511 | G Loss: 13.2098 (GAN: 1.2121, L1: 4.2055, Perceptual: 7.7922)\n",
      "Epoch [12/50], Batch [220/250] | D Loss: 0.7492 | G Loss: 11.5992 (GAN: 0.8872, L1: 3.2790, Perceptual: 7.4330)\n",
      "Epoch [12/50], Batch [220/250] | D Loss: 0.7492 | G Loss: 11.5992 (GAN: 0.8872, L1: 3.2790, Perceptual: 7.4330)\n",
      "Epoch [12/50], Batch [230/250] | D Loss: 0.5353 | G Loss: 11.4594 (GAN: 0.6609, L1: 3.2329, Perceptual: 7.5656)\n",
      "Epoch [12/50], Batch [230/250] | D Loss: 0.5353 | G Loss: 11.4594 (GAN: 0.6609, L1: 3.2329, Perceptual: 7.5656)\n",
      "Epoch [12/50], Batch [240/250] | D Loss: 0.6627 | G Loss: 12.6647 (GAN: 0.8038, L1: 3.7055, Perceptual: 8.1554)\n",
      "Epoch [12/50], Batch [240/250] | D Loss: 0.6627 | G Loss: 12.6647 (GAN: 0.8038, L1: 3.7055, Perceptual: 8.1554)\n",
      "Epoch [12/50], Batch [250/250] | D Loss: 0.5501 | G Loss: 11.9469 (GAN: 0.8330, L1: 3.6284, Perceptual: 7.4855)\n",
      "Epoch 12 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [12/50], Batch [250/250] | D Loss: 0.5501 | G Loss: 11.9469 (GAN: 0.8330, L1: 3.6284, Perceptual: 7.4855)\n",
      "Epoch 12 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [13/50], Batch [10/250] | D Loss: 0.6022 | G Loss: 11.7795 (GAN: 0.9004, L1: 3.4669, Perceptual: 7.4123)\n",
      "Epoch [13/50], Batch [10/250] | D Loss: 0.6022 | G Loss: 11.7795 (GAN: 0.9004, L1: 3.4669, Perceptual: 7.4123)\n",
      "Epoch [13/50], Batch [20/250] | D Loss: 0.6136 | G Loss: 13.4810 (GAN: 1.0093, L1: 4.1407, Perceptual: 8.3310)\n",
      "Epoch [13/50], Batch [20/250] | D Loss: 0.6136 | G Loss: 13.4810 (GAN: 1.0093, L1: 4.1407, Perceptual: 8.3310)\n",
      "Epoch [13/50], Batch [30/250] | D Loss: 0.5468 | G Loss: 12.2172 (GAN: 0.7965, L1: 3.4377, Perceptual: 7.9830)\n",
      "Epoch [13/50], Batch [30/250] | D Loss: 0.5468 | G Loss: 12.2172 (GAN: 0.7965, L1: 3.4377, Perceptual: 7.9830)\n",
      "Epoch [13/50], Batch [40/250] | D Loss: 0.5326 | G Loss: 12.1339 (GAN: 0.9227, L1: 3.5011, Perceptual: 7.7102)\n",
      "Epoch [13/50], Batch [40/250] | D Loss: 0.5326 | G Loss: 12.1339 (GAN: 0.9227, L1: 3.5011, Perceptual: 7.7102)\n",
      "Epoch [13/50], Batch [50/250] | D Loss: 0.9035 | G Loss: 11.8009 (GAN: 0.8348, L1: 3.6386, Perceptual: 7.3275)\n",
      "Epoch [13/50], Batch [50/250] | D Loss: 0.9035 | G Loss: 11.8009 (GAN: 0.8348, L1: 3.6386, Perceptual: 7.3275)\n",
      "Epoch [13/50], Batch [60/250] | D Loss: 0.5525 | G Loss: 11.8401 (GAN: 0.9293, L1: 3.3972, Perceptual: 7.5136)\n",
      "Epoch [13/50], Batch [60/250] | D Loss: 0.5525 | G Loss: 11.8401 (GAN: 0.9293, L1: 3.3972, Perceptual: 7.5136)\n",
      "Epoch [13/50], Batch [70/250] | D Loss: 0.7344 | G Loss: 12.7224 (GAN: 0.8808, L1: 3.8818, Perceptual: 7.9598)\n",
      "Epoch [13/50], Batch [70/250] | D Loss: 0.7344 | G Loss: 12.7224 (GAN: 0.8808, L1: 3.8818, Perceptual: 7.9598)\n",
      "Epoch [13/50], Batch [80/250] | D Loss: 0.6705 | G Loss: 11.6613 (GAN: 0.6740, L1: 3.5043, Perceptual: 7.4830)\n",
      "Epoch [13/50], Batch [80/250] | D Loss: 0.6705 | G Loss: 11.6613 (GAN: 0.6740, L1: 3.5043, Perceptual: 7.4830)\n",
      "Epoch [13/50], Batch [90/250] | D Loss: 0.9860 | G Loss: 11.0881 (GAN: 0.7679, L1: 3.3730, Perceptual: 6.9472)\n",
      "Epoch [13/50], Batch [90/250] | D Loss: 0.9860 | G Loss: 11.0881 (GAN: 0.7679, L1: 3.3730, Perceptual: 6.9472)\n",
      "Epoch [13/50], Batch [100/250] | D Loss: 0.7538 | G Loss: 13.1018 (GAN: 0.9096, L1: 4.1006, Perceptual: 8.0916)\n",
      "Epoch [13/50], Batch [100/250] | D Loss: 0.7538 | G Loss: 13.1018 (GAN: 0.9096, L1: 4.1006, Perceptual: 8.0916)\n",
      "Epoch [13/50], Batch [110/250] | D Loss: 0.5397 | G Loss: 11.7026 (GAN: 0.7968, L1: 3.3774, Perceptual: 7.5284)\n",
      "Epoch [13/50], Batch [110/250] | D Loss: 0.5397 | G Loss: 11.7026 (GAN: 0.7968, L1: 3.3774, Perceptual: 7.5284)\n",
      "Epoch [13/50], Batch [120/250] | D Loss: 0.5692 | G Loss: 11.6670 (GAN: 0.7694, L1: 3.5482, Perceptual: 7.3494)\n",
      "Epoch [13/50], Batch [120/250] | D Loss: 0.5692 | G Loss: 11.6670 (GAN: 0.7694, L1: 3.5482, Perceptual: 7.3494)\n",
      "Epoch [13/50], Batch [130/250] | D Loss: 0.5919 | G Loss: 13.3882 (GAN: 0.8784, L1: 4.2264, Perceptual: 8.2833)\n",
      "Epoch [13/50], Batch [130/250] | D Loss: 0.5919 | G Loss: 13.3882 (GAN: 0.8784, L1: 4.2264, Perceptual: 8.2833)\n",
      "Epoch [13/50], Batch [140/250] | D Loss: 0.6107 | G Loss: 11.7758 (GAN: 0.6096, L1: 3.6804, Perceptual: 7.4857)\n",
      "Epoch [13/50], Batch [140/250] | D Loss: 0.6107 | G Loss: 11.7758 (GAN: 0.6096, L1: 3.6804, Perceptual: 7.4857)\n",
      "Epoch [13/50], Batch [150/250] | D Loss: 0.7858 | G Loss: 11.5225 (GAN: 0.6765, L1: 3.6146, Perceptual: 7.2315)\n",
      "Epoch [13/50], Batch [150/250] | D Loss: 0.7858 | G Loss: 11.5225 (GAN: 0.6765, L1: 3.6146, Perceptual: 7.2315)\n",
      "Epoch [13/50], Batch [160/250] | D Loss: 0.6251 | G Loss: 11.4699 (GAN: 0.5381, L1: 3.4913, Perceptual: 7.4406)\n",
      "Epoch [13/50], Batch [160/250] | D Loss: 0.6251 | G Loss: 11.4699 (GAN: 0.5381, L1: 3.4913, Perceptual: 7.4406)\n",
      "Epoch [13/50], Batch [170/250] | D Loss: 0.6174 | G Loss: 12.1638 (GAN: 0.6697, L1: 3.9623, Perceptual: 7.5319)\n",
      "Epoch [13/50], Batch [170/250] | D Loss: 0.6174 | G Loss: 12.1638 (GAN: 0.6697, L1: 3.9623, Perceptual: 7.5319)\n",
      "Epoch [13/50], Batch [180/250] | D Loss: 0.6013 | G Loss: 11.8425 (GAN: 0.6875, L1: 3.8378, Perceptual: 7.3173)\n",
      "Epoch [13/50], Batch [180/250] | D Loss: 0.6013 | G Loss: 11.8425 (GAN: 0.6875, L1: 3.8378, Perceptual: 7.3173)\n",
      "Epoch [13/50], Batch [190/250] | D Loss: 0.5346 | G Loss: 12.3571 (GAN: 0.7887, L1: 4.1365, Perceptual: 7.4319)\n",
      "Epoch [13/50], Batch [190/250] | D Loss: 0.5346 | G Loss: 12.3571 (GAN: 0.7887, L1: 4.1365, Perceptual: 7.4319)\n",
      "Epoch [13/50], Batch [200/250] | D Loss: 0.5726 | G Loss: 11.6400 (GAN: 0.7082, L1: 3.6566, Perceptual: 7.2753)\n",
      "Epoch [13/50], Batch [200/250] | D Loss: 0.5726 | G Loss: 11.6400 (GAN: 0.7082, L1: 3.6566, Perceptual: 7.2753)\n",
      "Epoch [13/50], Batch [210/250] | D Loss: 0.5943 | G Loss: 11.6944 (GAN: 0.9581, L1: 3.6202, Perceptual: 7.1161)\n",
      "Epoch [13/50], Batch [210/250] | D Loss: 0.5943 | G Loss: 11.6944 (GAN: 0.9581, L1: 3.6202, Perceptual: 7.1161)\n",
      "Epoch [13/50], Batch [220/250] | D Loss: 0.5800 | G Loss: 11.8203 (GAN: 0.8009, L1: 3.6550, Perceptual: 7.3644)\n",
      "Epoch [13/50], Batch [220/250] | D Loss: 0.5800 | G Loss: 11.8203 (GAN: 0.8009, L1: 3.6550, Perceptual: 7.3644)\n",
      "Epoch [13/50], Batch [230/250] | D Loss: 0.6244 | G Loss: 11.8972 (GAN: 0.7118, L1: 3.7474, Perceptual: 7.4380)\n",
      "Epoch [13/50], Batch [230/250] | D Loss: 0.6244 | G Loss: 11.8972 (GAN: 0.7118, L1: 3.7474, Perceptual: 7.4380)\n",
      "Epoch [13/50], Batch [240/250] | D Loss: 0.5574 | G Loss: 14.1531 (GAN: 1.0376, L1: 4.3688, Perceptual: 8.7467)\n",
      "Epoch [13/50], Batch [240/250] | D Loss: 0.5574 | G Loss: 14.1531 (GAN: 1.0376, L1: 4.3688, Perceptual: 8.7467)\n",
      "Epoch [13/50], Batch [250/250] | D Loss: 0.6127 | G Loss: 11.9161 (GAN: 0.7253, L1: 3.6655, Perceptual: 7.5254)\n",
      "Epoch 13 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [13/50], Batch [250/250] | D Loss: 0.6127 | G Loss: 11.9161 (GAN: 0.7253, L1: 3.6655, Perceptual: 7.5254)\n",
      "Epoch 13 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [14/50], Batch [10/250] | D Loss: 0.5133 | G Loss: 11.8253 (GAN: 0.6745, L1: 3.9446, Perceptual: 7.2063)\n",
      "Epoch [14/50], Batch [10/250] | D Loss: 0.5133 | G Loss: 11.8253 (GAN: 0.6745, L1: 3.9446, Perceptual: 7.2063)\n",
      "Epoch [14/50], Batch [20/250] | D Loss: 0.5233 | G Loss: 12.8786 (GAN: 0.8013, L1: 4.0294, Perceptual: 8.0479)\n",
      "Epoch [14/50], Batch [20/250] | D Loss: 0.5233 | G Loss: 12.8786 (GAN: 0.8013, L1: 4.0294, Perceptual: 8.0479)\n",
      "Epoch [14/50], Batch [30/250] | D Loss: 0.5769 | G Loss: 11.3553 (GAN: 0.8320, L1: 3.2889, Perceptual: 7.2344)\n",
      "Epoch [14/50], Batch [30/250] | D Loss: 0.5769 | G Loss: 11.3553 (GAN: 0.8320, L1: 3.2889, Perceptual: 7.2344)\n",
      "Epoch [14/50], Batch [40/250] | D Loss: 0.5225 | G Loss: 12.7484 (GAN: 0.8231, L1: 4.0410, Perceptual: 7.8843)\n",
      "Epoch [14/50], Batch [40/250] | D Loss: 0.5225 | G Loss: 12.7484 (GAN: 0.8231, L1: 4.0410, Perceptual: 7.8843)\n",
      "Epoch [14/50], Batch [50/250] | D Loss: 0.8325 | G Loss: 10.8860 (GAN: 0.7497, L1: 3.5630, Perceptual: 6.5733)\n",
      "Epoch [14/50], Batch [50/250] | D Loss: 0.8325 | G Loss: 10.8860 (GAN: 0.7497, L1: 3.5630, Perceptual: 6.5733)\n",
      "Epoch [14/50], Batch [60/250] | D Loss: 0.5645 | G Loss: 10.9141 (GAN: 0.5872, L1: 3.3151, Perceptual: 7.0118)\n",
      "Epoch [14/50], Batch [60/250] | D Loss: 0.5645 | G Loss: 10.9141 (GAN: 0.5872, L1: 3.3151, Perceptual: 7.0118)\n",
      "Epoch [14/50], Batch [70/250] | D Loss: 0.6169 | G Loss: 12.6455 (GAN: 1.1792, L1: 3.7214, Perceptual: 7.7449)\n",
      "Epoch [14/50], Batch [70/250] | D Loss: 0.6169 | G Loss: 12.6455 (GAN: 1.1792, L1: 3.7214, Perceptual: 7.7449)\n",
      "Epoch [14/50], Batch [80/250] | D Loss: 0.7191 | G Loss: 11.8876 (GAN: 0.7199, L1: 3.6922, Perceptual: 7.4755)\n",
      "Epoch [14/50], Batch [80/250] | D Loss: 0.7191 | G Loss: 11.8876 (GAN: 0.7199, L1: 3.6922, Perceptual: 7.4755)\n",
      "Epoch [14/50], Batch [90/250] | D Loss: 0.7134 | G Loss: 11.3682 (GAN: 0.8130, L1: 3.1979, Perceptual: 7.3573)\n",
      "Epoch [14/50], Batch [90/250] | D Loss: 0.7134 | G Loss: 11.3682 (GAN: 0.8130, L1: 3.1979, Perceptual: 7.3573)\n",
      "Epoch [14/50], Batch [100/250] | D Loss: 0.4621 | G Loss: 13.3111 (GAN: 0.7487, L1: 4.4024, Perceptual: 8.1600)\n",
      "Epoch [14/50], Batch [100/250] | D Loss: 0.4621 | G Loss: 13.3111 (GAN: 0.7487, L1: 4.4024, Perceptual: 8.1600)\n",
      "Epoch [14/50], Batch [110/250] | D Loss: 0.8453 | G Loss: 11.9446 (GAN: 0.8267, L1: 4.0375, Perceptual: 7.0805)\n",
      "Epoch [14/50], Batch [110/250] | D Loss: 0.8453 | G Loss: 11.9446 (GAN: 0.8267, L1: 4.0375, Perceptual: 7.0805)\n",
      "Epoch [14/50], Batch [120/250] | D Loss: 0.6475 | G Loss: 11.0325 (GAN: 0.6239, L1: 3.2783, Perceptual: 7.1304)\n",
      "Epoch [14/50], Batch [120/250] | D Loss: 0.6475 | G Loss: 11.0325 (GAN: 0.6239, L1: 3.2783, Perceptual: 7.1304)\n",
      "Epoch [14/50], Batch [130/250] | D Loss: 0.6066 | G Loss: 12.3248 (GAN: 0.8204, L1: 3.8103, Perceptual: 7.6941)\n",
      "Epoch [14/50], Batch [130/250] | D Loss: 0.6066 | G Loss: 12.3248 (GAN: 0.8204, L1: 3.8103, Perceptual: 7.6941)\n",
      "Epoch [14/50], Batch [140/250] | D Loss: 0.6557 | G Loss: 11.4299 (GAN: 0.7742, L1: 3.4186, Perceptual: 7.2371)\n",
      "Epoch [14/50], Batch [140/250] | D Loss: 0.6557 | G Loss: 11.4299 (GAN: 0.7742, L1: 3.4186, Perceptual: 7.2371)\n",
      "Epoch [14/50], Batch [150/250] | D Loss: 0.5550 | G Loss: 11.8710 (GAN: 0.7869, L1: 3.6404, Perceptual: 7.4437)\n",
      "Epoch [14/50], Batch [150/250] | D Loss: 0.5550 | G Loss: 11.8710 (GAN: 0.7869, L1: 3.6404, Perceptual: 7.4437)\n",
      "Epoch [14/50], Batch [160/250] | D Loss: 0.4931 | G Loss: 12.3160 (GAN: 0.8514, L1: 3.6847, Perceptual: 7.7798)\n",
      "Epoch [14/50], Batch [160/250] | D Loss: 0.4931 | G Loss: 12.3160 (GAN: 0.8514, L1: 3.6847, Perceptual: 7.7798)\n",
      "Epoch [14/50], Batch [170/250] | D Loss: 0.5898 | G Loss: 11.1278 (GAN: 0.7656, L1: 3.2825, Perceptual: 7.0797)\n",
      "Epoch [14/50], Batch [170/250] | D Loss: 0.5898 | G Loss: 11.1278 (GAN: 0.7656, L1: 3.2825, Perceptual: 7.0797)\n",
      "Epoch [14/50], Batch [180/250] | D Loss: 0.9083 | G Loss: 11.6021 (GAN: 1.1009, L1: 3.3817, Perceptual: 7.1195)\n",
      "Epoch [14/50], Batch [180/250] | D Loss: 0.9083 | G Loss: 11.6021 (GAN: 1.1009, L1: 3.3817, Perceptual: 7.1195)\n",
      "Epoch [14/50], Batch [190/250] | D Loss: 0.5236 | G Loss: 11.3312 (GAN: 0.7132, L1: 3.4103, Perceptual: 7.2077)\n",
      "Epoch [14/50], Batch [190/250] | D Loss: 0.5236 | G Loss: 11.3312 (GAN: 0.7132, L1: 3.4103, Perceptual: 7.2077)\n",
      "Epoch [14/50], Batch [200/250] | D Loss: 0.5220 | G Loss: 11.5183 (GAN: 0.8109, L1: 3.6353, Perceptual: 7.0721)\n",
      "Epoch [14/50], Batch [200/250] | D Loss: 0.5220 | G Loss: 11.5183 (GAN: 0.8109, L1: 3.6353, Perceptual: 7.0721)\n",
      "Epoch [14/50], Batch [210/250] | D Loss: 0.5169 | G Loss: 12.9957 (GAN: 0.9910, L1: 4.1461, Perceptual: 7.8586)\n",
      "Epoch [14/50], Batch [210/250] | D Loss: 0.5169 | G Loss: 12.9957 (GAN: 0.9910, L1: 4.1461, Perceptual: 7.8586)\n",
      "Epoch [14/50], Batch [220/250] | D Loss: 0.6222 | G Loss: 12.2164 (GAN: 0.9136, L1: 3.5869, Perceptual: 7.7160)\n",
      "Epoch [14/50], Batch [220/250] | D Loss: 0.6222 | G Loss: 12.2164 (GAN: 0.9136, L1: 3.5869, Perceptual: 7.7160)\n",
      "Epoch [14/50], Batch [230/250] | D Loss: 0.6202 | G Loss: 11.2866 (GAN: 0.9319, L1: 3.2004, Perceptual: 7.1544)\n",
      "Epoch [14/50], Batch [230/250] | D Loss: 0.6202 | G Loss: 11.2866 (GAN: 0.9319, L1: 3.2004, Perceptual: 7.1544)\n",
      "Epoch [14/50], Batch [240/250] | D Loss: 0.5549 | G Loss: 11.7449 (GAN: 0.7765, L1: 3.4674, Perceptual: 7.5011)\n",
      "Epoch [14/50], Batch [240/250] | D Loss: 0.5549 | G Loss: 11.7449 (GAN: 0.7765, L1: 3.4674, Perceptual: 7.5011)\n",
      "Epoch [14/50], Batch [250/250] | D Loss: 0.5354 | G Loss: 13.4591 (GAN: 1.1235, L1: 4.3873, Perceptual: 7.9483)\n",
      "Epoch 14 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [14/50], Batch [250/250] | D Loss: 0.5354 | G Loss: 13.4591 (GAN: 1.1235, L1: 4.3873, Perceptual: 7.9483)\n",
      "Epoch 14 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [15/50], Batch [10/250] | D Loss: 0.7156 | G Loss: 12.1281 (GAN: 1.2035, L1: 3.4388, Perceptual: 7.4858)\n",
      "Epoch [15/50], Batch [10/250] | D Loss: 0.7156 | G Loss: 12.1281 (GAN: 1.2035, L1: 3.4388, Perceptual: 7.4858)\n",
      "Epoch [15/50], Batch [20/250] | D Loss: 0.8626 | G Loss: 11.8792 (GAN: 0.8964, L1: 3.7672, Perceptual: 7.2156)\n",
      "Epoch [15/50], Batch [20/250] | D Loss: 0.8626 | G Loss: 11.8792 (GAN: 0.8964, L1: 3.7672, Perceptual: 7.2156)\n",
      "Epoch [15/50], Batch [30/250] | D Loss: 0.5925 | G Loss: 12.2177 (GAN: 0.7838, L1: 3.9184, Perceptual: 7.5155)\n",
      "Epoch [15/50], Batch [30/250] | D Loss: 0.5925 | G Loss: 12.2177 (GAN: 0.7838, L1: 3.9184, Perceptual: 7.5155)\n",
      "Epoch [15/50], Batch [40/250] | D Loss: 0.5579 | G Loss: 11.8163 (GAN: 0.7279, L1: 3.3901, Perceptual: 7.6983)\n",
      "Epoch [15/50], Batch [40/250] | D Loss: 0.5579 | G Loss: 11.8163 (GAN: 0.7279, L1: 3.3901, Perceptual: 7.6983)\n",
      "Epoch [15/50], Batch [50/250] | D Loss: 0.5437 | G Loss: 11.5860 (GAN: 0.8215, L1: 3.5505, Perceptual: 7.2141)\n",
      "Epoch [15/50], Batch [50/250] | D Loss: 0.5437 | G Loss: 11.5860 (GAN: 0.8215, L1: 3.5505, Perceptual: 7.2141)\n",
      "Epoch [15/50], Batch [60/250] | D Loss: 0.8319 | G Loss: 12.1442 (GAN: 0.9725, L1: 3.8670, Perceptual: 7.3048)\n",
      "Epoch [15/50], Batch [60/250] | D Loss: 0.8319 | G Loss: 12.1442 (GAN: 0.9725, L1: 3.8670, Perceptual: 7.3048)\n",
      "Epoch [15/50], Batch [70/250] | D Loss: 0.5894 | G Loss: 11.8413 (GAN: 0.6665, L1: 3.8263, Perceptual: 7.3485)\n",
      "Epoch [15/50], Batch [70/250] | D Loss: 0.5894 | G Loss: 11.8413 (GAN: 0.6665, L1: 3.8263, Perceptual: 7.3485)\n",
      "Epoch [15/50], Batch [80/250] | D Loss: 0.5503 | G Loss: 12.6678 (GAN: 0.6890, L1: 4.2806, Perceptual: 7.6982)\n",
      "Epoch [15/50], Batch [80/250] | D Loss: 0.5503 | G Loss: 12.6678 (GAN: 0.6890, L1: 4.2806, Perceptual: 7.6982)\n",
      "Epoch [15/50], Batch [90/250] | D Loss: 0.6153 | G Loss: 11.8441 (GAN: 0.8514, L1: 3.6504, Perceptual: 7.3422)\n",
      "Epoch [15/50], Batch [90/250] | D Loss: 0.6153 | G Loss: 11.8441 (GAN: 0.8514, L1: 3.6504, Perceptual: 7.3422)\n",
      "Epoch [15/50], Batch [100/250] | D Loss: 0.4746 | G Loss: 12.1720 (GAN: 0.6660, L1: 3.8007, Perceptual: 7.7053)\n",
      "Epoch [15/50], Batch [100/250] | D Loss: 0.4746 | G Loss: 12.1720 (GAN: 0.6660, L1: 3.8007, Perceptual: 7.7053)\n",
      "Epoch [15/50], Batch [110/250] | D Loss: 0.7730 | G Loss: 12.6704 (GAN: 0.7813, L1: 4.1287, Perceptual: 7.7605)\n",
      "Epoch [15/50], Batch [110/250] | D Loss: 0.7730 | G Loss: 12.6704 (GAN: 0.7813, L1: 4.1287, Perceptual: 7.7605)\n",
      "Epoch [15/50], Batch [120/250] | D Loss: 0.6320 | G Loss: 11.7539 (GAN: 0.8924, L1: 3.5577, Perceptual: 7.3038)\n",
      "Epoch [15/50], Batch [120/250] | D Loss: 0.6320 | G Loss: 11.7539 (GAN: 0.8924, L1: 3.5577, Perceptual: 7.3038)\n",
      "Epoch [15/50], Batch [130/250] | D Loss: 0.7585 | G Loss: 11.5632 (GAN: 0.7652, L1: 3.4453, Perceptual: 7.3527)\n",
      "Epoch [15/50], Batch [130/250] | D Loss: 0.7585 | G Loss: 11.5632 (GAN: 0.7652, L1: 3.4453, Perceptual: 7.3527)\n",
      "Epoch [15/50], Batch [140/250] | D Loss: 0.8257 | G Loss: 11.5205 (GAN: 0.8098, L1: 3.4241, Perceptual: 7.2867)\n",
      "Epoch [15/50], Batch [140/250] | D Loss: 0.8257 | G Loss: 11.5205 (GAN: 0.8098, L1: 3.4241, Perceptual: 7.2867)\n",
      "Epoch [15/50], Batch [150/250] | D Loss: 0.5727 | G Loss: 11.5173 (GAN: 0.7971, L1: 3.6853, Perceptual: 7.0349)\n",
      "Epoch [15/50], Batch [150/250] | D Loss: 0.5727 | G Loss: 11.5173 (GAN: 0.7971, L1: 3.6853, Perceptual: 7.0349)\n",
      "Epoch [15/50], Batch [160/250] | D Loss: 0.5809 | G Loss: 12.5466 (GAN: 0.9486, L1: 3.7898, Perceptual: 7.8082)\n",
      "Epoch [15/50], Batch [160/250] | D Loss: 0.5809 | G Loss: 12.5466 (GAN: 0.9486, L1: 3.7898, Perceptual: 7.8082)\n",
      "Epoch [15/50], Batch [170/250] | D Loss: 0.5331 | G Loss: 12.0957 (GAN: 0.5229, L1: 4.0654, Perceptual: 7.5074)\n",
      "Epoch [15/50], Batch [170/250] | D Loss: 0.5331 | G Loss: 12.0957 (GAN: 0.5229, L1: 4.0654, Perceptual: 7.5074)\n",
      "Epoch [15/50], Batch [180/250] | D Loss: 0.5370 | G Loss: 12.5189 (GAN: 0.9859, L1: 3.7922, Perceptual: 7.7408)\n",
      "Epoch [15/50], Batch [180/250] | D Loss: 0.5370 | G Loss: 12.5189 (GAN: 0.9859, L1: 3.7922, Perceptual: 7.7408)\n",
      "Epoch [15/50], Batch [190/250] | D Loss: 0.6105 | G Loss: 11.9015 (GAN: 0.8533, L1: 3.9453, Perceptual: 7.1028)\n",
      "Epoch [15/50], Batch [190/250] | D Loss: 0.6105 | G Loss: 11.9015 (GAN: 0.8533, L1: 3.9453, Perceptual: 7.1028)\n",
      "Epoch [15/50], Batch [200/250] | D Loss: 0.6174 | G Loss: 11.9411 (GAN: 0.9935, L1: 3.4311, Perceptual: 7.5165)\n",
      "Epoch [15/50], Batch [200/250] | D Loss: 0.6174 | G Loss: 11.9411 (GAN: 0.9935, L1: 3.4311, Perceptual: 7.5165)\n",
      "Epoch [15/50], Batch [210/250] | D Loss: 0.7252 | G Loss: 11.4831 (GAN: 0.9885, L1: 3.2151, Perceptual: 7.2796)\n",
      "Epoch [15/50], Batch [210/250] | D Loss: 0.7252 | G Loss: 11.4831 (GAN: 0.9885, L1: 3.2151, Perceptual: 7.2796)\n",
      "Epoch [15/50], Batch [220/250] | D Loss: 0.6047 | G Loss: 11.3845 (GAN: 0.8141, L1: 3.4452, Perceptual: 7.1252)\n",
      "Epoch [15/50], Batch [220/250] | D Loss: 0.6047 | G Loss: 11.3845 (GAN: 0.8141, L1: 3.4452, Perceptual: 7.1252)\n",
      "Epoch [15/50], Batch [230/250] | D Loss: 0.4921 | G Loss: 12.9062 (GAN: 1.0787, L1: 3.9290, Perceptual: 7.8984)\n",
      "Epoch [15/50], Batch [230/250] | D Loss: 0.4921 | G Loss: 12.9062 (GAN: 1.0787, L1: 3.9290, Perceptual: 7.8984)\n",
      "Epoch [15/50], Batch [240/250] | D Loss: 0.5191 | G Loss: 12.3928 (GAN: 0.9143, L1: 3.7977, Perceptual: 7.6808)\n",
      "Epoch [15/50], Batch [240/250] | D Loss: 0.5191 | G Loss: 12.3928 (GAN: 0.9143, L1: 3.7977, Perceptual: 7.6808)\n",
      "Epoch [15/50], Batch [250/250] | D Loss: 0.6840 | G Loss: 13.1297 (GAN: 0.8206, L1: 4.2780, Perceptual: 8.0311)\n",
      "Epoch 15 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 15 ---\n",
      "Saving samples and checkpoint for epoch 15...\n",
      "Epoch [15/50], Batch [250/250] | D Loss: 0.6840 | G Loss: 13.1297 (GAN: 0.8206, L1: 4.2780, Perceptual: 8.0311)\n",
      "Epoch 15 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 15 ---\n",
      "Saving samples and checkpoint for epoch 15...\n",
      "Successfully saved image to: output/generated_images\\epoch_15_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_15_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [16/50], Batch [10/250] | D Loss: 0.5516 | G Loss: 12.2521 (GAN: 0.6705, L1: 3.7983, Perceptual: 7.7833)\n",
      "Epoch [16/50], Batch [10/250] | D Loss: 0.5516 | G Loss: 12.2521 (GAN: 0.6705, L1: 3.7983, Perceptual: 7.7833)\n",
      "Epoch [16/50], Batch [20/250] | D Loss: 0.5078 | G Loss: 11.0674 (GAN: 0.6814, L1: 3.2644, Perceptual: 7.1215)\n",
      "Epoch [16/50], Batch [20/250] | D Loss: 0.5078 | G Loss: 11.0674 (GAN: 0.6814, L1: 3.2644, Perceptual: 7.1215)\n",
      "Epoch [16/50], Batch [30/250] | D Loss: 0.5856 | G Loss: 10.4169 (GAN: 0.7268, L1: 3.1262, Perceptual: 6.5639)\n",
      "Epoch [16/50], Batch [30/250] | D Loss: 0.5856 | G Loss: 10.4169 (GAN: 0.7268, L1: 3.1262, Perceptual: 6.5639)\n",
      "Epoch [16/50], Batch [40/250] | D Loss: 0.6748 | G Loss: 11.8814 (GAN: 0.8089, L1: 3.6930, Perceptual: 7.3794)\n",
      "Epoch [16/50], Batch [40/250] | D Loss: 0.6748 | G Loss: 11.8814 (GAN: 0.8089, L1: 3.6930, Perceptual: 7.3794)\n",
      "Epoch [16/50], Batch [50/250] | D Loss: 0.4948 | G Loss: 12.1621 (GAN: 0.7410, L1: 3.9304, Perceptual: 7.4907)\n",
      "Epoch [16/50], Batch [50/250] | D Loss: 0.4948 | G Loss: 12.1621 (GAN: 0.7410, L1: 3.9304, Perceptual: 7.4907)\n",
      "Epoch [16/50], Batch [60/250] | D Loss: 0.6287 | G Loss: 11.3548 (GAN: 0.8727, L1: 3.3005, Perceptual: 7.1816)\n",
      "Epoch [16/50], Batch [60/250] | D Loss: 0.6287 | G Loss: 11.3548 (GAN: 0.8727, L1: 3.3005, Perceptual: 7.1816)\n",
      "Epoch [16/50], Batch [70/250] | D Loss: 0.6908 | G Loss: 12.5460 (GAN: 0.9591, L1: 3.5893, Perceptual: 7.9976)\n",
      "Epoch [16/50], Batch [70/250] | D Loss: 0.6908 | G Loss: 12.5460 (GAN: 0.9591, L1: 3.5893, Perceptual: 7.9976)\n",
      "Epoch [16/50], Batch [80/250] | D Loss: 0.6278 | G Loss: 11.5883 (GAN: 0.8863, L1: 3.1379, Perceptual: 7.5641)\n",
      "Epoch [16/50], Batch [80/250] | D Loss: 0.6278 | G Loss: 11.5883 (GAN: 0.8863, L1: 3.1379, Perceptual: 7.5641)\n",
      "Epoch [16/50], Batch [90/250] | D Loss: 0.6648 | G Loss: 13.1187 (GAN: 1.0224, L1: 3.8526, Perceptual: 8.2437)\n",
      "Epoch [16/50], Batch [90/250] | D Loss: 0.6648 | G Loss: 13.1187 (GAN: 1.0224, L1: 3.8526, Perceptual: 8.2437)\n",
      "Epoch [16/50], Batch [100/250] | D Loss: 0.5682 | G Loss: 12.0320 (GAN: 0.6611, L1: 3.9216, Perceptual: 7.4493)\n",
      "Epoch [16/50], Batch [100/250] | D Loss: 0.5682 | G Loss: 12.0320 (GAN: 0.6611, L1: 3.9216, Perceptual: 7.4493)\n",
      "Epoch [16/50], Batch [110/250] | D Loss: 0.5781 | G Loss: 12.2146 (GAN: 0.8019, L1: 3.8662, Perceptual: 7.5466)\n",
      "Epoch [16/50], Batch [110/250] | D Loss: 0.5781 | G Loss: 12.2146 (GAN: 0.8019, L1: 3.8662, Perceptual: 7.5466)\n",
      "Epoch [16/50], Batch [120/250] | D Loss: 0.6961 | G Loss: 12.0975 (GAN: 0.9713, L1: 3.6240, Perceptual: 7.5022)\n",
      "Epoch [16/50], Batch [120/250] | D Loss: 0.6961 | G Loss: 12.0975 (GAN: 0.9713, L1: 3.6240, Perceptual: 7.5022)\n",
      "Epoch [16/50], Batch [130/250] | D Loss: 0.7894 | G Loss: 11.5768 (GAN: 0.9161, L1: 3.5530, Perceptual: 7.1077)\n",
      "Epoch [16/50], Batch [130/250] | D Loss: 0.7894 | G Loss: 11.5768 (GAN: 0.9161, L1: 3.5530, Perceptual: 7.1077)\n",
      "Epoch [16/50], Batch [140/250] | D Loss: 0.6775 | G Loss: 11.4585 (GAN: 0.7920, L1: 3.4867, Perceptual: 7.1798)\n",
      "Epoch [16/50], Batch [140/250] | D Loss: 0.6775 | G Loss: 11.4585 (GAN: 0.7920, L1: 3.4867, Perceptual: 7.1798)\n",
      "Epoch [16/50], Batch [150/250] | D Loss: 0.6826 | G Loss: 13.1925 (GAN: 1.0473, L1: 3.9476, Perceptual: 8.1976)\n",
      "Epoch [16/50], Batch [150/250] | D Loss: 0.6826 | G Loss: 13.1925 (GAN: 1.0473, L1: 3.9476, Perceptual: 8.1976)\n",
      "Epoch [16/50], Batch [160/250] | D Loss: 0.7108 | G Loss: 11.2337 (GAN: 0.8405, L1: 3.5062, Perceptual: 6.8869)\n",
      "Epoch [16/50], Batch [160/250] | D Loss: 0.7108 | G Loss: 11.2337 (GAN: 0.8405, L1: 3.5062, Perceptual: 6.8869)\n",
      "Epoch [16/50], Batch [170/250] | D Loss: 0.5697 | G Loss: 11.5124 (GAN: 0.6950, L1: 3.4553, Perceptual: 7.3622)\n",
      "Epoch [16/50], Batch [170/250] | D Loss: 0.5697 | G Loss: 11.5124 (GAN: 0.6950, L1: 3.4553, Perceptual: 7.3622)\n",
      "Epoch [16/50], Batch [180/250] | D Loss: 0.7021 | G Loss: 11.6714 (GAN: 0.6314, L1: 3.7673, Perceptual: 7.2727)\n",
      "Epoch [16/50], Batch [180/250] | D Loss: 0.7021 | G Loss: 11.6714 (GAN: 0.6314, L1: 3.7673, Perceptual: 7.2727)\n",
      "Epoch [16/50], Batch [190/250] | D Loss: 0.6533 | G Loss: 11.5893 (GAN: 0.8645, L1: 3.2662, Perceptual: 7.4586)\n",
      "Epoch [16/50], Batch [190/250] | D Loss: 0.6533 | G Loss: 11.5893 (GAN: 0.8645, L1: 3.2662, Perceptual: 7.4586)\n",
      "Epoch [16/50], Batch [200/250] | D Loss: 0.7155 | G Loss: 11.4506 (GAN: 0.7953, L1: 3.4743, Perceptual: 7.1810)\n",
      "Epoch [16/50], Batch [200/250] | D Loss: 0.7155 | G Loss: 11.4506 (GAN: 0.7953, L1: 3.4743, Perceptual: 7.1810)\n",
      "Epoch [16/50], Batch [210/250] | D Loss: 0.5835 | G Loss: 12.3634 (GAN: 1.0745, L1: 3.5873, Perceptual: 7.7016)\n",
      "Epoch [16/50], Batch [210/250] | D Loss: 0.5835 | G Loss: 12.3634 (GAN: 1.0745, L1: 3.5873, Perceptual: 7.7016)\n",
      "Epoch [16/50], Batch [220/250] | D Loss: 0.6494 | G Loss: 12.6636 (GAN: 0.8994, L1: 3.8660, Perceptual: 7.8981)\n",
      "Epoch [16/50], Batch [220/250] | D Loss: 0.6494 | G Loss: 12.6636 (GAN: 0.8994, L1: 3.8660, Perceptual: 7.8981)\n",
      "Epoch [16/50], Batch [230/250] | D Loss: 0.6782 | G Loss: 11.2374 (GAN: 0.8658, L1: 3.2505, Perceptual: 7.1211)\n",
      "Epoch [16/50], Batch [230/250] | D Loss: 0.6782 | G Loss: 11.2374 (GAN: 0.8658, L1: 3.2505, Perceptual: 7.1211)\n",
      "Epoch [16/50], Batch [240/250] | D Loss: 0.6941 | G Loss: 12.5985 (GAN: 1.0011, L1: 3.7141, Perceptual: 7.8833)\n",
      "Epoch [16/50], Batch [240/250] | D Loss: 0.6941 | G Loss: 12.5985 (GAN: 1.0011, L1: 3.7141, Perceptual: 7.8833)\n",
      "Epoch [16/50], Batch [250/250] | D Loss: 0.6429 | G Loss: 12.5214 (GAN: 0.7798, L1: 4.1684, Perceptual: 7.5731)\n",
      "Epoch 16 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [16/50], Batch [250/250] | D Loss: 0.6429 | G Loss: 12.5214 (GAN: 0.7798, L1: 4.1684, Perceptual: 7.5731)\n",
      "Epoch 16 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [17/50], Batch [10/250] | D Loss: 0.6062 | G Loss: 11.6450 (GAN: 0.6375, L1: 3.4901, Perceptual: 7.5173)\n",
      "Epoch [17/50], Batch [10/250] | D Loss: 0.6062 | G Loss: 11.6450 (GAN: 0.6375, L1: 3.4901, Perceptual: 7.5173)\n",
      "Epoch [17/50], Batch [20/250] | D Loss: 0.8037 | G Loss: 12.6533 (GAN: 0.8729, L1: 3.8742, Perceptual: 7.9062)\n",
      "Epoch [17/50], Batch [20/250] | D Loss: 0.8037 | G Loss: 12.6533 (GAN: 0.8729, L1: 3.8742, Perceptual: 7.9062)\n",
      "Epoch [17/50], Batch [30/250] | D Loss: 0.6140 | G Loss: 11.0024 (GAN: 0.8100, L1: 3.3118, Perceptual: 6.8806)\n",
      "Epoch [17/50], Batch [30/250] | D Loss: 0.6140 | G Loss: 11.0024 (GAN: 0.8100, L1: 3.3118, Perceptual: 6.8806)\n",
      "Epoch [17/50], Batch [40/250] | D Loss: 0.6401 | G Loss: 11.9170 (GAN: 0.8036, L1: 3.5012, Perceptual: 7.6121)\n",
      "Epoch [17/50], Batch [40/250] | D Loss: 0.6401 | G Loss: 11.9170 (GAN: 0.8036, L1: 3.5012, Perceptual: 7.6121)\n",
      "Epoch [17/50], Batch [50/250] | D Loss: 0.8183 | G Loss: 11.4659 (GAN: 1.1936, L1: 3.2090, Perceptual: 7.0634)\n",
      "Epoch [17/50], Batch [50/250] | D Loss: 0.8183 | G Loss: 11.4659 (GAN: 1.1936, L1: 3.2090, Perceptual: 7.0634)\n",
      "Epoch [17/50], Batch [60/250] | D Loss: 0.7185 | G Loss: 11.4301 (GAN: 1.0519, L1: 3.2109, Perceptual: 7.1673)\n",
      "Epoch [17/50], Batch [60/250] | D Loss: 0.7185 | G Loss: 11.4301 (GAN: 1.0519, L1: 3.2109, Perceptual: 7.1673)\n",
      "Epoch [17/50], Batch [70/250] | D Loss: 0.5772 | G Loss: 11.0403 (GAN: 0.5236, L1: 3.3515, Perceptual: 7.1651)\n",
      "Epoch [17/50], Batch [70/250] | D Loss: 0.5772 | G Loss: 11.0403 (GAN: 0.5236, L1: 3.3515, Perceptual: 7.1651)\n",
      "Epoch [17/50], Batch [80/250] | D Loss: 0.4822 | G Loss: 12.6830 (GAN: 0.6872, L1: 3.8196, Perceptual: 8.1762)\n",
      "Epoch [17/50], Batch [80/250] | D Loss: 0.4822 | G Loss: 12.6830 (GAN: 0.6872, L1: 3.8196, Perceptual: 8.1762)\n",
      "Epoch [17/50], Batch [90/250] | D Loss: 0.7594 | G Loss: 10.7142 (GAN: 0.8979, L1: 3.1244, Perceptual: 6.6919)\n",
      "Epoch [17/50], Batch [90/250] | D Loss: 0.7594 | G Loss: 10.7142 (GAN: 0.8979, L1: 3.1244, Perceptual: 6.6919)\n",
      "Epoch [17/50], Batch [100/250] | D Loss: 0.7343 | G Loss: 11.5900 (GAN: 0.7165, L1: 3.7314, Perceptual: 7.1421)\n",
      "Epoch [17/50], Batch [100/250] | D Loss: 0.7343 | G Loss: 11.5900 (GAN: 0.7165, L1: 3.7314, Perceptual: 7.1421)\n",
      "Epoch [17/50], Batch [110/250] | D Loss: 0.6889 | G Loss: 12.5377 (GAN: 0.9452, L1: 3.7559, Perceptual: 7.8366)\n",
      "Epoch [17/50], Batch [110/250] | D Loss: 0.6889 | G Loss: 12.5377 (GAN: 0.9452, L1: 3.7559, Perceptual: 7.8366)\n",
      "Epoch [17/50], Batch [120/250] | D Loss: 0.7751 | G Loss: 10.6149 (GAN: 0.9313, L1: 2.7984, Perceptual: 6.8851)\n",
      "Epoch [17/50], Batch [120/250] | D Loss: 0.7751 | G Loss: 10.6149 (GAN: 0.9313, L1: 2.7984, Perceptual: 6.8851)\n",
      "Epoch [17/50], Batch [130/250] | D Loss: 0.7543 | G Loss: 10.8120 (GAN: 0.8649, L1: 3.2856, Perceptual: 6.6615)\n",
      "Epoch [17/50], Batch [130/250] | D Loss: 0.7543 | G Loss: 10.8120 (GAN: 0.8649, L1: 3.2856, Perceptual: 6.6615)\n",
      "Epoch [17/50], Batch [140/250] | D Loss: 0.6282 | G Loss: 11.4969 (GAN: 0.6647, L1: 3.5246, Perceptual: 7.3075)\n",
      "Epoch [17/50], Batch [140/250] | D Loss: 0.6282 | G Loss: 11.4969 (GAN: 0.6647, L1: 3.5246, Perceptual: 7.3075)\n",
      "Epoch [17/50], Batch [150/250] | D Loss: 0.5315 | G Loss: 11.4689 (GAN: 1.0399, L1: 3.1518, Perceptual: 7.2772)\n",
      "Epoch [17/50], Batch [150/250] | D Loss: 0.5315 | G Loss: 11.4689 (GAN: 1.0399, L1: 3.1518, Perceptual: 7.2772)\n",
      "Epoch [17/50], Batch [160/250] | D Loss: 0.5210 | G Loss: 12.8196 (GAN: 1.1316, L1: 3.7812, Perceptual: 7.9067)\n",
      "Epoch [17/50], Batch [160/250] | D Loss: 0.5210 | G Loss: 12.8196 (GAN: 1.1316, L1: 3.7812, Perceptual: 7.9067)\n",
      "Epoch [17/50], Batch [170/250] | D Loss: 0.7676 | G Loss: 12.8818 (GAN: 0.8213, L1: 4.3799, Perceptual: 7.6806)\n",
      "Epoch [17/50], Batch [170/250] | D Loss: 0.7676 | G Loss: 12.8818 (GAN: 0.8213, L1: 4.3799, Perceptual: 7.6806)\n",
      "Epoch [17/50], Batch [180/250] | D Loss: 0.7685 | G Loss: 10.1799 (GAN: 0.7682, L1: 2.7758, Perceptual: 6.6359)\n",
      "Epoch [17/50], Batch [180/250] | D Loss: 0.7685 | G Loss: 10.1799 (GAN: 0.7682, L1: 2.7758, Perceptual: 6.6359)\n",
      "Epoch [17/50], Batch [190/250] | D Loss: 0.5167 | G Loss: 11.5817 (GAN: 0.6592, L1: 3.7180, Perceptual: 7.2045)\n",
      "Epoch [17/50], Batch [190/250] | D Loss: 0.5167 | G Loss: 11.5817 (GAN: 0.6592, L1: 3.7180, Perceptual: 7.2045)\n",
      "Epoch [17/50], Batch [200/250] | D Loss: 0.4831 | G Loss: 12.3628 (GAN: 0.5561, L1: 4.0784, Perceptual: 7.7283)\n",
      "Epoch [17/50], Batch [200/250] | D Loss: 0.4831 | G Loss: 12.3628 (GAN: 0.5561, L1: 4.0784, Perceptual: 7.7283)\n",
      "Epoch [17/50], Batch [210/250] | D Loss: 0.6176 | G Loss: 11.7316 (GAN: 0.8865, L1: 3.5702, Perceptual: 7.2749)\n",
      "Epoch [17/50], Batch [210/250] | D Loss: 0.6176 | G Loss: 11.7316 (GAN: 0.8865, L1: 3.5702, Perceptual: 7.2749)\n",
      "Epoch [17/50], Batch [220/250] | D Loss: 0.6798 | G Loss: 13.1613 (GAN: 0.9043, L1: 4.1709, Perceptual: 8.0861)\n",
      "Epoch [17/50], Batch [220/250] | D Loss: 0.6798 | G Loss: 13.1613 (GAN: 0.9043, L1: 4.1709, Perceptual: 8.0861)\n",
      "Epoch [17/50], Batch [230/250] | D Loss: 0.6829 | G Loss: 10.8438 (GAN: 0.6185, L1: 3.2761, Perceptual: 6.9493)\n",
      "Epoch [17/50], Batch [230/250] | D Loss: 0.6829 | G Loss: 10.8438 (GAN: 0.6185, L1: 3.2761, Perceptual: 6.9493)\n",
      "Epoch [17/50], Batch [240/250] | D Loss: 0.4732 | G Loss: 11.9184 (GAN: 0.6919, L1: 3.7543, Perceptual: 7.4722)\n",
      "Epoch [17/50], Batch [240/250] | D Loss: 0.4732 | G Loss: 11.9184 (GAN: 0.6919, L1: 3.7543, Perceptual: 7.4722)\n",
      "Epoch [17/50], Batch [250/250] | D Loss: 0.5203 | G Loss: 12.2955 (GAN: 0.7856, L1: 3.6581, Perceptual: 7.8518)\n",
      "Epoch 17 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [17/50], Batch [250/250] | D Loss: 0.5203 | G Loss: 12.2955 (GAN: 0.7856, L1: 3.6581, Perceptual: 7.8518)\n",
      "Epoch 17 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [18/50], Batch [10/250] | D Loss: 0.6088 | G Loss: 12.6281 (GAN: 0.9791, L1: 3.9448, Perceptual: 7.7043)\n",
      "Epoch [18/50], Batch [10/250] | D Loss: 0.6088 | G Loss: 12.6281 (GAN: 0.9791, L1: 3.9448, Perceptual: 7.7043)\n",
      "Epoch [18/50], Batch [20/250] | D Loss: 0.7166 | G Loss: 11.2877 (GAN: 0.8793, L1: 3.4278, Perceptual: 6.9806)\n",
      "Epoch [18/50], Batch [20/250] | D Loss: 0.7166 | G Loss: 11.2877 (GAN: 0.8793, L1: 3.4278, Perceptual: 6.9806)\n",
      "Epoch [18/50], Batch [30/250] | D Loss: 0.6444 | G Loss: 12.5081 (GAN: 1.0321, L1: 3.9541, Perceptual: 7.5220)\n",
      "Epoch [18/50], Batch [30/250] | D Loss: 0.6444 | G Loss: 12.5081 (GAN: 1.0321, L1: 3.9541, Perceptual: 7.5220)\n",
      "Epoch [18/50], Batch [40/250] | D Loss: 0.7541 | G Loss: 11.7038 (GAN: 1.0055, L1: 3.2607, Perceptual: 7.4376)\n",
      "Epoch [18/50], Batch [40/250] | D Loss: 0.7541 | G Loss: 11.7038 (GAN: 1.0055, L1: 3.2607, Perceptual: 7.4376)\n",
      "Epoch [18/50], Batch [50/250] | D Loss: 0.5807 | G Loss: 13.2884 (GAN: 0.9876, L1: 4.2144, Perceptual: 8.0864)\n",
      "Epoch [18/50], Batch [50/250] | D Loss: 0.5807 | G Loss: 13.2884 (GAN: 0.9876, L1: 4.2144, Perceptual: 8.0864)\n",
      "Epoch [18/50], Batch [60/250] | D Loss: 0.5922 | G Loss: 11.2309 (GAN: 0.8227, L1: 3.7683, Perceptual: 6.6400)\n",
      "Epoch [18/50], Batch [60/250] | D Loss: 0.5922 | G Loss: 11.2309 (GAN: 0.8227, L1: 3.7683, Perceptual: 6.6400)\n",
      "Epoch [18/50], Batch [70/250] | D Loss: 0.4430 | G Loss: 12.0716 (GAN: 0.7942, L1: 3.7047, Perceptual: 7.5727)\n",
      "Epoch [18/50], Batch [70/250] | D Loss: 0.4430 | G Loss: 12.0716 (GAN: 0.7942, L1: 3.7047, Perceptual: 7.5727)\n",
      "Epoch [18/50], Batch [80/250] | D Loss: 0.5376 | G Loss: 11.9508 (GAN: 0.6280, L1: 4.0038, Perceptual: 7.3190)\n",
      "Epoch [18/50], Batch [80/250] | D Loss: 0.5376 | G Loss: 11.9508 (GAN: 0.6280, L1: 4.0038, Perceptual: 7.3190)\n",
      "Epoch [18/50], Batch [90/250] | D Loss: 0.5557 | G Loss: 12.1847 (GAN: 0.7117, L1: 3.8062, Perceptual: 7.6667)\n",
      "Epoch [18/50], Batch [90/250] | D Loss: 0.5557 | G Loss: 12.1847 (GAN: 0.7117, L1: 3.8062, Perceptual: 7.6667)\n",
      "Epoch [18/50], Batch [100/250] | D Loss: 0.5715 | G Loss: 13.0127 (GAN: 0.7868, L1: 4.1880, Perceptual: 8.0379)\n",
      "Epoch [18/50], Batch [100/250] | D Loss: 0.5715 | G Loss: 13.0127 (GAN: 0.7868, L1: 4.1880, Perceptual: 8.0379)\n",
      "Epoch [18/50], Batch [110/250] | D Loss: 0.6882 | G Loss: 12.0724 (GAN: 0.7618, L1: 3.8566, Perceptual: 7.4539)\n",
      "Epoch [18/50], Batch [110/250] | D Loss: 0.6882 | G Loss: 12.0724 (GAN: 0.7618, L1: 3.8566, Perceptual: 7.4539)\n",
      "Epoch [18/50], Batch [120/250] | D Loss: 0.7877 | G Loss: 12.2795 (GAN: 0.9214, L1: 3.7230, Perceptual: 7.6350)\n",
      "Epoch [18/50], Batch [120/250] | D Loss: 0.7877 | G Loss: 12.2795 (GAN: 0.9214, L1: 3.7230, Perceptual: 7.6350)\n",
      "Epoch [18/50], Batch [130/250] | D Loss: 0.5598 | G Loss: 10.8387 (GAN: 0.7852, L1: 3.0866, Perceptual: 6.9668)\n",
      "Epoch [18/50], Batch [130/250] | D Loss: 0.5598 | G Loss: 10.8387 (GAN: 0.7852, L1: 3.0866, Perceptual: 6.9668)\n",
      "Epoch [18/50], Batch [140/250] | D Loss: 0.7177 | G Loss: 10.9932 (GAN: 0.6359, L1: 3.5109, Perceptual: 6.8464)\n",
      "Epoch [18/50], Batch [140/250] | D Loss: 0.7177 | G Loss: 10.9932 (GAN: 0.6359, L1: 3.5109, Perceptual: 6.8464)\n",
      "Epoch [18/50], Batch [150/250] | D Loss: 0.5161 | G Loss: 12.1319 (GAN: 0.6842, L1: 3.7798, Perceptual: 7.6678)\n",
      "Epoch [18/50], Batch [150/250] | D Loss: 0.5161 | G Loss: 12.1319 (GAN: 0.6842, L1: 3.7798, Perceptual: 7.6678)\n",
      "Epoch [18/50], Batch [160/250] | D Loss: 0.6605 | G Loss: 11.9601 (GAN: 0.9651, L1: 3.8711, Perceptual: 7.1239)\n",
      "Epoch [18/50], Batch [160/250] | D Loss: 0.6605 | G Loss: 11.9601 (GAN: 0.9651, L1: 3.8711, Perceptual: 7.1239)\n",
      "Epoch [18/50], Batch [170/250] | D Loss: 0.7876 | G Loss: 10.9337 (GAN: 0.6334, L1: 3.3928, Perceptual: 6.9074)\n",
      "Epoch [18/50], Batch [170/250] | D Loss: 0.7876 | G Loss: 10.9337 (GAN: 0.6334, L1: 3.3928, Perceptual: 6.9074)\n",
      "Epoch [18/50], Batch [180/250] | D Loss: 0.6318 | G Loss: 11.6818 (GAN: 0.8294, L1: 3.6146, Perceptual: 7.2378)\n",
      "Epoch [18/50], Batch [180/250] | D Loss: 0.6318 | G Loss: 11.6818 (GAN: 0.8294, L1: 3.6146, Perceptual: 7.2378)\n",
      "Epoch [18/50], Batch [190/250] | D Loss: 0.5791 | G Loss: 12.3970 (GAN: 0.8233, L1: 3.9109, Perceptual: 7.6628)\n",
      "Epoch [18/50], Batch [190/250] | D Loss: 0.5791 | G Loss: 12.3970 (GAN: 0.8233, L1: 3.9109, Perceptual: 7.6628)\n",
      "Epoch [18/50], Batch [200/250] | D Loss: 0.5408 | G Loss: 11.5323 (GAN: 0.6600, L1: 3.7473, Perceptual: 7.1250)\n",
      "Epoch [18/50], Batch [200/250] | D Loss: 0.5408 | G Loss: 11.5323 (GAN: 0.6600, L1: 3.7473, Perceptual: 7.1250)\n",
      "Epoch [18/50], Batch [210/250] | D Loss: 0.6151 | G Loss: 12.9464 (GAN: 0.8742, L1: 3.8855, Perceptual: 8.1868)\n",
      "Epoch [18/50], Batch [210/250] | D Loss: 0.6151 | G Loss: 12.9464 (GAN: 0.8742, L1: 3.8855, Perceptual: 8.1868)\n",
      "Epoch [18/50], Batch [220/250] | D Loss: 0.5306 | G Loss: 12.1825 (GAN: 1.0281, L1: 3.5838, Perceptual: 7.5706)\n",
      "Epoch [18/50], Batch [220/250] | D Loss: 0.5306 | G Loss: 12.1825 (GAN: 1.0281, L1: 3.5838, Perceptual: 7.5706)\n",
      "Epoch [18/50], Batch [230/250] | D Loss: 0.6473 | G Loss: 12.5815 (GAN: 0.7048, L1: 4.1302, Perceptual: 7.7464)\n",
      "Epoch [18/50], Batch [230/250] | D Loss: 0.6473 | G Loss: 12.5815 (GAN: 0.7048, L1: 4.1302, Perceptual: 7.7464)\n",
      "Epoch [18/50], Batch [240/250] | D Loss: 0.6077 | G Loss: 12.2002 (GAN: 0.7761, L1: 3.8955, Perceptual: 7.5287)\n",
      "Epoch [18/50], Batch [240/250] | D Loss: 0.6077 | G Loss: 12.2002 (GAN: 0.7761, L1: 3.8955, Perceptual: 7.5287)\n",
      "Epoch [18/50], Batch [250/250] | D Loss: 0.6566 | G Loss: 11.6881 (GAN: 0.6613, L1: 3.5403, Perceptual: 7.4865)\n",
      "Epoch 18 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [18/50], Batch [250/250] | D Loss: 0.6566 | G Loss: 11.6881 (GAN: 0.6613, L1: 3.5403, Perceptual: 7.4865)\n",
      "Epoch 18 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [19/50], Batch [10/250] | D Loss: 0.7130 | G Loss: 11.2776 (GAN: 1.0409, L1: 3.2266, Perceptual: 7.0102)\n",
      "Epoch [19/50], Batch [10/250] | D Loss: 0.7130 | G Loss: 11.2776 (GAN: 1.0409, L1: 3.2266, Perceptual: 7.0102)\n",
      "Epoch [19/50], Batch [20/250] | D Loss: 0.6316 | G Loss: 10.8168 (GAN: 0.6332, L1: 3.0221, Perceptual: 7.1615)\n",
      "Epoch [19/50], Batch [20/250] | D Loss: 0.6316 | G Loss: 10.8168 (GAN: 0.6332, L1: 3.0221, Perceptual: 7.1615)\n",
      "Epoch [19/50], Batch [30/250] | D Loss: 0.7132 | G Loss: 11.7469 (GAN: 0.8016, L1: 3.7654, Perceptual: 7.1800)\n",
      "Epoch [19/50], Batch [30/250] | D Loss: 0.7132 | G Loss: 11.7469 (GAN: 0.8016, L1: 3.7654, Perceptual: 7.1800)\n",
      "Epoch [19/50], Batch [40/250] | D Loss: 0.5559 | G Loss: 12.2209 (GAN: 0.9159, L1: 3.5532, Perceptual: 7.7518)\n",
      "Epoch [19/50], Batch [40/250] | D Loss: 0.5559 | G Loss: 12.2209 (GAN: 0.9159, L1: 3.5532, Perceptual: 7.7518)\n",
      "Epoch [19/50], Batch [50/250] | D Loss: 0.5464 | G Loss: 11.3457 (GAN: 0.7238, L1: 3.3942, Perceptual: 7.2277)\n",
      "Epoch [19/50], Batch [50/250] | D Loss: 0.5464 | G Loss: 11.3457 (GAN: 0.7238, L1: 3.3942, Perceptual: 7.2277)\n",
      "Epoch [19/50], Batch [60/250] | D Loss: 0.5528 | G Loss: 11.6605 (GAN: 0.5931, L1: 3.6812, Perceptual: 7.3862)\n",
      "Epoch [19/50], Batch [60/250] | D Loss: 0.5528 | G Loss: 11.6605 (GAN: 0.5931, L1: 3.6812, Perceptual: 7.3862)\n",
      "Epoch [19/50], Batch [70/250] | D Loss: 0.5034 | G Loss: 12.5381 (GAN: 0.8269, L1: 4.0126, Perceptual: 7.6986)\n",
      "Epoch [19/50], Batch [70/250] | D Loss: 0.5034 | G Loss: 12.5381 (GAN: 0.8269, L1: 4.0126, Perceptual: 7.6986)\n",
      "Epoch [19/50], Batch [80/250] | D Loss: 0.8075 | G Loss: 11.0581 (GAN: 0.9330, L1: 3.1840, Perceptual: 6.9411)\n",
      "Epoch [19/50], Batch [80/250] | D Loss: 0.8075 | G Loss: 11.0581 (GAN: 0.9330, L1: 3.1840, Perceptual: 6.9411)\n",
      "Epoch [19/50], Batch [90/250] | D Loss: 0.6582 | G Loss: 12.1057 (GAN: 1.2364, L1: 3.3176, Perceptual: 7.5516)\n",
      "Epoch [19/50], Batch [90/250] | D Loss: 0.6582 | G Loss: 12.1057 (GAN: 1.2364, L1: 3.3176, Perceptual: 7.5516)\n",
      "Epoch [19/50], Batch [100/250] | D Loss: 0.5359 | G Loss: 11.4319 (GAN: 0.6753, L1: 3.3632, Perceptual: 7.3934)\n",
      "Epoch [19/50], Batch [100/250] | D Loss: 0.5359 | G Loss: 11.4319 (GAN: 0.6753, L1: 3.3632, Perceptual: 7.3934)\n",
      "Epoch [19/50], Batch [110/250] | D Loss: 0.6857 | G Loss: 13.4734 (GAN: 1.2767, L1: 4.3175, Perceptual: 7.8792)\n",
      "Epoch [19/50], Batch [110/250] | D Loss: 0.6857 | G Loss: 13.4734 (GAN: 1.2767, L1: 4.3175, Perceptual: 7.8792)\n",
      "Epoch [19/50], Batch [120/250] | D Loss: 0.5845 | G Loss: 11.8379 (GAN: 0.8833, L1: 3.3580, Perceptual: 7.5966)\n",
      "Epoch [19/50], Batch [120/250] | D Loss: 0.5845 | G Loss: 11.8379 (GAN: 0.8833, L1: 3.3580, Perceptual: 7.5966)\n",
      "Epoch [19/50], Batch [130/250] | D Loss: 0.5230 | G Loss: 13.0480 (GAN: 1.0262, L1: 4.0012, Perceptual: 8.0206)\n",
      "Epoch [19/50], Batch [130/250] | D Loss: 0.5230 | G Loss: 13.0480 (GAN: 1.0262, L1: 4.0012, Perceptual: 8.0206)\n",
      "Epoch [19/50], Batch [140/250] | D Loss: 0.7588 | G Loss: 12.7374 (GAN: 1.2003, L1: 3.6824, Perceptual: 7.8546)\n",
      "Epoch [19/50], Batch [140/250] | D Loss: 0.7588 | G Loss: 12.7374 (GAN: 1.2003, L1: 3.6824, Perceptual: 7.8546)\n",
      "Epoch [19/50], Batch [150/250] | D Loss: 0.5219 | G Loss: 10.9650 (GAN: 0.6980, L1: 3.3621, Perceptual: 6.9049)\n",
      "Epoch [19/50], Batch [150/250] | D Loss: 0.5219 | G Loss: 10.9650 (GAN: 0.6980, L1: 3.3621, Perceptual: 6.9049)\n",
      "Epoch [19/50], Batch [160/250] | D Loss: 0.5268 | G Loss: 11.9030 (GAN: 0.9993, L1: 3.5704, Perceptual: 7.3334)\n",
      "Epoch [19/50], Batch [160/250] | D Loss: 0.5268 | G Loss: 11.9030 (GAN: 0.9993, L1: 3.5704, Perceptual: 7.3334)\n",
      "Epoch [19/50], Batch [170/250] | D Loss: 0.5935 | G Loss: 11.9007 (GAN: 0.8560, L1: 3.4224, Perceptual: 7.6223)\n",
      "Epoch [19/50], Batch [170/250] | D Loss: 0.5935 | G Loss: 11.9007 (GAN: 0.8560, L1: 3.4224, Perceptual: 7.6223)\n",
      "Epoch [19/50], Batch [180/250] | D Loss: 0.6015 | G Loss: 12.7901 (GAN: 0.9371, L1: 3.9914, Perceptual: 7.8616)\n",
      "Epoch [19/50], Batch [180/250] | D Loss: 0.6015 | G Loss: 12.7901 (GAN: 0.9371, L1: 3.9914, Perceptual: 7.8616)\n",
      "Epoch [19/50], Batch [190/250] | D Loss: 0.5864 | G Loss: 11.3960 (GAN: 0.7930, L1: 3.1482, Perceptual: 7.4548)\n",
      "Epoch [19/50], Batch [190/250] | D Loss: 0.5864 | G Loss: 11.3960 (GAN: 0.7930, L1: 3.1482, Perceptual: 7.4548)\n",
      "Epoch [19/50], Batch [200/250] | D Loss: 0.5866 | G Loss: 11.8363 (GAN: 0.9740, L1: 3.3403, Perceptual: 7.5220)\n",
      "Epoch [19/50], Batch [200/250] | D Loss: 0.5866 | G Loss: 11.8363 (GAN: 0.9740, L1: 3.3403, Perceptual: 7.5220)\n",
      "Epoch [19/50], Batch [210/250] | D Loss: 0.5923 | G Loss: 12.7387 (GAN: 0.6775, L1: 3.9191, Perceptual: 8.1421)\n",
      "Epoch [19/50], Batch [210/250] | D Loss: 0.5923 | G Loss: 12.7387 (GAN: 0.6775, L1: 3.9191, Perceptual: 8.1421)\n",
      "Epoch [19/50], Batch [220/250] | D Loss: 0.6408 | G Loss: 10.8245 (GAN: 0.6262, L1: 3.2906, Perceptual: 6.9076)\n",
      "Epoch [19/50], Batch [220/250] | D Loss: 0.6408 | G Loss: 10.8245 (GAN: 0.6262, L1: 3.2906, Perceptual: 6.9076)\n",
      "Epoch [19/50], Batch [230/250] | D Loss: 0.5244 | G Loss: 11.6645 (GAN: 0.6765, L1: 3.6733, Perceptual: 7.3147)\n",
      "Epoch [19/50], Batch [230/250] | D Loss: 0.5244 | G Loss: 11.6645 (GAN: 0.6765, L1: 3.6733, Perceptual: 7.3147)\n",
      "Epoch [19/50], Batch [240/250] | D Loss: 0.7273 | G Loss: 12.1776 (GAN: 0.9571, L1: 3.4921, Perceptual: 7.7284)\n",
      "Epoch [19/50], Batch [240/250] | D Loss: 0.7273 | G Loss: 12.1776 (GAN: 0.9571, L1: 3.4921, Perceptual: 7.7284)\n",
      "Epoch [19/50], Batch [250/250] | D Loss: 0.5230 | G Loss: 12.1646 (GAN: 0.8937, L1: 3.7622, Perceptual: 7.5087)\n",
      "Epoch 19 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [19/50], Batch [250/250] | D Loss: 0.5230 | G Loss: 12.1646 (GAN: 0.8937, L1: 3.7622, Perceptual: 7.5087)\n",
      "Epoch 19 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [20/50], Batch [10/250] | D Loss: 0.4895 | G Loss: 13.1150 (GAN: 1.0029, L1: 3.9495, Perceptual: 8.1625)\n",
      "Epoch [20/50], Batch [10/250] | D Loss: 0.4895 | G Loss: 13.1150 (GAN: 1.0029, L1: 3.9495, Perceptual: 8.1625)\n",
      "Epoch [20/50], Batch [20/250] | D Loss: 0.5178 | G Loss: 11.4809 (GAN: 0.8170, L1: 3.5741, Perceptual: 7.0899)\n",
      "Epoch [20/50], Batch [20/250] | D Loss: 0.5178 | G Loss: 11.4809 (GAN: 0.8170, L1: 3.5741, Perceptual: 7.0899)\n",
      "Epoch [20/50], Batch [30/250] | D Loss: 0.6817 | G Loss: 11.3250 (GAN: 0.7296, L1: 3.2506, Perceptual: 7.3449)\n",
      "Epoch [20/50], Batch [30/250] | D Loss: 0.6817 | G Loss: 11.3250 (GAN: 0.7296, L1: 3.2506, Perceptual: 7.3449)\n",
      "Epoch [20/50], Batch [40/250] | D Loss: 0.6645 | G Loss: 11.2176 (GAN: 0.7911, L1: 3.5711, Perceptual: 6.8554)\n",
      "Epoch [20/50], Batch [40/250] | D Loss: 0.6645 | G Loss: 11.2176 (GAN: 0.7911, L1: 3.5711, Perceptual: 6.8554)\n",
      "Epoch [20/50], Batch [50/250] | D Loss: 0.4938 | G Loss: 12.5982 (GAN: 0.7433, L1: 4.0214, Perceptual: 7.8335)\n",
      "Epoch [20/50], Batch [50/250] | D Loss: 0.4938 | G Loss: 12.5982 (GAN: 0.7433, L1: 4.0214, Perceptual: 7.8335)\n",
      "Epoch [20/50], Batch [60/250] | D Loss: 0.5142 | G Loss: 11.8568 (GAN: 0.7035, L1: 3.7109, Perceptual: 7.4424)\n",
      "Epoch [20/50], Batch [60/250] | D Loss: 0.5142 | G Loss: 11.8568 (GAN: 0.7035, L1: 3.7109, Perceptual: 7.4424)\n",
      "Epoch [20/50], Batch [70/250] | D Loss: 0.4760 | G Loss: 12.6284 (GAN: 0.6185, L1: 3.8871, Perceptual: 8.1228)\n",
      "Epoch [20/50], Batch [70/250] | D Loss: 0.4760 | G Loss: 12.6284 (GAN: 0.6185, L1: 3.8871, Perceptual: 8.1228)\n",
      "Epoch [20/50], Batch [80/250] | D Loss: 0.7249 | G Loss: 12.7477 (GAN: 0.9012, L1: 3.8154, Perceptual: 8.0312)\n",
      "Epoch [20/50], Batch [80/250] | D Loss: 0.7249 | G Loss: 12.7477 (GAN: 0.9012, L1: 3.8154, Perceptual: 8.0312)\n",
      "Epoch [20/50], Batch [90/250] | D Loss: 0.5292 | G Loss: 11.2422 (GAN: 0.6304, L1: 3.4661, Perceptual: 7.1457)\n",
      "Epoch [20/50], Batch [90/250] | D Loss: 0.5292 | G Loss: 11.2422 (GAN: 0.6304, L1: 3.4661, Perceptual: 7.1457)\n",
      "Epoch [20/50], Batch [100/250] | D Loss: 0.7979 | G Loss: 11.1527 (GAN: 1.0336, L1: 2.9167, Perceptual: 7.2024)\n",
      "Epoch [20/50], Batch [100/250] | D Loss: 0.7979 | G Loss: 11.1527 (GAN: 1.0336, L1: 2.9167, Perceptual: 7.2024)\n",
      "Epoch [20/50], Batch [110/250] | D Loss: 0.7267 | G Loss: 12.4023 (GAN: 1.0393, L1: 3.9309, Perceptual: 7.4321)\n",
      "Epoch [20/50], Batch [110/250] | D Loss: 0.7267 | G Loss: 12.4023 (GAN: 1.0393, L1: 3.9309, Perceptual: 7.4321)\n",
      "Epoch [20/50], Batch [120/250] | D Loss: 0.5937 | G Loss: 11.3663 (GAN: 0.7949, L1: 3.4899, Perceptual: 7.0815)\n",
      "Epoch [20/50], Batch [120/250] | D Loss: 0.5937 | G Loss: 11.3663 (GAN: 0.7949, L1: 3.4899, Perceptual: 7.0815)\n",
      "Epoch [20/50], Batch [130/250] | D Loss: 0.5746 | G Loss: 10.8660 (GAN: 0.6255, L1: 3.3156, Perceptual: 6.9249)\n",
      "Epoch [20/50], Batch [130/250] | D Loss: 0.5746 | G Loss: 10.8660 (GAN: 0.6255, L1: 3.3156, Perceptual: 6.9249)\n",
      "Epoch [20/50], Batch [140/250] | D Loss: 0.5456 | G Loss: 11.6573 (GAN: 0.7694, L1: 3.5387, Perceptual: 7.3493)\n",
      "Epoch [20/50], Batch [140/250] | D Loss: 0.5456 | G Loss: 11.6573 (GAN: 0.7694, L1: 3.5387, Perceptual: 7.3493)\n",
      "Epoch [20/50], Batch [150/250] | D Loss: 0.7255 | G Loss: 12.8986 (GAN: 1.1249, L1: 3.8467, Perceptual: 7.9270)\n",
      "Epoch [20/50], Batch [150/250] | D Loss: 0.7255 | G Loss: 12.8986 (GAN: 1.1249, L1: 3.8467, Perceptual: 7.9270)\n",
      "Epoch [20/50], Batch [160/250] | D Loss: 0.6059 | G Loss: 11.0296 (GAN: 0.6315, L1: 3.6698, Perceptual: 6.7283)\n",
      "Epoch [20/50], Batch [160/250] | D Loss: 0.6059 | G Loss: 11.0296 (GAN: 0.6315, L1: 3.6698, Perceptual: 6.7283)\n",
      "Epoch [20/50], Batch [170/250] | D Loss: 0.5433 | G Loss: 12.1019 (GAN: 0.7370, L1: 3.6085, Perceptual: 7.7564)\n",
      "Epoch [20/50], Batch [170/250] | D Loss: 0.5433 | G Loss: 12.1019 (GAN: 0.7370, L1: 3.6085, Perceptual: 7.7564)\n",
      "Epoch [20/50], Batch [180/250] | D Loss: 0.6746 | G Loss: 10.6330 (GAN: 0.7521, L1: 3.1419, Perceptual: 6.7390)\n",
      "Epoch [20/50], Batch [180/250] | D Loss: 0.6746 | G Loss: 10.6330 (GAN: 0.7521, L1: 3.1419, Perceptual: 6.7390)\n",
      "Epoch [20/50], Batch [190/250] | D Loss: 0.6270 | G Loss: 13.4661 (GAN: 0.8866, L1: 4.9170, Perceptual: 7.6624)\n",
      "Epoch [20/50], Batch [190/250] | D Loss: 0.6270 | G Loss: 13.4661 (GAN: 0.8866, L1: 4.9170, Perceptual: 7.6624)\n",
      "Epoch [20/50], Batch [200/250] | D Loss: 0.6217 | G Loss: 11.3374 (GAN: 0.7937, L1: 3.2499, Perceptual: 7.2938)\n",
      "Epoch [20/50], Batch [200/250] | D Loss: 0.6217 | G Loss: 11.3374 (GAN: 0.7937, L1: 3.2499, Perceptual: 7.2938)\n",
      "Epoch [20/50], Batch [210/250] | D Loss: 0.5497 | G Loss: 11.8946 (GAN: 0.6864, L1: 3.5542, Perceptual: 7.6541)\n",
      "Epoch [20/50], Batch [210/250] | D Loss: 0.5497 | G Loss: 11.8946 (GAN: 0.6864, L1: 3.5542, Perceptual: 7.6541)\n",
      "Epoch [20/50], Batch [220/250] | D Loss: 0.4971 | G Loss: 11.7394 (GAN: 0.7531, L1: 3.6023, Perceptual: 7.3839)\n",
      "Epoch [20/50], Batch [220/250] | D Loss: 0.4971 | G Loss: 11.7394 (GAN: 0.7531, L1: 3.6023, Perceptual: 7.3839)\n",
      "Epoch [20/50], Batch [230/250] | D Loss: 0.6390 | G Loss: 12.8534 (GAN: 0.8268, L1: 3.6674, Perceptual: 8.3592)\n",
      "Epoch [20/50], Batch [230/250] | D Loss: 0.6390 | G Loss: 12.8534 (GAN: 0.8268, L1: 3.6674, Perceptual: 8.3592)\n",
      "Epoch [20/50], Batch [240/250] | D Loss: 0.3902 | G Loss: 11.5185 (GAN: 0.6096, L1: 3.4592, Perceptual: 7.4497)\n",
      "Epoch [20/50], Batch [240/250] | D Loss: 0.3902 | G Loss: 11.5185 (GAN: 0.6096, L1: 3.4592, Perceptual: 7.4497)\n",
      "Epoch [20/50], Batch [250/250] | D Loss: 0.6426 | G Loss: 11.2958 (GAN: 0.7799, L1: 3.2672, Perceptual: 7.2488)\n",
      "Epoch 20 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 20 ---\n",
      "Saving samples and checkpoint for epoch 20...\n",
      "Epoch [20/50], Batch [250/250] | D Loss: 0.6426 | G Loss: 11.2958 (GAN: 0.7799, L1: 3.2672, Perceptual: 7.2488)\n",
      "Epoch 20 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 20 ---\n",
      "Saving samples and checkpoint for epoch 20...\n",
      "Successfully saved image to: output/generated_images\\epoch_20_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_20_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [21/50], Batch [10/250] | D Loss: 0.6643 | G Loss: 11.8724 (GAN: 0.8062, L1: 3.6944, Perceptual: 7.3718)\n",
      "Epoch [21/50], Batch [10/250] | D Loss: 0.6643 | G Loss: 11.8724 (GAN: 0.8062, L1: 3.6944, Perceptual: 7.3718)\n",
      "Epoch [21/50], Batch [20/250] | D Loss: 0.5310 | G Loss: 13.1361 (GAN: 0.8457, L1: 4.1239, Perceptual: 8.1665)\n",
      "Epoch [21/50], Batch [20/250] | D Loss: 0.5310 | G Loss: 13.1361 (GAN: 0.8457, L1: 4.1239, Perceptual: 8.1665)\n",
      "Epoch [21/50], Batch [30/250] | D Loss: 0.5668 | G Loss: 12.1005 (GAN: 0.6969, L1: 3.8507, Perceptual: 7.5529)\n",
      "Epoch [21/50], Batch [30/250] | D Loss: 0.5668 | G Loss: 12.1005 (GAN: 0.6969, L1: 3.8507, Perceptual: 7.5529)\n",
      "Epoch [21/50], Batch [40/250] | D Loss: 0.5233 | G Loss: 11.9816 (GAN: 0.7450, L1: 3.7390, Perceptual: 7.4976)\n",
      "Epoch [21/50], Batch [40/250] | D Loss: 0.5233 | G Loss: 11.9816 (GAN: 0.7450, L1: 3.7390, Perceptual: 7.4976)\n",
      "Epoch [21/50], Batch [50/250] | D Loss: 0.5943 | G Loss: 11.8517 (GAN: 0.6631, L1: 3.2756, Perceptual: 7.9129)\n",
      "Epoch [21/50], Batch [50/250] | D Loss: 0.5943 | G Loss: 11.8517 (GAN: 0.6631, L1: 3.2756, Perceptual: 7.9129)\n",
      "Epoch [21/50], Batch [60/250] | D Loss: 0.4752 | G Loss: 12.5960 (GAN: 0.9017, L1: 3.7718, Perceptual: 7.9225)\n",
      "Epoch [21/50], Batch [60/250] | D Loss: 0.4752 | G Loss: 12.5960 (GAN: 0.9017, L1: 3.7718, Perceptual: 7.9225)\n",
      "Epoch [21/50], Batch [70/250] | D Loss: 0.6936 | G Loss: 10.2070 (GAN: 0.7355, L1: 2.9262, Perceptual: 6.5453)\n",
      "Epoch [21/50], Batch [70/250] | D Loss: 0.6936 | G Loss: 10.2070 (GAN: 0.7355, L1: 2.9262, Perceptual: 6.5453)\n",
      "Epoch [21/50], Batch [80/250] | D Loss: 0.5539 | G Loss: 12.3367 (GAN: 0.6612, L1: 4.0929, Perceptual: 7.5826)\n",
      "Epoch [21/50], Batch [80/250] | D Loss: 0.5539 | G Loss: 12.3367 (GAN: 0.6612, L1: 4.0929, Perceptual: 7.5826)\n",
      "Epoch [21/50], Batch [90/250] | D Loss: 0.5639 | G Loss: 11.1445 (GAN: 0.6048, L1: 3.2866, Perceptual: 7.2530)\n",
      "Epoch [21/50], Batch [90/250] | D Loss: 0.5639 | G Loss: 11.1445 (GAN: 0.6048, L1: 3.2866, Perceptual: 7.2530)\n",
      "Epoch [21/50], Batch [100/250] | D Loss: 0.6170 | G Loss: 11.1846 (GAN: 0.6746, L1: 3.4862, Perceptual: 7.0238)\n",
      "Epoch [21/50], Batch [100/250] | D Loss: 0.6170 | G Loss: 11.1846 (GAN: 0.6746, L1: 3.4862, Perceptual: 7.0238)\n",
      "Epoch [21/50], Batch [110/250] | D Loss: 0.9687 | G Loss: 12.6433 (GAN: 1.3582, L1: 3.6553, Perceptual: 7.6299)\n",
      "Epoch [21/50], Batch [110/250] | D Loss: 0.9687 | G Loss: 12.6433 (GAN: 1.3582, L1: 3.6553, Perceptual: 7.6299)\n",
      "Epoch [21/50], Batch [120/250] | D Loss: 0.5352 | G Loss: 12.0589 (GAN: 0.7562, L1: 3.6908, Perceptual: 7.6118)\n",
      "Epoch [21/50], Batch [120/250] | D Loss: 0.5352 | G Loss: 12.0589 (GAN: 0.7562, L1: 3.6908, Perceptual: 7.6118)\n",
      "Epoch [21/50], Batch [130/250] | D Loss: 0.8153 | G Loss: 11.7675 (GAN: 1.0960, L1: 3.5887, Perceptual: 7.0828)\n",
      "Epoch [21/50], Batch [130/250] | D Loss: 0.8153 | G Loss: 11.7675 (GAN: 1.0960, L1: 3.5887, Perceptual: 7.0828)\n",
      "Epoch [21/50], Batch [140/250] | D Loss: 0.6850 | G Loss: 11.4085 (GAN: 0.7228, L1: 3.6428, Perceptual: 7.0429)\n",
      "Epoch [21/50], Batch [140/250] | D Loss: 0.6850 | G Loss: 11.4085 (GAN: 0.7228, L1: 3.6428, Perceptual: 7.0429)\n",
      "Epoch [21/50], Batch [150/250] | D Loss: 0.4824 | G Loss: 13.0435 (GAN: 0.7878, L1: 4.3221, Perceptual: 7.9336)\n",
      "Epoch [21/50], Batch [150/250] | D Loss: 0.4824 | G Loss: 13.0435 (GAN: 0.7878, L1: 4.3221, Perceptual: 7.9336)\n",
      "Epoch [21/50], Batch [160/250] | D Loss: 0.6830 | G Loss: 11.6865 (GAN: 0.8074, L1: 3.2512, Perceptual: 7.6279)\n",
      "Epoch [21/50], Batch [160/250] | D Loss: 0.6830 | G Loss: 11.6865 (GAN: 0.8074, L1: 3.2512, Perceptual: 7.6279)\n",
      "Epoch [21/50], Batch [170/250] | D Loss: 0.8721 | G Loss: 11.4585 (GAN: 1.1022, L1: 3.5611, Perceptual: 6.7952)\n",
      "Epoch [21/50], Batch [170/250] | D Loss: 0.8721 | G Loss: 11.4585 (GAN: 1.1022, L1: 3.5611, Perceptual: 6.7952)\n",
      "Epoch [21/50], Batch [180/250] | D Loss: 0.6982 | G Loss: 11.6501 (GAN: 1.2278, L1: 3.2173, Perceptual: 7.2051)\n",
      "Epoch [21/50], Batch [180/250] | D Loss: 0.6982 | G Loss: 11.6501 (GAN: 1.2278, L1: 3.2173, Perceptual: 7.2051)\n",
      "Epoch [21/50], Batch [190/250] | D Loss: 0.5536 | G Loss: 11.5514 (GAN: 0.6783, L1: 3.7745, Perceptual: 7.0986)\n",
      "Epoch [21/50], Batch [190/250] | D Loss: 0.5536 | G Loss: 11.5514 (GAN: 0.6783, L1: 3.7745, Perceptual: 7.0986)\n",
      "Epoch [21/50], Batch [200/250] | D Loss: 0.5304 | G Loss: 10.7486 (GAN: 0.5759, L1: 3.0078, Perceptual: 7.1648)\n",
      "Epoch [21/50], Batch [200/250] | D Loss: 0.5304 | G Loss: 10.7486 (GAN: 0.5759, L1: 3.0078, Perceptual: 7.1648)\n",
      "Epoch [21/50], Batch [210/250] | D Loss: 0.5557 | G Loss: 10.8752 (GAN: 0.7539, L1: 3.1747, Perceptual: 6.9465)\n",
      "Epoch [21/50], Batch [210/250] | D Loss: 0.5557 | G Loss: 10.8752 (GAN: 0.7539, L1: 3.1747, Perceptual: 6.9465)\n",
      "Epoch [21/50], Batch [220/250] | D Loss: 0.7142 | G Loss: 10.8500 (GAN: 0.7627, L1: 3.1563, Perceptual: 6.9309)\n",
      "Epoch [21/50], Batch [220/250] | D Loss: 0.7142 | G Loss: 10.8500 (GAN: 0.7627, L1: 3.1563, Perceptual: 6.9309)\n",
      "Epoch [21/50], Batch [230/250] | D Loss: 0.4814 | G Loss: 11.1950 (GAN: 0.5880, L1: 3.2399, Perceptual: 7.3671)\n",
      "Epoch [21/50], Batch [230/250] | D Loss: 0.4814 | G Loss: 11.1950 (GAN: 0.5880, L1: 3.2399, Perceptual: 7.3671)\n",
      "Epoch [21/50], Batch [240/250] | D Loss: 0.5988 | G Loss: 11.5628 (GAN: 0.9394, L1: 3.5929, Perceptual: 7.0305)\n",
      "Epoch [21/50], Batch [240/250] | D Loss: 0.5988 | G Loss: 11.5628 (GAN: 0.9394, L1: 3.5929, Perceptual: 7.0305)\n",
      "Epoch [21/50], Batch [250/250] | D Loss: 0.6136 | G Loss: 11.4042 (GAN: 0.7936, L1: 3.4673, Perceptual: 7.1433)\n",
      "Epoch 21 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [21/50], Batch [250/250] | D Loss: 0.6136 | G Loss: 11.4042 (GAN: 0.7936, L1: 3.4673, Perceptual: 7.1433)\n",
      "Epoch 21 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [22/50], Batch [10/250] | D Loss: 0.5661 | G Loss: 11.5746 (GAN: 0.6182, L1: 3.7046, Perceptual: 7.2518)\n",
      "Epoch [22/50], Batch [10/250] | D Loss: 0.5661 | G Loss: 11.5746 (GAN: 0.6182, L1: 3.7046, Perceptual: 7.2518)\n",
      "Epoch [22/50], Batch [20/250] | D Loss: 0.5907 | G Loss: 12.0798 (GAN: 0.6179, L1: 3.7188, Perceptual: 7.7432)\n",
      "Epoch [22/50], Batch [20/250] | D Loss: 0.5907 | G Loss: 12.0798 (GAN: 0.6179, L1: 3.7188, Perceptual: 7.7432)\n",
      "Epoch [22/50], Batch [30/250] | D Loss: 0.6841 | G Loss: 12.0593 (GAN: 0.9050, L1: 3.6511, Perceptual: 7.5032)\n",
      "Epoch [22/50], Batch [30/250] | D Loss: 0.6841 | G Loss: 12.0593 (GAN: 0.9050, L1: 3.6511, Perceptual: 7.5032)\n",
      "Epoch [22/50], Batch [40/250] | D Loss: 0.7274 | G Loss: 11.5064 (GAN: 0.7646, L1: 3.4706, Perceptual: 7.2713)\n",
      "Epoch [22/50], Batch [40/250] | D Loss: 0.7274 | G Loss: 11.5064 (GAN: 0.7646, L1: 3.4706, Perceptual: 7.2713)\n",
      "Epoch [22/50], Batch [50/250] | D Loss: 0.5587 | G Loss: 11.8692 (GAN: 0.6295, L1: 3.7683, Perceptual: 7.4714)\n",
      "Epoch [22/50], Batch [50/250] | D Loss: 0.5587 | G Loss: 11.8692 (GAN: 0.6295, L1: 3.7683, Perceptual: 7.4714)\n",
      "Epoch [22/50], Batch [60/250] | D Loss: 0.6290 | G Loss: 11.1865 (GAN: 0.7541, L1: 3.5690, Perceptual: 6.8634)\n",
      "Epoch [22/50], Batch [60/250] | D Loss: 0.6290 | G Loss: 11.1865 (GAN: 0.7541, L1: 3.5690, Perceptual: 6.8634)\n",
      "Epoch [22/50], Batch [70/250] | D Loss: 0.6623 | G Loss: 11.5805 (GAN: 0.8415, L1: 3.4978, Perceptual: 7.2412)\n",
      "Epoch [22/50], Batch [70/250] | D Loss: 0.6623 | G Loss: 11.5805 (GAN: 0.8415, L1: 3.4978, Perceptual: 7.2412)\n",
      "Epoch [22/50], Batch [80/250] | D Loss: 0.4984 | G Loss: 11.3851 (GAN: 0.8129, L1: 3.6475, Perceptual: 6.9247)\n",
      "Epoch [22/50], Batch [80/250] | D Loss: 0.4984 | G Loss: 11.3851 (GAN: 0.8129, L1: 3.6475, Perceptual: 6.9247)\n",
      "Epoch [22/50], Batch [90/250] | D Loss: 0.6631 | G Loss: 10.6885 (GAN: 0.8830, L1: 2.8810, Perceptual: 6.9245)\n",
      "Epoch [22/50], Batch [90/250] | D Loss: 0.6631 | G Loss: 10.6885 (GAN: 0.8830, L1: 2.8810, Perceptual: 6.9245)\n",
      "Epoch [22/50], Batch [100/250] | D Loss: 0.6442 | G Loss: 13.1508 (GAN: 0.9028, L1: 4.0178, Perceptual: 8.2302)\n",
      "Epoch [22/50], Batch [100/250] | D Loss: 0.6442 | G Loss: 13.1508 (GAN: 0.9028, L1: 4.0178, Perceptual: 8.2302)\n",
      "Epoch [22/50], Batch [110/250] | D Loss: 0.7206 | G Loss: 12.3589 (GAN: 0.9897, L1: 3.7633, Perceptual: 7.6059)\n",
      "Epoch [22/50], Batch [110/250] | D Loss: 0.7206 | G Loss: 12.3589 (GAN: 0.9897, L1: 3.7633, Perceptual: 7.6059)\n",
      "Epoch [22/50], Batch [120/250] | D Loss: 0.5366 | G Loss: 11.9886 (GAN: 0.9056, L1: 3.5412, Perceptual: 7.5418)\n",
      "Epoch [22/50], Batch [120/250] | D Loss: 0.5366 | G Loss: 11.9886 (GAN: 0.9056, L1: 3.5412, Perceptual: 7.5418)\n",
      "Epoch [22/50], Batch [130/250] | D Loss: 0.6246 | G Loss: 11.1715 (GAN: 0.7978, L1: 3.2867, Perceptual: 7.0870)\n",
      "Epoch [22/50], Batch [130/250] | D Loss: 0.6246 | G Loss: 11.1715 (GAN: 0.7978, L1: 3.2867, Perceptual: 7.0870)\n",
      "Epoch [22/50], Batch [140/250] | D Loss: 0.5296 | G Loss: 12.2703 (GAN: 0.9139, L1: 3.6868, Perceptual: 7.6697)\n",
      "Epoch [22/50], Batch [140/250] | D Loss: 0.5296 | G Loss: 12.2703 (GAN: 0.9139, L1: 3.6868, Perceptual: 7.6697)\n",
      "Epoch [22/50], Batch [150/250] | D Loss: 0.8159 | G Loss: 11.1093 (GAN: 0.7829, L1: 3.2403, Perceptual: 7.0861)\n",
      "Epoch [22/50], Batch [150/250] | D Loss: 0.8159 | G Loss: 11.1093 (GAN: 0.7829, L1: 3.2403, Perceptual: 7.0861)\n",
      "Epoch [22/50], Batch [160/250] | D Loss: 0.5789 | G Loss: 10.9775 (GAN: 0.8302, L1: 3.2773, Perceptual: 6.8700)\n",
      "Epoch [22/50], Batch [160/250] | D Loss: 0.5789 | G Loss: 10.9775 (GAN: 0.8302, L1: 3.2773, Perceptual: 6.8700)\n",
      "Epoch [22/50], Batch [170/250] | D Loss: 0.5112 | G Loss: 11.7068 (GAN: 0.6570, L1: 3.4973, Perceptual: 7.5525)\n",
      "Epoch [22/50], Batch [170/250] | D Loss: 0.5112 | G Loss: 11.7068 (GAN: 0.6570, L1: 3.4973, Perceptual: 7.5525)\n",
      "Epoch [22/50], Batch [180/250] | D Loss: 0.7884 | G Loss: 11.5065 (GAN: 0.8697, L1: 3.4286, Perceptual: 7.2081)\n",
      "Epoch [22/50], Batch [180/250] | D Loss: 0.7884 | G Loss: 11.5065 (GAN: 0.8697, L1: 3.4286, Perceptual: 7.2081)\n",
      "Epoch [22/50], Batch [190/250] | D Loss: 0.7096 | G Loss: 10.7143 (GAN: 0.7301, L1: 3.3495, Perceptual: 6.6347)\n",
      "Epoch [22/50], Batch [190/250] | D Loss: 0.7096 | G Loss: 10.7143 (GAN: 0.7301, L1: 3.3495, Perceptual: 6.6347)\n",
      "Epoch [22/50], Batch [200/250] | D Loss: 0.4424 | G Loss: 11.3250 (GAN: 0.6976, L1: 3.5280, Perceptual: 7.0994)\n",
      "Epoch [22/50], Batch [200/250] | D Loss: 0.4424 | G Loss: 11.3250 (GAN: 0.6976, L1: 3.5280, Perceptual: 7.0994)\n",
      "Epoch [22/50], Batch [210/250] | D Loss: 0.7960 | G Loss: 12.8056 (GAN: 0.7653, L1: 4.2400, Perceptual: 7.8003)\n",
      "Epoch [22/50], Batch [210/250] | D Loss: 0.7960 | G Loss: 12.8056 (GAN: 0.7653, L1: 4.2400, Perceptual: 7.8003)\n",
      "Epoch [22/50], Batch [220/250] | D Loss: 0.6934 | G Loss: 11.6368 (GAN: 0.7840, L1: 3.5345, Perceptual: 7.3183)\n",
      "Epoch [22/50], Batch [220/250] | D Loss: 0.6934 | G Loss: 11.6368 (GAN: 0.7840, L1: 3.5345, Perceptual: 7.3183)\n",
      "Epoch [22/50], Batch [230/250] | D Loss: 0.6751 | G Loss: 10.8124 (GAN: 0.5699, L1: 3.1718, Perceptual: 7.0708)\n",
      "Epoch [22/50], Batch [230/250] | D Loss: 0.6751 | G Loss: 10.8124 (GAN: 0.5699, L1: 3.1718, Perceptual: 7.0708)\n",
      "Epoch [22/50], Batch [240/250] | D Loss: 0.6790 | G Loss: 11.9041 (GAN: 0.7626, L1: 3.7945, Perceptual: 7.3470)\n",
      "Epoch [22/50], Batch [240/250] | D Loss: 0.6790 | G Loss: 11.9041 (GAN: 0.7626, L1: 3.7945, Perceptual: 7.3470)\n",
      "Epoch [22/50], Batch [250/250] | D Loss: 0.5849 | G Loss: 12.0011 (GAN: 0.6772, L1: 3.8413, Perceptual: 7.4826)\n",
      "Epoch 22 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [22/50], Batch [250/250] | D Loss: 0.5849 | G Loss: 12.0011 (GAN: 0.6772, L1: 3.8413, Perceptual: 7.4826)\n",
      "Epoch 22 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [23/50], Batch [10/250] | D Loss: 0.6540 | G Loss: 10.4743 (GAN: 0.7417, L1: 3.1098, Perceptual: 6.6228)\n",
      "Epoch [23/50], Batch [10/250] | D Loss: 0.6540 | G Loss: 10.4743 (GAN: 0.7417, L1: 3.1098, Perceptual: 6.6228)\n",
      "Epoch [23/50], Batch [20/250] | D Loss: 0.4712 | G Loss: 13.0931 (GAN: 0.5570, L1: 4.6480, Perceptual: 7.8882)\n",
      "Epoch [23/50], Batch [20/250] | D Loss: 0.4712 | G Loss: 13.0931 (GAN: 0.5570, L1: 4.6480, Perceptual: 7.8882)\n",
      "Epoch [23/50], Batch [30/250] | D Loss: 0.5702 | G Loss: 12.6346 (GAN: 0.9184, L1: 4.0317, Perceptual: 7.6845)\n",
      "Epoch [23/50], Batch [30/250] | D Loss: 0.5702 | G Loss: 12.6346 (GAN: 0.9184, L1: 4.0317, Perceptual: 7.6845)\n",
      "Epoch [23/50], Batch [40/250] | D Loss: 0.6516 | G Loss: 11.5540 (GAN: 0.5281, L1: 3.8680, Perceptual: 7.1579)\n",
      "Epoch [23/50], Batch [40/250] | D Loss: 0.6516 | G Loss: 11.5540 (GAN: 0.5281, L1: 3.8680, Perceptual: 7.1579)\n",
      "Epoch [23/50], Batch [50/250] | D Loss: 0.5307 | G Loss: 11.6404 (GAN: 0.6415, L1: 3.4433, Perceptual: 7.5556)\n",
      "Epoch [23/50], Batch [50/250] | D Loss: 0.5307 | G Loss: 11.6404 (GAN: 0.6415, L1: 3.4433, Perceptual: 7.5556)\n",
      "Epoch [23/50], Batch [60/250] | D Loss: 0.5394 | G Loss: 12.4290 (GAN: 0.7163, L1: 3.8513, Perceptual: 7.8614)\n",
      "Epoch [23/50], Batch [60/250] | D Loss: 0.5394 | G Loss: 12.4290 (GAN: 0.7163, L1: 3.8513, Perceptual: 7.8614)\n",
      "Epoch [23/50], Batch [70/250] | D Loss: 0.5982 | G Loss: 10.9133 (GAN: 0.6397, L1: 3.2567, Perceptual: 7.0170)\n",
      "Epoch [23/50], Batch [70/250] | D Loss: 0.5982 | G Loss: 10.9133 (GAN: 0.6397, L1: 3.2567, Perceptual: 7.0170)\n",
      "Epoch [23/50], Batch [80/250] | D Loss: 0.6968 | G Loss: 10.8487 (GAN: 0.9047, L1: 3.2019, Perceptual: 6.7421)\n",
      "Epoch [23/50], Batch [80/250] | D Loss: 0.6968 | G Loss: 10.8487 (GAN: 0.9047, L1: 3.2019, Perceptual: 6.7421)\n",
      "Epoch [23/50], Batch [90/250] | D Loss: 0.4644 | G Loss: 11.8108 (GAN: 0.7210, L1: 3.6845, Perceptual: 7.4053)\n",
      "Epoch [23/50], Batch [90/250] | D Loss: 0.4644 | G Loss: 11.8108 (GAN: 0.7210, L1: 3.6845, Perceptual: 7.4053)\n",
      "Epoch [23/50], Batch [100/250] | D Loss: 0.6264 | G Loss: 11.6577 (GAN: 0.6668, L1: 3.4589, Perceptual: 7.5321)\n",
      "Epoch [23/50], Batch [100/250] | D Loss: 0.6264 | G Loss: 11.6577 (GAN: 0.6668, L1: 3.4589, Perceptual: 7.5321)\n",
      "Epoch [23/50], Batch [110/250] | D Loss: 0.5587 | G Loss: 12.6602 (GAN: 0.8077, L1: 3.8625, Perceptual: 7.9899)\n",
      "Epoch [23/50], Batch [110/250] | D Loss: 0.5587 | G Loss: 12.6602 (GAN: 0.8077, L1: 3.8625, Perceptual: 7.9899)\n",
      "Epoch [23/50], Batch [120/250] | D Loss: 0.6289 | G Loss: 11.7104 (GAN: 0.8109, L1: 3.6115, Perceptual: 7.2880)\n",
      "Epoch [23/50], Batch [120/250] | D Loss: 0.6289 | G Loss: 11.7104 (GAN: 0.8109, L1: 3.6115, Perceptual: 7.2880)\n",
      "Epoch [23/50], Batch [130/250] | D Loss: 0.7168 | G Loss: 11.9868 (GAN: 0.8525, L1: 3.8458, Perceptual: 7.2885)\n",
      "Epoch [23/50], Batch [130/250] | D Loss: 0.7168 | G Loss: 11.9868 (GAN: 0.8525, L1: 3.8458, Perceptual: 7.2885)\n",
      "Epoch [23/50], Batch [140/250] | D Loss: 0.6501 | G Loss: 10.6648 (GAN: 0.7647, L1: 3.3076, Perceptual: 6.5925)\n",
      "Epoch [23/50], Batch [140/250] | D Loss: 0.6501 | G Loss: 10.6648 (GAN: 0.7647, L1: 3.3076, Perceptual: 6.5925)\n",
      "Epoch [23/50], Batch [150/250] | D Loss: 0.6122 | G Loss: 10.0575 (GAN: 0.6643, L1: 2.8569, Perceptual: 6.5363)\n",
      "Epoch [23/50], Batch [150/250] | D Loss: 0.6122 | G Loss: 10.0575 (GAN: 0.6643, L1: 2.8569, Perceptual: 6.5363)\n",
      "Epoch [23/50], Batch [160/250] | D Loss: 0.7937 | G Loss: 13.3117 (GAN: 1.0008, L1: 4.0796, Perceptual: 8.2313)\n",
      "Epoch [23/50], Batch [160/250] | D Loss: 0.7937 | G Loss: 13.3117 (GAN: 1.0008, L1: 4.0796, Perceptual: 8.2313)\n",
      "Epoch [23/50], Batch [170/250] | D Loss: 0.5731 | G Loss: 12.5804 (GAN: 0.8403, L1: 4.1051, Perceptual: 7.6350)\n",
      "Epoch [23/50], Batch [170/250] | D Loss: 0.5731 | G Loss: 12.5804 (GAN: 0.8403, L1: 4.1051, Perceptual: 7.6350)\n",
      "Epoch [23/50], Batch [180/250] | D Loss: 0.5165 | G Loss: 13.0617 (GAN: 0.9191, L1: 4.1333, Perceptual: 8.0093)\n",
      "Epoch [23/50], Batch [180/250] | D Loss: 0.5165 | G Loss: 13.0617 (GAN: 0.9191, L1: 4.1333, Perceptual: 8.0093)\n",
      "Epoch [23/50], Batch [190/250] | D Loss: 0.6910 | G Loss: 11.7074 (GAN: 0.9917, L1: 3.1567, Perceptual: 7.5591)\n",
      "Epoch [23/50], Batch [190/250] | D Loss: 0.6910 | G Loss: 11.7074 (GAN: 0.9917, L1: 3.1567, Perceptual: 7.5591)\n",
      "Epoch [23/50], Batch [200/250] | D Loss: 0.6411 | G Loss: 11.4821 (GAN: 0.8130, L1: 3.2600, Perceptual: 7.4091)\n",
      "Epoch [23/50], Batch [200/250] | D Loss: 0.6411 | G Loss: 11.4821 (GAN: 0.8130, L1: 3.2600, Perceptual: 7.4091)\n",
      "Epoch [23/50], Batch [210/250] | D Loss: 0.5145 | G Loss: 11.9489 (GAN: 0.8677, L1: 4.0999, Perceptual: 6.9812)\n",
      "Epoch [23/50], Batch [210/250] | D Loss: 0.5145 | G Loss: 11.9489 (GAN: 0.8677, L1: 4.0999, Perceptual: 6.9812)\n",
      "Epoch [23/50], Batch [220/250] | D Loss: 0.6340 | G Loss: 11.0795 (GAN: 0.9124, L1: 3.2545, Perceptual: 6.9126)\n",
      "Epoch [23/50], Batch [220/250] | D Loss: 0.6340 | G Loss: 11.0795 (GAN: 0.9124, L1: 3.2545, Perceptual: 6.9126)\n",
      "Epoch [23/50], Batch [230/250] | D Loss: 0.6191 | G Loss: 11.7885 (GAN: 0.6255, L1: 3.5057, Perceptual: 7.6572)\n",
      "Epoch [23/50], Batch [230/250] | D Loss: 0.6191 | G Loss: 11.7885 (GAN: 0.6255, L1: 3.5057, Perceptual: 7.6572)\n",
      "Epoch [23/50], Batch [240/250] | D Loss: 0.6247 | G Loss: 11.1771 (GAN: 0.7322, L1: 3.1796, Perceptual: 7.2653)\n",
      "Epoch [23/50], Batch [240/250] | D Loss: 0.6247 | G Loss: 11.1771 (GAN: 0.7322, L1: 3.1796, Perceptual: 7.2653)\n",
      "Epoch [23/50], Batch [250/250] | D Loss: 0.6243 | G Loss: 13.1756 (GAN: 0.9353, L1: 4.3149, Perceptual: 7.9255)\n",
      "Epoch 23 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [23/50], Batch [250/250] | D Loss: 0.6243 | G Loss: 13.1756 (GAN: 0.9353, L1: 4.3149, Perceptual: 7.9255)\n",
      "Epoch 23 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [24/50], Batch [10/250] | D Loss: 0.5817 | G Loss: 10.6438 (GAN: 0.6033, L1: 3.3078, Perceptual: 6.7327)\n",
      "Epoch [24/50], Batch [10/250] | D Loss: 0.5817 | G Loss: 10.6438 (GAN: 0.6033, L1: 3.3078, Perceptual: 6.7327)\n",
      "Epoch [24/50], Batch [20/250] | D Loss: 0.5065 | G Loss: 13.2469 (GAN: 0.9210, L1: 4.0599, Perceptual: 8.2660)\n",
      "Epoch [24/50], Batch [20/250] | D Loss: 0.5065 | G Loss: 13.2469 (GAN: 0.9210, L1: 4.0599, Perceptual: 8.2660)\n",
      "Epoch [24/50], Batch [30/250] | D Loss: 0.6331 | G Loss: 11.1712 (GAN: 0.6827, L1: 3.4037, Perceptual: 7.0848)\n",
      "Epoch [24/50], Batch [30/250] | D Loss: 0.6331 | G Loss: 11.1712 (GAN: 0.6827, L1: 3.4037, Perceptual: 7.0848)\n",
      "Epoch [24/50], Batch [40/250] | D Loss: 0.6487 | G Loss: 11.2310 (GAN: 0.8852, L1: 3.2552, Perceptual: 7.0906)\n",
      "Epoch [24/50], Batch [40/250] | D Loss: 0.6487 | G Loss: 11.2310 (GAN: 0.8852, L1: 3.2552, Perceptual: 7.0906)\n",
      "Epoch [24/50], Batch [50/250] | D Loss: 0.5415 | G Loss: 13.1350 (GAN: 0.9212, L1: 4.1254, Perceptual: 8.0884)\n",
      "Epoch [24/50], Batch [50/250] | D Loss: 0.5415 | G Loss: 13.1350 (GAN: 0.9212, L1: 4.1254, Perceptual: 8.0884)\n",
      "Epoch [24/50], Batch [60/250] | D Loss: 0.5087 | G Loss: 11.6932 (GAN: 0.9150, L1: 3.2032, Perceptual: 7.5751)\n",
      "Epoch [24/50], Batch [60/250] | D Loss: 0.5087 | G Loss: 11.6932 (GAN: 0.9150, L1: 3.2032, Perceptual: 7.5751)\n",
      "Epoch [24/50], Batch [70/250] | D Loss: 0.5633 | G Loss: 11.9050 (GAN: 0.7646, L1: 3.8198, Perceptual: 7.3206)\n",
      "Epoch [24/50], Batch [70/250] | D Loss: 0.5633 | G Loss: 11.9050 (GAN: 0.7646, L1: 3.8198, Perceptual: 7.3206)\n",
      "Epoch [24/50], Batch [80/250] | D Loss: 0.6837 | G Loss: 11.1773 (GAN: 0.7175, L1: 3.2379, Perceptual: 7.2219)\n",
      "Epoch [24/50], Batch [80/250] | D Loss: 0.6837 | G Loss: 11.1773 (GAN: 0.7175, L1: 3.2379, Perceptual: 7.2219)\n",
      "Epoch [24/50], Batch [90/250] | D Loss: 0.9743 | G Loss: 11.1557 (GAN: 0.7102, L1: 3.5145, Perceptual: 6.9310)\n",
      "Epoch [24/50], Batch [90/250] | D Loss: 0.9743 | G Loss: 11.1557 (GAN: 0.7102, L1: 3.5145, Perceptual: 6.9310)\n",
      "Epoch [24/50], Batch [100/250] | D Loss: 0.4932 | G Loss: 12.0156 (GAN: 0.7981, L1: 3.5537, Perceptual: 7.6637)\n",
      "Epoch [24/50], Batch [100/250] | D Loss: 0.4932 | G Loss: 12.0156 (GAN: 0.7981, L1: 3.5537, Perceptual: 7.6637)\n",
      "Epoch [24/50], Batch [110/250] | D Loss: 0.9192 | G Loss: 10.3859 (GAN: 0.9036, L1: 2.9661, Perceptual: 6.5162)\n",
      "Epoch [24/50], Batch [110/250] | D Loss: 0.9192 | G Loss: 10.3859 (GAN: 0.9036, L1: 2.9661, Perceptual: 6.5162)\n",
      "Epoch [24/50], Batch [120/250] | D Loss: 0.4844 | G Loss: 11.8630 (GAN: 0.7095, L1: 3.6291, Perceptual: 7.5244)\n",
      "Epoch [24/50], Batch [120/250] | D Loss: 0.4844 | G Loss: 11.8630 (GAN: 0.7095, L1: 3.6291, Perceptual: 7.5244)\n",
      "Epoch [24/50], Batch [130/250] | D Loss: 0.7650 | G Loss: 11.7857 (GAN: 0.7008, L1: 3.7973, Perceptual: 7.2875)\n",
      "Epoch [24/50], Batch [130/250] | D Loss: 0.7650 | G Loss: 11.7857 (GAN: 0.7008, L1: 3.7973, Perceptual: 7.2875)\n",
      "Epoch [24/50], Batch [140/250] | D Loss: 0.5560 | G Loss: 12.3523 (GAN: 0.8152, L1: 4.0189, Perceptual: 7.5181)\n",
      "Epoch [24/50], Batch [140/250] | D Loss: 0.5560 | G Loss: 12.3523 (GAN: 0.8152, L1: 4.0189, Perceptual: 7.5181)\n",
      "Epoch [24/50], Batch [150/250] | D Loss: 0.6594 | G Loss: 11.9695 (GAN: 0.9211, L1: 3.5591, Perceptual: 7.4893)\n",
      "Epoch [24/50], Batch [150/250] | D Loss: 0.6594 | G Loss: 11.9695 (GAN: 0.9211, L1: 3.5591, Perceptual: 7.4893)\n",
      "Epoch [24/50], Batch [160/250] | D Loss: 0.6537 | G Loss: 12.3456 (GAN: 0.7602, L1: 3.8544, Perceptual: 7.7310)\n",
      "Epoch [24/50], Batch [160/250] | D Loss: 0.6537 | G Loss: 12.3456 (GAN: 0.7602, L1: 3.8544, Perceptual: 7.7310)\n",
      "Epoch [24/50], Batch [170/250] | D Loss: 0.6067 | G Loss: 12.8121 (GAN: 1.1371, L1: 3.8980, Perceptual: 7.7770)\n",
      "Epoch [24/50], Batch [170/250] | D Loss: 0.6067 | G Loss: 12.8121 (GAN: 1.1371, L1: 3.8980, Perceptual: 7.7770)\n",
      "Epoch [24/50], Batch [180/250] | D Loss: 0.6660 | G Loss: 11.6503 (GAN: 0.9924, L1: 3.4536, Perceptual: 7.2042)\n",
      "Epoch [24/50], Batch [180/250] | D Loss: 0.6660 | G Loss: 11.6503 (GAN: 0.9924, L1: 3.4536, Perceptual: 7.2042)\n",
      "Epoch [24/50], Batch [190/250] | D Loss: 0.5813 | G Loss: 11.1224 (GAN: 0.6218, L1: 3.3144, Perceptual: 7.1861)\n",
      "Epoch [24/50], Batch [190/250] | D Loss: 0.5813 | G Loss: 11.1224 (GAN: 0.6218, L1: 3.3144, Perceptual: 7.1861)\n",
      "Epoch [24/50], Batch [200/250] | D Loss: 0.5965 | G Loss: 13.2259 (GAN: 0.9535, L1: 4.1582, Perceptual: 8.1142)\n",
      "Epoch [24/50], Batch [200/250] | D Loss: 0.5965 | G Loss: 13.2259 (GAN: 0.9535, L1: 4.1582, Perceptual: 8.1142)\n",
      "Epoch [24/50], Batch [210/250] | D Loss: 0.6514 | G Loss: 11.2220 (GAN: 0.8217, L1: 3.2021, Perceptual: 7.1981)\n",
      "Epoch [24/50], Batch [210/250] | D Loss: 0.6514 | G Loss: 11.2220 (GAN: 0.8217, L1: 3.2021, Perceptual: 7.1981)\n",
      "Epoch [24/50], Batch [220/250] | D Loss: 0.5545 | G Loss: 12.0725 (GAN: 1.0758, L1: 3.6066, Perceptual: 7.3901)\n",
      "Epoch [24/50], Batch [220/250] | D Loss: 0.5545 | G Loss: 12.0725 (GAN: 1.0758, L1: 3.6066, Perceptual: 7.3901)\n",
      "Epoch [24/50], Batch [230/250] | D Loss: 0.6223 | G Loss: 12.1363 (GAN: 0.8213, L1: 3.6548, Perceptual: 7.6602)\n",
      "Epoch [24/50], Batch [230/250] | D Loss: 0.6223 | G Loss: 12.1363 (GAN: 0.8213, L1: 3.6548, Perceptual: 7.6602)\n",
      "Epoch [24/50], Batch [240/250] | D Loss: 0.6139 | G Loss: 10.6424 (GAN: 0.7526, L1: 3.0928, Perceptual: 6.7970)\n",
      "Epoch [24/50], Batch [240/250] | D Loss: 0.6139 | G Loss: 10.6424 (GAN: 0.7526, L1: 3.0928, Perceptual: 6.7970)\n",
      "Epoch [24/50], Batch [250/250] | D Loss: 0.6042 | G Loss: 11.2187 (GAN: 0.7631, L1: 3.3224, Perceptual: 7.1333)\n",
      "Epoch 24 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [24/50], Batch [250/250] | D Loss: 0.6042 | G Loss: 11.2187 (GAN: 0.7631, L1: 3.3224, Perceptual: 7.1333)\n",
      "Epoch 24 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [25/50], Batch [10/250] | D Loss: 0.4560 | G Loss: 11.8860 (GAN: 0.5115, L1: 3.7411, Perceptual: 7.6334)\n",
      "Epoch [25/50], Batch [10/250] | D Loss: 0.4560 | G Loss: 11.8860 (GAN: 0.5115, L1: 3.7411, Perceptual: 7.6334)\n",
      "Epoch [25/50], Batch [20/250] | D Loss: 0.5463 | G Loss: 11.1558 (GAN: 0.6821, L1: 3.4233, Perceptual: 7.0504)\n",
      "Epoch [25/50], Batch [20/250] | D Loss: 0.5463 | G Loss: 11.1558 (GAN: 0.6821, L1: 3.4233, Perceptual: 7.0504)\n",
      "Epoch [25/50], Batch [30/250] | D Loss: 0.7198 | G Loss: 11.0306 (GAN: 0.9488, L1: 2.9824, Perceptual: 7.0993)\n",
      "Epoch [25/50], Batch [30/250] | D Loss: 0.7198 | G Loss: 11.0306 (GAN: 0.9488, L1: 2.9824, Perceptual: 7.0993)\n",
      "Epoch [25/50], Batch [40/250] | D Loss: 0.7521 | G Loss: 10.6709 (GAN: 0.8711, L1: 3.3093, Perceptual: 6.4905)\n",
      "Epoch [25/50], Batch [40/250] | D Loss: 0.7521 | G Loss: 10.6709 (GAN: 0.8711, L1: 3.3093, Perceptual: 6.4905)\n",
      "Epoch [25/50], Batch [50/250] | D Loss: 0.5816 | G Loss: 11.3650 (GAN: 0.9116, L1: 3.2009, Perceptual: 7.2525)\n",
      "Epoch [25/50], Batch [50/250] | D Loss: 0.5816 | G Loss: 11.3650 (GAN: 0.9116, L1: 3.2009, Perceptual: 7.2525)\n",
      "Epoch [25/50], Batch [60/250] | D Loss: 0.7269 | G Loss: 12.1341 (GAN: 1.0256, L1: 3.4221, Perceptual: 7.6864)\n",
      "Epoch [25/50], Batch [60/250] | D Loss: 0.7269 | G Loss: 12.1341 (GAN: 1.0256, L1: 3.4221, Perceptual: 7.6864)\n",
      "Epoch [25/50], Batch [70/250] | D Loss: 0.7049 | G Loss: 10.8145 (GAN: 0.8397, L1: 3.1167, Perceptual: 6.8582)\n",
      "Epoch [25/50], Batch [70/250] | D Loss: 0.7049 | G Loss: 10.8145 (GAN: 0.8397, L1: 3.1167, Perceptual: 6.8582)\n",
      "Epoch [25/50], Batch [80/250] | D Loss: 0.5372 | G Loss: 11.8130 (GAN: 0.7361, L1: 3.5313, Perceptual: 7.5456)\n",
      "Epoch [25/50], Batch [80/250] | D Loss: 0.5372 | G Loss: 11.8130 (GAN: 0.7361, L1: 3.5313, Perceptual: 7.5456)\n",
      "Epoch [25/50], Batch [90/250] | D Loss: 0.6196 | G Loss: 11.0511 (GAN: 0.7329, L1: 3.2533, Perceptual: 7.0649)\n",
      "Epoch [25/50], Batch [90/250] | D Loss: 0.6196 | G Loss: 11.0511 (GAN: 0.7329, L1: 3.2533, Perceptual: 7.0649)\n",
      "Epoch [25/50], Batch [100/250] | D Loss: 0.5743 | G Loss: 10.9703 (GAN: 0.6096, L1: 3.3154, Perceptual: 7.0454)\n",
      "Epoch [25/50], Batch [100/250] | D Loss: 0.5743 | G Loss: 10.9703 (GAN: 0.6096, L1: 3.3154, Perceptual: 7.0454)\n",
      "Epoch [25/50], Batch [110/250] | D Loss: 0.5459 | G Loss: 11.9568 (GAN: 0.8503, L1: 3.6769, Perceptual: 7.4297)\n",
      "Epoch [25/50], Batch [110/250] | D Loss: 0.5459 | G Loss: 11.9568 (GAN: 0.8503, L1: 3.6769, Perceptual: 7.4297)\n",
      "Epoch [25/50], Batch [120/250] | D Loss: 0.5713 | G Loss: 11.8141 (GAN: 0.6509, L1: 4.0802, Perceptual: 7.0829)\n",
      "Epoch [25/50], Batch [120/250] | D Loss: 0.5713 | G Loss: 11.8141 (GAN: 0.6509, L1: 4.0802, Perceptual: 7.0829)\n",
      "Epoch [25/50], Batch [130/250] | D Loss: 0.6472 | G Loss: 12.1180 (GAN: 1.0524, L1: 3.5670, Perceptual: 7.4985)\n",
      "Epoch [25/50], Batch [130/250] | D Loss: 0.6472 | G Loss: 12.1180 (GAN: 1.0524, L1: 3.5670, Perceptual: 7.4985)\n",
      "Epoch [25/50], Batch [140/250] | D Loss: 0.5938 | G Loss: 10.9750 (GAN: 0.7755, L1: 3.1131, Perceptual: 7.0864)\n",
      "Epoch [25/50], Batch [140/250] | D Loss: 0.5938 | G Loss: 10.9750 (GAN: 0.7755, L1: 3.1131, Perceptual: 7.0864)\n",
      "Epoch [25/50], Batch [150/250] | D Loss: 0.6800 | G Loss: 10.5005 (GAN: 0.6946, L1: 3.2165, Perceptual: 6.5893)\n",
      "Epoch [25/50], Batch [150/250] | D Loss: 0.6800 | G Loss: 10.5005 (GAN: 0.6946, L1: 3.2165, Perceptual: 6.5893)\n",
      "Epoch [25/50], Batch [160/250] | D Loss: 0.6275 | G Loss: 11.4392 (GAN: 0.7778, L1: 3.3194, Perceptual: 7.3420)\n",
      "Epoch [25/50], Batch [160/250] | D Loss: 0.6275 | G Loss: 11.4392 (GAN: 0.7778, L1: 3.3194, Perceptual: 7.3420)\n",
      "Epoch [25/50], Batch [170/250] | D Loss: 0.6621 | G Loss: 9.9879 (GAN: 0.9039, L1: 2.7421, Perceptual: 6.3419)\n",
      "Epoch [25/50], Batch [170/250] | D Loss: 0.6621 | G Loss: 9.9879 (GAN: 0.9039, L1: 2.7421, Perceptual: 6.3419)\n",
      "Epoch [25/50], Batch [180/250] | D Loss: 0.4710 | G Loss: 12.3163 (GAN: 0.7564, L1: 3.7536, Perceptual: 7.8064)\n",
      "Epoch [25/50], Batch [180/250] | D Loss: 0.4710 | G Loss: 12.3163 (GAN: 0.7564, L1: 3.7536, Perceptual: 7.8064)\n",
      "Epoch [25/50], Batch [190/250] | D Loss: 0.5374 | G Loss: 11.4600 (GAN: 0.6888, L1: 3.4069, Perceptual: 7.3642)\n",
      "Epoch [25/50], Batch [190/250] | D Loss: 0.5374 | G Loss: 11.4600 (GAN: 0.6888, L1: 3.4069, Perceptual: 7.3642)\n",
      "Epoch [25/50], Batch [200/250] | D Loss: 0.5238 | G Loss: 11.9451 (GAN: 0.9777, L1: 3.4602, Perceptual: 7.5071)\n",
      "Epoch [25/50], Batch [200/250] | D Loss: 0.5238 | G Loss: 11.9451 (GAN: 0.9777, L1: 3.4602, Perceptual: 7.5071)\n",
      "Epoch [25/50], Batch [210/250] | D Loss: 0.6507 | G Loss: 11.1879 (GAN: 0.7241, L1: 3.2047, Perceptual: 7.2590)\n",
      "Epoch [25/50], Batch [210/250] | D Loss: 0.6507 | G Loss: 11.1879 (GAN: 0.7241, L1: 3.2047, Perceptual: 7.2590)\n",
      "Epoch [25/50], Batch [220/250] | D Loss: 0.5859 | G Loss: 11.3632 (GAN: 0.8622, L1: 3.4276, Perceptual: 7.0733)\n",
      "Epoch [25/50], Batch [220/250] | D Loss: 0.5859 | G Loss: 11.3632 (GAN: 0.8622, L1: 3.4276, Perceptual: 7.0733)\n",
      "Epoch [25/50], Batch [230/250] | D Loss: 0.5044 | G Loss: 11.8123 (GAN: 0.7211, L1: 3.6828, Perceptual: 7.4084)\n",
      "Epoch [25/50], Batch [230/250] | D Loss: 0.5044 | G Loss: 11.8123 (GAN: 0.7211, L1: 3.6828, Perceptual: 7.4084)\n",
      "Epoch [25/50], Batch [240/250] | D Loss: 0.7178 | G Loss: 10.6463 (GAN: 0.7305, L1: 3.2699, Perceptual: 6.6460)\n",
      "Epoch [25/50], Batch [240/250] | D Loss: 0.7178 | G Loss: 10.6463 (GAN: 0.7305, L1: 3.2699, Perceptual: 6.6460)\n",
      "Epoch [25/50], Batch [250/250] | D Loss: 0.7711 | G Loss: 11.7475 (GAN: 0.9818, L1: 3.3954, Perceptual: 7.3703)\n",
      "Epoch 25 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 25 ---\n",
      "Saving samples and checkpoint for epoch 25...\n",
      "Epoch [25/50], Batch [250/250] | D Loss: 0.7711 | G Loss: 11.7475 (GAN: 0.9818, L1: 3.3954, Perceptual: 7.3703)\n",
      "Epoch 25 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 25 ---\n",
      "Saving samples and checkpoint for epoch 25...\n",
      "Successfully saved image to: output/generated_images\\epoch_25_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_25_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [26/50], Batch [10/250] | D Loss: 0.6894 | G Loss: 9.7062 (GAN: 0.6711, L1: 2.7227, Perceptual: 6.3123)\n",
      "Epoch [26/50], Batch [10/250] | D Loss: 0.6894 | G Loss: 9.7062 (GAN: 0.6711, L1: 2.7227, Perceptual: 6.3123)\n",
      "Epoch [26/50], Batch [20/250] | D Loss: 0.5827 | G Loss: 11.7658 (GAN: 0.8525, L1: 3.8301, Perceptual: 7.0832)\n",
      "Epoch [26/50], Batch [20/250] | D Loss: 0.5827 | G Loss: 11.7658 (GAN: 0.8525, L1: 3.8301, Perceptual: 7.0832)\n",
      "Epoch [26/50], Batch [30/250] | D Loss: 0.6134 | G Loss: 11.7113 (GAN: 0.7292, L1: 3.5738, Perceptual: 7.4083)\n",
      "Epoch [26/50], Batch [30/250] | D Loss: 0.6134 | G Loss: 11.7113 (GAN: 0.7292, L1: 3.5738, Perceptual: 7.4083)\n",
      "Epoch [26/50], Batch [40/250] | D Loss: 0.5951 | G Loss: 10.7462 (GAN: 0.5760, L1: 3.0649, Perceptual: 7.1053)\n",
      "Epoch [26/50], Batch [40/250] | D Loss: 0.5951 | G Loss: 10.7462 (GAN: 0.5760, L1: 3.0649, Perceptual: 7.1053)\n",
      "Epoch [26/50], Batch [50/250] | D Loss: 0.6094 | G Loss: 11.0728 (GAN: 0.8819, L1: 2.9728, Perceptual: 7.2182)\n",
      "Epoch [26/50], Batch [50/250] | D Loss: 0.6094 | G Loss: 11.0728 (GAN: 0.8819, L1: 2.9728, Perceptual: 7.2182)\n",
      "Epoch [26/50], Batch [60/250] | D Loss: 0.7114 | G Loss: 11.3297 (GAN: 0.5636, L1: 3.4712, Perceptual: 7.2950)\n",
      "Epoch [26/50], Batch [60/250] | D Loss: 0.7114 | G Loss: 11.3297 (GAN: 0.5636, L1: 3.4712, Perceptual: 7.2950)\n",
      "Epoch [26/50], Batch [70/250] | D Loss: 0.7154 | G Loss: 11.4078 (GAN: 0.7324, L1: 3.5020, Perceptual: 7.1734)\n",
      "Epoch [26/50], Batch [70/250] | D Loss: 0.7154 | G Loss: 11.4078 (GAN: 0.7324, L1: 3.5020, Perceptual: 7.1734)\n",
      "Epoch [26/50], Batch [80/250] | D Loss: 0.4626 | G Loss: 11.4152 (GAN: 0.6501, L1: 3.3360, Perceptual: 7.4291)\n",
      "Epoch [26/50], Batch [80/250] | D Loss: 0.4626 | G Loss: 11.4152 (GAN: 0.6501, L1: 3.3360, Perceptual: 7.4291)\n",
      "Epoch [26/50], Batch [90/250] | D Loss: 0.6659 | G Loss: 11.2072 (GAN: 0.8164, L1: 3.2955, Perceptual: 7.0954)\n",
      "Epoch [26/50], Batch [90/250] | D Loss: 0.6659 | G Loss: 11.2072 (GAN: 0.8164, L1: 3.2955, Perceptual: 7.0954)\n",
      "Epoch [26/50], Batch [100/250] | D Loss: 0.7518 | G Loss: 10.0859 (GAN: 0.8245, L1: 2.7784, Perceptual: 6.4830)\n",
      "Epoch [26/50], Batch [100/250] | D Loss: 0.7518 | G Loss: 10.0859 (GAN: 0.8245, L1: 2.7784, Perceptual: 6.4830)\n",
      "Epoch [26/50], Batch [110/250] | D Loss: 0.6421 | G Loss: 11.0658 (GAN: 0.7295, L1: 3.0518, Perceptual: 7.2844)\n",
      "Epoch [26/50], Batch [110/250] | D Loss: 0.6421 | G Loss: 11.0658 (GAN: 0.7295, L1: 3.0518, Perceptual: 7.2844)\n",
      "Epoch [26/50], Batch [120/250] | D Loss: 0.6401 | G Loss: 11.1433 (GAN: 0.8597, L1: 3.3238, Perceptual: 6.9598)\n",
      "Epoch [26/50], Batch [120/250] | D Loss: 0.6401 | G Loss: 11.1433 (GAN: 0.8597, L1: 3.3238, Perceptual: 6.9598)\n",
      "Epoch [26/50], Batch [130/250] | D Loss: 0.6653 | G Loss: 11.0200 (GAN: 0.8050, L1: 3.3582, Perceptual: 6.8567)\n",
      "Epoch [26/50], Batch [130/250] | D Loss: 0.6653 | G Loss: 11.0200 (GAN: 0.8050, L1: 3.3582, Perceptual: 6.8567)\n",
      "Epoch [26/50], Batch [140/250] | D Loss: 0.7900 | G Loss: 11.3267 (GAN: 1.1169, L1: 3.0628, Perceptual: 7.1469)\n",
      "Epoch [26/50], Batch [140/250] | D Loss: 0.7900 | G Loss: 11.3267 (GAN: 1.1169, L1: 3.0628, Perceptual: 7.1469)\n",
      "Epoch [26/50], Batch [150/250] | D Loss: 0.5924 | G Loss: 12.9369 (GAN: 0.7344, L1: 4.3440, Perceptual: 7.8586)\n",
      "Epoch [26/50], Batch [150/250] | D Loss: 0.5924 | G Loss: 12.9369 (GAN: 0.7344, L1: 4.3440, Perceptual: 7.8586)\n",
      "Epoch [26/50], Batch [160/250] | D Loss: 0.4860 | G Loss: 12.6333 (GAN: 0.8278, L1: 3.6913, Perceptual: 8.1142)\n",
      "Epoch [26/50], Batch [160/250] | D Loss: 0.4860 | G Loss: 12.6333 (GAN: 0.8278, L1: 3.6913, Perceptual: 8.1142)\n",
      "Epoch [26/50], Batch [170/250] | D Loss: 0.5354 | G Loss: 12.1975 (GAN: 0.8253, L1: 3.4566, Perceptual: 7.9156)\n",
      "Epoch [26/50], Batch [170/250] | D Loss: 0.5354 | G Loss: 12.1975 (GAN: 0.8253, L1: 3.4566, Perceptual: 7.9156)\n",
      "Epoch [26/50], Batch [180/250] | D Loss: 0.5050 | G Loss: 11.4087 (GAN: 0.5976, L1: 3.5306, Perceptual: 7.2805)\n",
      "Epoch [26/50], Batch [180/250] | D Loss: 0.5050 | G Loss: 11.4087 (GAN: 0.5976, L1: 3.5306, Perceptual: 7.2805)\n",
      "Epoch [26/50], Batch [190/250] | D Loss: 0.7798 | G Loss: 11.3330 (GAN: 0.8275, L1: 3.3934, Perceptual: 7.1121)\n",
      "Epoch [26/50], Batch [190/250] | D Loss: 0.7798 | G Loss: 11.3330 (GAN: 0.8275, L1: 3.3934, Perceptual: 7.1121)\n",
      "Epoch [26/50], Batch [200/250] | D Loss: 0.6298 | G Loss: 10.3951 (GAN: 0.7128, L1: 3.1888, Perceptual: 6.4935)\n",
      "Epoch [26/50], Batch [200/250] | D Loss: 0.6298 | G Loss: 10.3951 (GAN: 0.7128, L1: 3.1888, Perceptual: 6.4935)\n",
      "Epoch [26/50], Batch [210/250] | D Loss: 0.5313 | G Loss: 10.9802 (GAN: 0.7303, L1: 3.1221, Perceptual: 7.1278)\n",
      "Epoch [26/50], Batch [210/250] | D Loss: 0.5313 | G Loss: 10.9802 (GAN: 0.7303, L1: 3.1221, Perceptual: 7.1278)\n",
      "Epoch [26/50], Batch [220/250] | D Loss: 0.5819 | G Loss: 11.6362 (GAN: 0.9461, L1: 3.1549, Perceptual: 7.5352)\n",
      "Epoch [26/50], Batch [220/250] | D Loss: 0.5819 | G Loss: 11.6362 (GAN: 0.9461, L1: 3.1549, Perceptual: 7.5352)\n",
      "Epoch [26/50], Batch [230/250] | D Loss: 0.5392 | G Loss: 11.3285 (GAN: 0.7191, L1: 3.2512, Perceptual: 7.3582)\n",
      "Epoch [26/50], Batch [230/250] | D Loss: 0.5392 | G Loss: 11.3285 (GAN: 0.7191, L1: 3.2512, Perceptual: 7.3582)\n",
      "Epoch [26/50], Batch [240/250] | D Loss: 0.7060 | G Loss: 12.9243 (GAN: 0.9499, L1: 4.1014, Perceptual: 7.8730)\n",
      "Epoch [26/50], Batch [240/250] | D Loss: 0.7060 | G Loss: 12.9243 (GAN: 0.9499, L1: 4.1014, Perceptual: 7.8730)\n",
      "Epoch [26/50], Batch [250/250] | D Loss: 0.6969 | G Loss: 11.1210 (GAN: 0.7932, L1: 3.2646, Perceptual: 7.0631)\n",
      "Epoch 26 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [26/50], Batch [250/250] | D Loss: 0.6969 | G Loss: 11.1210 (GAN: 0.7932, L1: 3.2646, Perceptual: 7.0631)\n",
      "Epoch 26 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [27/50], Batch [10/250] | D Loss: 0.5019 | G Loss: 10.6785 (GAN: 0.9106, L1: 3.3058, Perceptual: 6.4620)\n",
      "Epoch [27/50], Batch [10/250] | D Loss: 0.5019 | G Loss: 10.6785 (GAN: 0.9106, L1: 3.3058, Perceptual: 6.4620)\n",
      "Epoch [27/50], Batch [20/250] | D Loss: 0.4279 | G Loss: 12.2558 (GAN: 1.0433, L1: 3.5378, Perceptual: 7.6746)\n",
      "Epoch [27/50], Batch [20/250] | D Loss: 0.4279 | G Loss: 12.2558 (GAN: 1.0433, L1: 3.5378, Perceptual: 7.6746)\n",
      "Epoch [27/50], Batch [30/250] | D Loss: 0.6878 | G Loss: 10.8036 (GAN: 0.8320, L1: 3.1069, Perceptual: 6.8648)\n",
      "Epoch [27/50], Batch [30/250] | D Loss: 0.6878 | G Loss: 10.8036 (GAN: 0.8320, L1: 3.1069, Perceptual: 6.8648)\n",
      "Epoch [27/50], Batch [40/250] | D Loss: 0.5833 | G Loss: 11.7795 (GAN: 0.9305, L1: 3.3755, Perceptual: 7.4735)\n",
      "Epoch [27/50], Batch [40/250] | D Loss: 0.5833 | G Loss: 11.7795 (GAN: 0.9305, L1: 3.3755, Perceptual: 7.4735)\n",
      "Epoch [27/50], Batch [50/250] | D Loss: 0.6404 | G Loss: 11.0843 (GAN: 0.9233, L1: 2.9366, Perceptual: 7.2244)\n",
      "Epoch [27/50], Batch [50/250] | D Loss: 0.6404 | G Loss: 11.0843 (GAN: 0.9233, L1: 2.9366, Perceptual: 7.2244)\n",
      "Epoch [27/50], Batch [60/250] | D Loss: 0.5705 | G Loss: 12.9731 (GAN: 0.8534, L1: 3.9911, Perceptual: 8.1287)\n",
      "Epoch [27/50], Batch [60/250] | D Loss: 0.5705 | G Loss: 12.9731 (GAN: 0.8534, L1: 3.9911, Perceptual: 8.1287)\n",
      "Epoch [27/50], Batch [70/250] | D Loss: 0.4719 | G Loss: 12.3453 (GAN: 0.7829, L1: 3.7541, Perceptual: 7.8083)\n",
      "Epoch [27/50], Batch [70/250] | D Loss: 0.4719 | G Loss: 12.3453 (GAN: 0.7829, L1: 3.7541, Perceptual: 7.8083)\n",
      "Epoch [27/50], Batch [80/250] | D Loss: 0.5538 | G Loss: 13.6528 (GAN: 1.0789, L1: 4.1113, Perceptual: 8.4626)\n",
      "Epoch [27/50], Batch [80/250] | D Loss: 0.5538 | G Loss: 13.6528 (GAN: 1.0789, L1: 4.1113, Perceptual: 8.4626)\n",
      "Epoch [27/50], Batch [90/250] | D Loss: 0.5914 | G Loss: 12.8071 (GAN: 1.0711, L1: 3.8383, Perceptual: 7.8977)\n",
      "Epoch [27/50], Batch [90/250] | D Loss: 0.5914 | G Loss: 12.8071 (GAN: 1.0711, L1: 3.8383, Perceptual: 7.8977)\n",
      "Epoch [27/50], Batch [100/250] | D Loss: 0.6298 | G Loss: 11.0353 (GAN: 0.6500, L1: 3.5065, Perceptual: 6.8788)\n",
      "Epoch [27/50], Batch [100/250] | D Loss: 0.6298 | G Loss: 11.0353 (GAN: 0.6500, L1: 3.5065, Perceptual: 6.8788)\n",
      "Epoch [27/50], Batch [110/250] | D Loss: 0.5656 | G Loss: 11.7211 (GAN: 0.6453, L1: 3.6617, Perceptual: 7.4141)\n",
      "Epoch [27/50], Batch [110/250] | D Loss: 0.5656 | G Loss: 11.7211 (GAN: 0.6453, L1: 3.6617, Perceptual: 7.4141)\n",
      "Epoch [27/50], Batch [120/250] | D Loss: 0.5349 | G Loss: 11.9635 (GAN: 0.8396, L1: 3.3692, Perceptual: 7.7547)\n",
      "Epoch [27/50], Batch [120/250] | D Loss: 0.5349 | G Loss: 11.9635 (GAN: 0.8396, L1: 3.3692, Perceptual: 7.7547)\n",
      "Epoch [27/50], Batch [130/250] | D Loss: 0.6495 | G Loss: 11.2451 (GAN: 0.7470, L1: 3.5336, Perceptual: 6.9645)\n",
      "Epoch [27/50], Batch [130/250] | D Loss: 0.6495 | G Loss: 11.2451 (GAN: 0.7470, L1: 3.5336, Perceptual: 6.9645)\n",
      "Epoch [27/50], Batch [140/250] | D Loss: 0.7803 | G Loss: 11.6792 (GAN: 1.0744, L1: 3.1827, Perceptual: 7.4221)\n",
      "Epoch [27/50], Batch [140/250] | D Loss: 0.7803 | G Loss: 11.6792 (GAN: 1.0744, L1: 3.1827, Perceptual: 7.4221)\n",
      "Epoch [27/50], Batch [150/250] | D Loss: 0.4118 | G Loss: 12.1651 (GAN: 0.6330, L1: 4.0043, Perceptual: 7.5279)\n",
      "Epoch [27/50], Batch [150/250] | D Loss: 0.4118 | G Loss: 12.1651 (GAN: 0.6330, L1: 4.0043, Perceptual: 7.5279)\n",
      "Epoch [27/50], Batch [160/250] | D Loss: 0.7777 | G Loss: 11.1453 (GAN: 0.6586, L1: 3.2900, Perceptual: 7.1967)\n",
      "Epoch [27/50], Batch [160/250] | D Loss: 0.7777 | G Loss: 11.1453 (GAN: 0.6586, L1: 3.2900, Perceptual: 7.1967)\n",
      "Epoch [27/50], Batch [170/250] | D Loss: 0.6088 | G Loss: 11.5989 (GAN: 0.7818, L1: 3.6652, Perceptual: 7.1519)\n",
      "Epoch [27/50], Batch [170/250] | D Loss: 0.6088 | G Loss: 11.5989 (GAN: 0.7818, L1: 3.6652, Perceptual: 7.1519)\n",
      "Epoch [27/50], Batch [180/250] | D Loss: 0.6390 | G Loss: 11.0109 (GAN: 0.9119, L1: 2.9603, Perceptual: 7.1387)\n",
      "Epoch [27/50], Batch [180/250] | D Loss: 0.6390 | G Loss: 11.0109 (GAN: 0.9119, L1: 2.9603, Perceptual: 7.1387)\n",
      "Epoch [27/50], Batch [190/250] | D Loss: 0.7019 | G Loss: 9.9218 (GAN: 0.8603, L1: 2.6328, Perceptual: 6.4287)\n",
      "Epoch [27/50], Batch [190/250] | D Loss: 0.7019 | G Loss: 9.9218 (GAN: 0.8603, L1: 2.6328, Perceptual: 6.4287)\n",
      "Epoch [27/50], Batch [200/250] | D Loss: 0.6491 | G Loss: 12.7711 (GAN: 0.8924, L1: 3.9515, Perceptual: 7.9272)\n",
      "Epoch [27/50], Batch [200/250] | D Loss: 0.6491 | G Loss: 12.7711 (GAN: 0.8924, L1: 3.9515, Perceptual: 7.9272)\n",
      "Epoch [27/50], Batch [210/250] | D Loss: 0.5546 | G Loss: 12.3472 (GAN: 0.9010, L1: 3.9641, Perceptual: 7.4821)\n",
      "Epoch [27/50], Batch [210/250] | D Loss: 0.5546 | G Loss: 12.3472 (GAN: 0.9010, L1: 3.9641, Perceptual: 7.4821)\n",
      "Epoch [27/50], Batch [220/250] | D Loss: 0.7549 | G Loss: 11.9164 (GAN: 0.8615, L1: 3.5330, Perceptual: 7.5220)\n",
      "Epoch [27/50], Batch [220/250] | D Loss: 0.7549 | G Loss: 11.9164 (GAN: 0.8615, L1: 3.5330, Perceptual: 7.5220)\n",
      "Epoch [27/50], Batch [230/250] | D Loss: 0.6663 | G Loss: 12.5799 (GAN: 0.8422, L1: 4.4765, Perceptual: 7.2612)\n",
      "Epoch [27/50], Batch [230/250] | D Loss: 0.6663 | G Loss: 12.5799 (GAN: 0.8422, L1: 4.4765, Perceptual: 7.2612)\n",
      "Epoch [27/50], Batch [240/250] | D Loss: 0.6870 | G Loss: 10.8541 (GAN: 0.6909, L1: 3.0219, Perceptual: 7.1413)\n",
      "Epoch [27/50], Batch [240/250] | D Loss: 0.6870 | G Loss: 10.8541 (GAN: 0.6909, L1: 3.0219, Perceptual: 7.1413)\n",
      "Epoch [27/50], Batch [250/250] | D Loss: 0.4280 | G Loss: 11.9205 (GAN: 0.7530, L1: 3.5028, Perceptual: 7.6647)\n",
      "Epoch 27 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [27/50], Batch [250/250] | D Loss: 0.4280 | G Loss: 11.9205 (GAN: 0.7530, L1: 3.5028, Perceptual: 7.6647)\n",
      "Epoch 27 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [28/50], Batch [10/250] | D Loss: 0.6363 | G Loss: 11.7779 (GAN: 0.7623, L1: 3.8035, Perceptual: 7.2121)\n",
      "Epoch [28/50], Batch [10/250] | D Loss: 0.6363 | G Loss: 11.7779 (GAN: 0.7623, L1: 3.8035, Perceptual: 7.2121)\n",
      "Epoch [28/50], Batch [20/250] | D Loss: 0.5377 | G Loss: 12.2953 (GAN: 0.8449, L1: 3.7265, Perceptual: 7.7238)\n",
      "Epoch [28/50], Batch [20/250] | D Loss: 0.5377 | G Loss: 12.2953 (GAN: 0.8449, L1: 3.7265, Perceptual: 7.7238)\n",
      "Epoch [28/50], Batch [30/250] | D Loss: 0.6615 | G Loss: 10.9617 (GAN: 0.7489, L1: 3.1263, Perceptual: 7.0865)\n",
      "Epoch [28/50], Batch [30/250] | D Loss: 0.6615 | G Loss: 10.9617 (GAN: 0.7489, L1: 3.1263, Perceptual: 7.0865)\n",
      "Epoch [28/50], Batch [40/250] | D Loss: 0.6063 | G Loss: 11.0597 (GAN: 0.7421, L1: 3.1872, Perceptual: 7.1304)\n",
      "Epoch [28/50], Batch [40/250] | D Loss: 0.6063 | G Loss: 11.0597 (GAN: 0.7421, L1: 3.1872, Perceptual: 7.1304)\n",
      "Epoch [28/50], Batch [50/250] | D Loss: 0.6626 | G Loss: 11.6629 (GAN: 0.9262, L1: 3.3023, Perceptual: 7.4344)\n",
      "Epoch [28/50], Batch [50/250] | D Loss: 0.6626 | G Loss: 11.6629 (GAN: 0.9262, L1: 3.3023, Perceptual: 7.4344)\n",
      "Epoch [28/50], Batch [60/250] | D Loss: 0.5790 | G Loss: 11.7647 (GAN: 0.8866, L1: 3.4190, Perceptual: 7.4591)\n",
      "Epoch [28/50], Batch [60/250] | D Loss: 0.5790 | G Loss: 11.7647 (GAN: 0.8866, L1: 3.4190, Perceptual: 7.4591)\n",
      "Epoch [28/50], Batch [70/250] | D Loss: 0.8042 | G Loss: 10.6084 (GAN: 0.9375, L1: 2.8225, Perceptual: 6.8483)\n",
      "Epoch [28/50], Batch [70/250] | D Loss: 0.8042 | G Loss: 10.6084 (GAN: 0.9375, L1: 2.8225, Perceptual: 6.8483)\n",
      "Epoch [28/50], Batch [80/250] | D Loss: 0.7781 | G Loss: 11.2800 (GAN: 0.8388, L1: 3.0791, Perceptual: 7.3620)\n",
      "Epoch [28/50], Batch [80/250] | D Loss: 0.7781 | G Loss: 11.2800 (GAN: 0.8388, L1: 3.0791, Perceptual: 7.3620)\n",
      "Epoch [28/50], Batch [90/250] | D Loss: 0.6332 | G Loss: 13.2008 (GAN: 0.9378, L1: 4.3186, Perceptual: 7.9443)\n",
      "Epoch [28/50], Batch [90/250] | D Loss: 0.6332 | G Loss: 13.2008 (GAN: 0.9378, L1: 4.3186, Perceptual: 7.9443)\n",
      "Epoch [28/50], Batch [100/250] | D Loss: 0.5050 | G Loss: 12.2618 (GAN: 0.8997, L1: 3.7447, Perceptual: 7.6174)\n",
      "Epoch [28/50], Batch [100/250] | D Loss: 0.5050 | G Loss: 12.2618 (GAN: 0.8997, L1: 3.7447, Perceptual: 7.6174)\n",
      "Epoch [28/50], Batch [110/250] | D Loss: 0.9269 | G Loss: 10.8027 (GAN: 0.9680, L1: 3.0448, Perceptual: 6.7899)\n",
      "Epoch [28/50], Batch [110/250] | D Loss: 0.9269 | G Loss: 10.8027 (GAN: 0.9680, L1: 3.0448, Perceptual: 6.7899)\n",
      "Epoch [28/50], Batch [120/250] | D Loss: 0.6433 | G Loss: 10.0848 (GAN: 0.7601, L1: 2.8763, Perceptual: 6.4484)\n",
      "Epoch [28/50], Batch [120/250] | D Loss: 0.6433 | G Loss: 10.0848 (GAN: 0.7601, L1: 2.8763, Perceptual: 6.4484)\n",
      "Epoch [28/50], Batch [130/250] | D Loss: 0.7764 | G Loss: 11.1138 (GAN: 0.8282, L1: 3.2672, Perceptual: 7.0184)\n",
      "Epoch [28/50], Batch [130/250] | D Loss: 0.7764 | G Loss: 11.1138 (GAN: 0.8282, L1: 3.2672, Perceptual: 7.0184)\n",
      "Epoch [28/50], Batch [140/250] | D Loss: 0.6744 | G Loss: 12.3272 (GAN: 0.9405, L1: 3.7747, Perceptual: 7.6120)\n",
      "Epoch [28/50], Batch [140/250] | D Loss: 0.6744 | G Loss: 12.3272 (GAN: 0.9405, L1: 3.7747, Perceptual: 7.6120)\n",
      "Epoch [28/50], Batch [150/250] | D Loss: 0.6435 | G Loss: 11.4681 (GAN: 0.8214, L1: 3.4215, Perceptual: 7.2252)\n",
      "Epoch [28/50], Batch [150/250] | D Loss: 0.6435 | G Loss: 11.4681 (GAN: 0.8214, L1: 3.4215, Perceptual: 7.2252)\n",
      "Epoch [28/50], Batch [160/250] | D Loss: 0.7521 | G Loss: 10.6705 (GAN: 0.9692, L1: 2.8216, Perceptual: 6.8797)\n",
      "Epoch [28/50], Batch [160/250] | D Loss: 0.7521 | G Loss: 10.6705 (GAN: 0.9692, L1: 2.8216, Perceptual: 6.8797)\n",
      "Epoch [28/50], Batch [170/250] | D Loss: 1.0011 | G Loss: 12.2277 (GAN: 0.9891, L1: 3.7225, Perceptual: 7.5162)\n",
      "Epoch [28/50], Batch [170/250] | D Loss: 1.0011 | G Loss: 12.2277 (GAN: 0.9891, L1: 3.7225, Perceptual: 7.5162)\n",
      "Epoch [28/50], Batch [180/250] | D Loss: 0.4782 | G Loss: 13.3075 (GAN: 0.9576, L1: 4.2007, Perceptual: 8.1492)\n",
      "Epoch [28/50], Batch [180/250] | D Loss: 0.4782 | G Loss: 13.3075 (GAN: 0.9576, L1: 4.2007, Perceptual: 8.1492)\n",
      "Epoch [28/50], Batch [190/250] | D Loss: 0.6415 | G Loss: 12.2742 (GAN: 0.7807, L1: 3.7244, Perceptual: 7.7692)\n",
      "Epoch [28/50], Batch [190/250] | D Loss: 0.6415 | G Loss: 12.2742 (GAN: 0.7807, L1: 3.7244, Perceptual: 7.7692)\n",
      "Epoch [28/50], Batch [200/250] | D Loss: 0.5768 | G Loss: 11.9098 (GAN: 0.8324, L1: 3.4370, Perceptual: 7.6404)\n",
      "Epoch [28/50], Batch [200/250] | D Loss: 0.5768 | G Loss: 11.9098 (GAN: 0.8324, L1: 3.4370, Perceptual: 7.6404)\n",
      "Epoch [28/50], Batch [210/250] | D Loss: 0.7271 | G Loss: 11.1531 (GAN: 0.8098, L1: 3.3613, Perceptual: 6.9820)\n",
      "Epoch [28/50], Batch [210/250] | D Loss: 0.7271 | G Loss: 11.1531 (GAN: 0.8098, L1: 3.3613, Perceptual: 6.9820)\n",
      "Epoch [28/50], Batch [220/250] | D Loss: 0.7009 | G Loss: 11.8576 (GAN: 0.8757, L1: 3.2762, Perceptual: 7.7057)\n",
      "Epoch [28/50], Batch [220/250] | D Loss: 0.7009 | G Loss: 11.8576 (GAN: 0.8757, L1: 3.2762, Perceptual: 7.7057)\n",
      "Epoch [28/50], Batch [230/250] | D Loss: 0.6526 | G Loss: 11.9305 (GAN: 0.7919, L1: 3.8382, Perceptual: 7.3003)\n",
      "Epoch [28/50], Batch [230/250] | D Loss: 0.6526 | G Loss: 11.9305 (GAN: 0.7919, L1: 3.8382, Perceptual: 7.3003)\n",
      "Epoch [28/50], Batch [240/250] | D Loss: 0.5657 | G Loss: 9.6268 (GAN: 0.6753, L1: 2.5729, Perceptual: 6.3787)\n",
      "Epoch [28/50], Batch [240/250] | D Loss: 0.5657 | G Loss: 9.6268 (GAN: 0.6753, L1: 2.5729, Perceptual: 6.3787)\n",
      "Epoch [28/50], Batch [250/250] | D Loss: 0.6444 | G Loss: 10.6271 (GAN: 0.7836, L1: 3.3643, Perceptual: 6.4792)\n",
      "Epoch 28 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [28/50], Batch [250/250] | D Loss: 0.6444 | G Loss: 10.6271 (GAN: 0.7836, L1: 3.3643, Perceptual: 6.4792)\n",
      "Epoch 28 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [29/50], Batch [10/250] | D Loss: 0.7456 | G Loss: 11.1858 (GAN: 0.9949, L1: 3.0132, Perceptual: 7.1777)\n",
      "Epoch [29/50], Batch [10/250] | D Loss: 0.7456 | G Loss: 11.1858 (GAN: 0.9949, L1: 3.0132, Perceptual: 7.1777)\n",
      "Epoch [29/50], Batch [20/250] | D Loss: 0.5792 | G Loss: 11.1328 (GAN: 0.7607, L1: 3.4371, Perceptual: 6.9350)\n",
      "Epoch [29/50], Batch [20/250] | D Loss: 0.5792 | G Loss: 11.1328 (GAN: 0.7607, L1: 3.4371, Perceptual: 6.9350)\n",
      "Epoch [29/50], Batch [30/250] | D Loss: 0.7966 | G Loss: 11.6225 (GAN: 0.8677, L1: 3.4133, Perceptual: 7.3415)\n",
      "Epoch [29/50], Batch [30/250] | D Loss: 0.7966 | G Loss: 11.6225 (GAN: 0.8677, L1: 3.4133, Perceptual: 7.3415)\n",
      "Epoch [29/50], Batch [40/250] | D Loss: 0.6132 | G Loss: 11.7044 (GAN: 0.8587, L1: 3.3370, Perceptual: 7.5087)\n",
      "Epoch [29/50], Batch [40/250] | D Loss: 0.6132 | G Loss: 11.7044 (GAN: 0.8587, L1: 3.3370, Perceptual: 7.5087)\n",
      "Epoch [29/50], Batch [50/250] | D Loss: 0.6867 | G Loss: 10.6802 (GAN: 0.9384, L1: 3.2268, Perceptual: 6.5150)\n",
      "Epoch [29/50], Batch [50/250] | D Loss: 0.6867 | G Loss: 10.6802 (GAN: 0.9384, L1: 3.2268, Perceptual: 6.5150)\n",
      "Epoch [29/50], Batch [60/250] | D Loss: 0.5309 | G Loss: 11.5053 (GAN: 0.9106, L1: 3.1576, Perceptual: 7.4370)\n",
      "Epoch [29/50], Batch [60/250] | D Loss: 0.5309 | G Loss: 11.5053 (GAN: 0.9106, L1: 3.1576, Perceptual: 7.4370)\n",
      "Epoch [29/50], Batch [70/250] | D Loss: 0.6834 | G Loss: 11.0469 (GAN: 0.5504, L1: 3.2698, Perceptual: 7.2267)\n",
      "Epoch [29/50], Batch [70/250] | D Loss: 0.6834 | G Loss: 11.0469 (GAN: 0.5504, L1: 3.2698, Perceptual: 7.2267)\n",
      "Epoch [29/50], Batch [80/250] | D Loss: 0.8510 | G Loss: 10.8558 (GAN: 0.9323, L1: 3.0782, Perceptual: 6.8453)\n",
      "Epoch [29/50], Batch [80/250] | D Loss: 0.8510 | G Loss: 10.8558 (GAN: 0.9323, L1: 3.0782, Perceptual: 6.8453)\n",
      "Epoch [29/50], Batch [90/250] | D Loss: 0.5484 | G Loss: 11.8594 (GAN: 0.7699, L1: 3.6403, Perceptual: 7.4492)\n",
      "Epoch [29/50], Batch [90/250] | D Loss: 0.5484 | G Loss: 11.8594 (GAN: 0.7699, L1: 3.6403, Perceptual: 7.4492)\n",
      "Epoch [29/50], Batch [100/250] | D Loss: 0.6718 | G Loss: 11.3712 (GAN: 0.9375, L1: 3.4195, Perceptual: 7.0142)\n",
      "Epoch [29/50], Batch [100/250] | D Loss: 0.6718 | G Loss: 11.3712 (GAN: 0.9375, L1: 3.4195, Perceptual: 7.0142)\n",
      "Epoch [29/50], Batch [110/250] | D Loss: 0.6880 | G Loss: 13.0165 (GAN: 1.1513, L1: 4.0737, Perceptual: 7.7915)\n",
      "Epoch [29/50], Batch [110/250] | D Loss: 0.6880 | G Loss: 13.0165 (GAN: 1.1513, L1: 4.0737, Perceptual: 7.7915)\n",
      "Epoch [29/50], Batch [120/250] | D Loss: 0.6795 | G Loss: 11.3308 (GAN: 1.0042, L1: 3.1592, Perceptual: 7.1673)\n",
      "Epoch [29/50], Batch [120/250] | D Loss: 0.6795 | G Loss: 11.3308 (GAN: 1.0042, L1: 3.1592, Perceptual: 7.1673)\n",
      "Epoch [29/50], Batch [130/250] | D Loss: 0.4979 | G Loss: 12.5557 (GAN: 0.8523, L1: 3.8866, Perceptual: 7.8168)\n",
      "Epoch [29/50], Batch [130/250] | D Loss: 0.4979 | G Loss: 12.5557 (GAN: 0.8523, L1: 3.8866, Perceptual: 7.8168)\n",
      "Epoch [29/50], Batch [140/250] | D Loss: 0.6540 | G Loss: 11.3127 (GAN: 0.8911, L1: 3.1240, Perceptual: 7.2976)\n",
      "Epoch [29/50], Batch [140/250] | D Loss: 0.6540 | G Loss: 11.3127 (GAN: 0.8911, L1: 3.1240, Perceptual: 7.2976)\n",
      "Epoch [29/50], Batch [150/250] | D Loss: 0.5411 | G Loss: 11.3282 (GAN: 1.0033, L1: 3.2021, Perceptual: 7.1229)\n",
      "Epoch [29/50], Batch [150/250] | D Loss: 0.5411 | G Loss: 11.3282 (GAN: 1.0033, L1: 3.2021, Perceptual: 7.1229)\n",
      "Epoch [29/50], Batch [160/250] | D Loss: 0.6616 | G Loss: 10.7478 (GAN: 0.5999, L1: 3.2630, Perceptual: 6.8849)\n",
      "Epoch [29/50], Batch [160/250] | D Loss: 0.6616 | G Loss: 10.7478 (GAN: 0.5999, L1: 3.2630, Perceptual: 6.8849)\n",
      "Epoch [29/50], Batch [170/250] | D Loss: 0.6412 | G Loss: 10.7081 (GAN: 0.7610, L1: 3.1184, Perceptual: 6.8288)\n",
      "Epoch [29/50], Batch [170/250] | D Loss: 0.6412 | G Loss: 10.7081 (GAN: 0.7610, L1: 3.1184, Perceptual: 6.8288)\n",
      "Epoch [29/50], Batch [180/250] | D Loss: 0.6090 | G Loss: 10.0989 (GAN: 0.5764, L1: 3.0095, Perceptual: 6.5130)\n",
      "Epoch [29/50], Batch [180/250] | D Loss: 0.6090 | G Loss: 10.0989 (GAN: 0.5764, L1: 3.0095, Perceptual: 6.5130)\n",
      "Epoch [29/50], Batch [190/250] | D Loss: 0.8287 | G Loss: 10.1978 (GAN: 0.6805, L1: 2.9263, Perceptual: 6.5910)\n",
      "Epoch [29/50], Batch [190/250] | D Loss: 0.8287 | G Loss: 10.1978 (GAN: 0.6805, L1: 2.9263, Perceptual: 6.5910)\n",
      "Epoch [29/50], Batch [200/250] | D Loss: 0.5997 | G Loss: 11.7993 (GAN: 0.8914, L1: 3.5823, Perceptual: 7.3255)\n",
      "Epoch [29/50], Batch [200/250] | D Loss: 0.5997 | G Loss: 11.7993 (GAN: 0.8914, L1: 3.5823, Perceptual: 7.3255)\n",
      "Epoch [29/50], Batch [210/250] | D Loss: 0.6584 | G Loss: 11.2718 (GAN: 0.8684, L1: 3.3976, Perceptual: 7.0057)\n",
      "Epoch [29/50], Batch [210/250] | D Loss: 0.6584 | G Loss: 11.2718 (GAN: 0.8684, L1: 3.3976, Perceptual: 7.0057)\n",
      "Epoch [29/50], Batch [220/250] | D Loss: 0.5200 | G Loss: 12.2535 (GAN: 1.1276, L1: 3.6316, Perceptual: 7.4943)\n",
      "Epoch [29/50], Batch [220/250] | D Loss: 0.5200 | G Loss: 12.2535 (GAN: 1.1276, L1: 3.6316, Perceptual: 7.4943)\n",
      "Epoch [29/50], Batch [230/250] | D Loss: 0.7469 | G Loss: 10.8602 (GAN: 0.9147, L1: 3.1696, Perceptual: 6.7759)\n",
      "Epoch [29/50], Batch [230/250] | D Loss: 0.7469 | G Loss: 10.8602 (GAN: 0.9147, L1: 3.1696, Perceptual: 6.7759)\n",
      "Epoch [29/50], Batch [240/250] | D Loss: 0.5400 | G Loss: 12.0604 (GAN: 0.8243, L1: 3.6806, Perceptual: 7.5555)\n",
      "Epoch [29/50], Batch [240/250] | D Loss: 0.5400 | G Loss: 12.0604 (GAN: 0.8243, L1: 3.6806, Perceptual: 7.5555)\n",
      "Epoch [29/50], Batch [250/250] | D Loss: 0.6115 | G Loss: 11.9052 (GAN: 0.7461, L1: 3.6346, Perceptual: 7.5246)\n",
      "Epoch 29 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [29/50], Batch [250/250] | D Loss: 0.6115 | G Loss: 11.9052 (GAN: 0.7461, L1: 3.6346, Perceptual: 7.5246)\n",
      "Epoch 29 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "Epoch [30/50], Batch [10/250] | D Loss: 0.6407 | G Loss: 12.1413 (GAN: 0.7145, L1: 3.7493, Perceptual: 7.6775)\n",
      "Epoch [30/50], Batch [10/250] | D Loss: 0.6407 | G Loss: 12.1413 (GAN: 0.7145, L1: 3.7493, Perceptual: 7.6775)\n",
      "Epoch [30/50], Batch [20/250] | D Loss: 0.5365 | G Loss: 12.1548 (GAN: 0.7719, L1: 3.6689, Perceptual: 7.7140)\n",
      "Epoch [30/50], Batch [20/250] | D Loss: 0.5365 | G Loss: 12.1548 (GAN: 0.7719, L1: 3.6689, Perceptual: 7.7140)\n",
      "Epoch [30/50], Batch [30/250] | D Loss: 0.8065 | G Loss: 11.7634 (GAN: 1.1814, L1: 3.4267, Perceptual: 7.1553)\n",
      "Epoch [30/50], Batch [30/250] | D Loss: 0.8065 | G Loss: 11.7634 (GAN: 1.1814, L1: 3.4267, Perceptual: 7.1553)\n",
      "Epoch [30/50], Batch [40/250] | D Loss: 0.6118 | G Loss: 10.6920 (GAN: 0.6341, L1: 3.2709, Perceptual: 6.7870)\n",
      "Epoch [30/50], Batch [40/250] | D Loss: 0.6118 | G Loss: 10.6920 (GAN: 0.6341, L1: 3.2709, Perceptual: 6.7870)\n",
      "Epoch [30/50], Batch [50/250] | D Loss: 0.7041 | G Loss: 12.2985 (GAN: 1.0741, L1: 3.7586, Perceptual: 7.4658)\n",
      "Epoch [30/50], Batch [50/250] | D Loss: 0.7041 | G Loss: 12.2985 (GAN: 1.0741, L1: 3.7586, Perceptual: 7.4658)\n",
      "Epoch [30/50], Batch [60/250] | D Loss: 0.6331 | G Loss: 12.1972 (GAN: 1.0891, L1: 3.5902, Perceptual: 7.5180)\n",
      "Epoch [30/50], Batch [60/250] | D Loss: 0.6331 | G Loss: 12.1972 (GAN: 1.0891, L1: 3.5902, Perceptual: 7.5180)\n",
      "Epoch [30/50], Batch [70/250] | D Loss: 0.6960 | G Loss: 10.2725 (GAN: 0.7507, L1: 3.0091, Perceptual: 6.5127)\n",
      "Epoch [30/50], Batch [70/250] | D Loss: 0.6960 | G Loss: 10.2725 (GAN: 0.7507, L1: 3.0091, Perceptual: 6.5127)\n",
      "Epoch [30/50], Batch [80/250] | D Loss: 0.5392 | G Loss: 11.9868 (GAN: 0.6938, L1: 3.6303, Perceptual: 7.6627)\n",
      "Epoch [30/50], Batch [80/250] | D Loss: 0.5392 | G Loss: 11.9868 (GAN: 0.6938, L1: 3.6303, Perceptual: 7.6627)\n",
      "Epoch [30/50], Batch [90/250] | D Loss: 0.6077 | G Loss: 12.7438 (GAN: 0.8724, L1: 3.9085, Perceptual: 7.9629)\n",
      "Epoch [30/50], Batch [90/250] | D Loss: 0.6077 | G Loss: 12.7438 (GAN: 0.8724, L1: 3.9085, Perceptual: 7.9629)\n",
      "Epoch [30/50], Batch [100/250] | D Loss: 0.5748 | G Loss: 11.5950 (GAN: 0.9221, L1: 3.3046, Perceptual: 7.3682)\n",
      "Epoch [30/50], Batch [100/250] | D Loss: 0.5748 | G Loss: 11.5950 (GAN: 0.9221, L1: 3.3046, Perceptual: 7.3682)\n",
      "Epoch [30/50], Batch [110/250] | D Loss: 0.6950 | G Loss: 11.8519 (GAN: 0.7457, L1: 3.5852, Perceptual: 7.5209)\n",
      "Epoch [30/50], Batch [110/250] | D Loss: 0.6950 | G Loss: 11.8519 (GAN: 0.7457, L1: 3.5852, Perceptual: 7.5209)\n",
      "Epoch [30/50], Batch [120/250] | D Loss: 0.4708 | G Loss: 10.8551 (GAN: 0.5579, L1: 3.3018, Perceptual: 6.9954)\n",
      "Epoch [30/50], Batch [120/250] | D Loss: 0.4708 | G Loss: 10.8551 (GAN: 0.5579, L1: 3.3018, Perceptual: 6.9954)\n",
      "Epoch [30/50], Batch [130/250] | D Loss: 0.7336 | G Loss: 12.0966 (GAN: 0.8238, L1: 3.5766, Perceptual: 7.6962)\n",
      "Epoch [30/50], Batch [130/250] | D Loss: 0.7336 | G Loss: 12.0966 (GAN: 0.8238, L1: 3.5766, Perceptual: 7.6962)\n",
      "Epoch [30/50], Batch [140/250] | D Loss: 0.6637 | G Loss: 11.9778 (GAN: 1.0505, L1: 3.3505, Perceptual: 7.5768)\n",
      "Epoch [30/50], Batch [140/250] | D Loss: 0.6637 | G Loss: 11.9778 (GAN: 1.0505, L1: 3.3505, Perceptual: 7.5768)\n",
      "Epoch [30/50], Batch [150/250] | D Loss: 0.5767 | G Loss: 11.4030 (GAN: 0.6756, L1: 3.6365, Perceptual: 7.0908)\n",
      "Epoch [30/50], Batch [150/250] | D Loss: 0.5767 | G Loss: 11.4030 (GAN: 0.6756, L1: 3.6365, Perceptual: 7.0908)\n",
      "Epoch [30/50], Batch [160/250] | D Loss: 0.7907 | G Loss: 11.5061 (GAN: 0.9919, L1: 3.3514, Perceptual: 7.1627)\n",
      "Epoch [30/50], Batch [160/250] | D Loss: 0.7907 | G Loss: 11.5061 (GAN: 0.9919, L1: 3.3514, Perceptual: 7.1627)\n",
      "Epoch [30/50], Batch [170/250] | D Loss: 0.5863 | G Loss: 11.3407 (GAN: 0.6871, L1: 3.7250, Perceptual: 6.9286)\n",
      "Epoch [30/50], Batch [170/250] | D Loss: 0.5863 | G Loss: 11.3407 (GAN: 0.6871, L1: 3.7250, Perceptual: 6.9286)\n",
      "Epoch [30/50], Batch [180/250] | D Loss: 0.8307 | G Loss: 11.2630 (GAN: 0.6807, L1: 3.6559, Perceptual: 6.9264)\n",
      "Epoch [30/50], Batch [180/250] | D Loss: 0.8307 | G Loss: 11.2630 (GAN: 0.6807, L1: 3.6559, Perceptual: 6.9264)\n",
      "Epoch [30/50], Batch [190/250] | D Loss: 0.7432 | G Loss: 12.2553 (GAN: 0.9385, L1: 3.7786, Perceptual: 7.5382)\n",
      "Epoch [30/50], Batch [190/250] | D Loss: 0.7432 | G Loss: 12.2553 (GAN: 0.9385, L1: 3.7786, Perceptual: 7.5382)\n",
      "Epoch [30/50], Batch [200/250] | D Loss: 0.8634 | G Loss: 10.9996 (GAN: 1.0952, L1: 3.0600, Perceptual: 6.8444)\n",
      "Epoch [30/50], Batch [200/250] | D Loss: 0.8634 | G Loss: 10.9996 (GAN: 1.0952, L1: 3.0600, Perceptual: 6.8444)\n",
      "Epoch [30/50], Batch [210/250] | D Loss: 0.6879 | G Loss: 10.5911 (GAN: 0.7773, L1: 3.2206, Perceptual: 6.5932)\n",
      "Epoch [30/50], Batch [210/250] | D Loss: 0.6879 | G Loss: 10.5911 (GAN: 0.7773, L1: 3.2206, Perceptual: 6.5932)\n",
      "Epoch [30/50], Batch [220/250] | D Loss: 0.4995 | G Loss: 11.9057 (GAN: 0.6316, L1: 3.7583, Perceptual: 7.5159)\n",
      "Epoch [30/50], Batch [220/250] | D Loss: 0.4995 | G Loss: 11.9057 (GAN: 0.6316, L1: 3.7583, Perceptual: 7.5159)\n",
      "Epoch [30/50], Batch [230/250] | D Loss: 0.5493 | G Loss: 12.2462 (GAN: 0.8226, L1: 3.4741, Perceptual: 7.9496)\n",
      "Epoch [30/50], Batch [230/250] | D Loss: 0.5493 | G Loss: 12.2462 (GAN: 0.8226, L1: 3.4741, Perceptual: 7.9496)\n",
      "Epoch [30/50], Batch [240/250] | D Loss: 0.7615 | G Loss: 12.5598 (GAN: 1.0855, L1: 3.7304, Perceptual: 7.7440)\n",
      "Epoch [30/50], Batch [240/250] | D Loss: 0.7615 | G Loss: 12.5598 (GAN: 1.0855, L1: 3.7304, Perceptual: 7.7440)\n",
      "Epoch [30/50], Batch [250/250] | D Loss: 0.6117 | G Loss: 11.1729 (GAN: 0.7411, L1: 3.3194, Perceptual: 7.1124)\n",
      "Epoch 30 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 30 ---\n",
      "Saving samples and checkpoint for epoch 30...\n",
      "Epoch [30/50], Batch [250/250] | D Loss: 0.6117 | G Loss: 11.1729 (GAN: 0.7411, L1: 3.3194, Perceptual: 7.1124)\n",
      "Epoch 30 completed. Current LR G: 0.000200, LR D: 0.000100\n",
      "--- Entering save/checkpoint block for epoch 30 ---\n",
      "Saving samples and checkpoint for epoch 30...\n",
      "Successfully saved image to: output/generated_images\\epoch_30_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_30_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [31/50], Batch [10/250] | D Loss: 0.5948 | G Loss: 10.8034 (GAN: 0.8369, L1: 3.2225, Perceptual: 6.7440)\n",
      "Epoch [31/50], Batch [10/250] | D Loss: 0.5948 | G Loss: 10.8034 (GAN: 0.8369, L1: 3.2225, Perceptual: 6.7440)\n",
      "Epoch [31/50], Batch [20/250] | D Loss: 0.5257 | G Loss: 11.9637 (GAN: 0.6194, L1: 3.8483, Perceptual: 7.4960)\n",
      "Epoch [31/50], Batch [20/250] | D Loss: 0.5257 | G Loss: 11.9637 (GAN: 0.6194, L1: 3.8483, Perceptual: 7.4960)\n",
      "Epoch [31/50], Batch [30/250] | D Loss: 0.4846 | G Loss: 11.9319 (GAN: 0.8073, L1: 3.5193, Perceptual: 7.6054)\n",
      "Epoch [31/50], Batch [30/250] | D Loss: 0.4846 | G Loss: 11.9319 (GAN: 0.8073, L1: 3.5193, Perceptual: 7.6054)\n",
      "Epoch [31/50], Batch [40/250] | D Loss: 0.6831 | G Loss: 12.1381 (GAN: 0.8592, L1: 3.6226, Perceptual: 7.6562)\n",
      "Epoch [31/50], Batch [40/250] | D Loss: 0.6831 | G Loss: 12.1381 (GAN: 0.8592, L1: 3.6226, Perceptual: 7.6562)\n",
      "Epoch [31/50], Batch [50/250] | D Loss: 0.7050 | G Loss: 11.2934 (GAN: 0.7435, L1: 3.3117, Perceptual: 7.2381)\n",
      "Epoch [31/50], Batch [50/250] | D Loss: 0.7050 | G Loss: 11.2934 (GAN: 0.7435, L1: 3.3117, Perceptual: 7.2381)\n",
      "Epoch [31/50], Batch [60/250] | D Loss: 0.6147 | G Loss: 10.6658 (GAN: 0.6971, L1: 3.0208, Perceptual: 6.9479)\n",
      "Epoch [31/50], Batch [60/250] | D Loss: 0.6147 | G Loss: 10.6658 (GAN: 0.6971, L1: 3.0208, Perceptual: 6.9479)\n",
      "Epoch [31/50], Batch [70/250] | D Loss: 0.5563 | G Loss: 11.4656 (GAN: 0.7902, L1: 3.6320, Perceptual: 7.0434)\n",
      "Epoch [31/50], Batch [70/250] | D Loss: 0.5563 | G Loss: 11.4656 (GAN: 0.7902, L1: 3.6320, Perceptual: 7.0434)\n",
      "Epoch [31/50], Batch [80/250] | D Loss: 0.6506 | G Loss: 12.2794 (GAN: 1.2675, L1: 3.4391, Perceptual: 7.5728)\n",
      "Epoch [31/50], Batch [80/250] | D Loss: 0.6506 | G Loss: 12.2794 (GAN: 1.2675, L1: 3.4391, Perceptual: 7.5728)\n",
      "Epoch [31/50], Batch [90/250] | D Loss: 0.6061 | G Loss: 11.3979 (GAN: 0.5578, L1: 3.5137, Perceptual: 7.3264)\n",
      "Epoch [31/50], Batch [90/250] | D Loss: 0.6061 | G Loss: 11.3979 (GAN: 0.5578, L1: 3.5137, Perceptual: 7.3264)\n",
      "Epoch [31/50], Batch [100/250] | D Loss: 0.6206 | G Loss: 12.0722 (GAN: 1.0247, L1: 3.4975, Perceptual: 7.5501)\n",
      "Epoch [31/50], Batch [100/250] | D Loss: 0.6206 | G Loss: 12.0722 (GAN: 1.0247, L1: 3.4975, Perceptual: 7.5501)\n",
      "Epoch [31/50], Batch [110/250] | D Loss: 0.6690 | G Loss: 11.0571 (GAN: 0.6950, L1: 3.2646, Perceptual: 7.0975)\n",
      "Epoch [31/50], Batch [110/250] | D Loss: 0.6690 | G Loss: 11.0571 (GAN: 0.6950, L1: 3.2646, Perceptual: 7.0975)\n",
      "Epoch [31/50], Batch [120/250] | D Loss: 0.5154 | G Loss: 11.8162 (GAN: 0.7487, L1: 3.5637, Perceptual: 7.5037)\n",
      "Epoch [31/50], Batch [120/250] | D Loss: 0.5154 | G Loss: 11.8162 (GAN: 0.7487, L1: 3.5637, Perceptual: 7.5037)\n",
      "Epoch [31/50], Batch [130/250] | D Loss: 0.6601 | G Loss: 11.9348 (GAN: 0.7826, L1: 3.8429, Perceptual: 7.3094)\n",
      "Epoch [31/50], Batch [130/250] | D Loss: 0.6601 | G Loss: 11.9348 (GAN: 0.7826, L1: 3.8429, Perceptual: 7.3094)\n",
      "Epoch [31/50], Batch [140/250] | D Loss: 0.4517 | G Loss: 11.9287 (GAN: 0.8275, L1: 3.2736, Perceptual: 7.8275)\n",
      "Epoch [31/50], Batch [140/250] | D Loss: 0.4517 | G Loss: 11.9287 (GAN: 0.8275, L1: 3.2736, Perceptual: 7.8275)\n",
      "Epoch [31/50], Batch [150/250] | D Loss: 0.5423 | G Loss: 11.3930 (GAN: 0.8995, L1: 3.2822, Perceptual: 7.2112)\n",
      "Epoch [31/50], Batch [150/250] | D Loss: 0.5423 | G Loss: 11.3930 (GAN: 0.8995, L1: 3.2822, Perceptual: 7.2112)\n",
      "Epoch [31/50], Batch [160/250] | D Loss: 0.8547 | G Loss: 10.2821 (GAN: 0.8144, L1: 3.0055, Perceptual: 6.4623)\n",
      "Epoch [31/50], Batch [160/250] | D Loss: 0.8547 | G Loss: 10.2821 (GAN: 0.8144, L1: 3.0055, Perceptual: 6.4623)\n",
      "Epoch [31/50], Batch [170/250] | D Loss: 0.4886 | G Loss: 11.5628 (GAN: 0.9581, L1: 3.3080, Perceptual: 7.2967)\n",
      "Epoch [31/50], Batch [170/250] | D Loss: 0.4886 | G Loss: 11.5628 (GAN: 0.9581, L1: 3.3080, Perceptual: 7.2967)\n",
      "Epoch [31/50], Batch [180/250] | D Loss: 0.6190 | G Loss: 11.8652 (GAN: 1.2157, L1: 3.5120, Perceptual: 7.1375)\n",
      "Epoch [31/50], Batch [180/250] | D Loss: 0.6190 | G Loss: 11.8652 (GAN: 1.2157, L1: 3.5120, Perceptual: 7.1375)\n",
      "Epoch [31/50], Batch [190/250] | D Loss: 0.7570 | G Loss: 10.1495 (GAN: 0.8583, L1: 2.7747, Perceptual: 6.5165)\n",
      "Epoch [31/50], Batch [190/250] | D Loss: 0.7570 | G Loss: 10.1495 (GAN: 0.8583, L1: 2.7747, Perceptual: 6.5165)\n",
      "Epoch [31/50], Batch [200/250] | D Loss: 0.6378 | G Loss: 12.5676 (GAN: 1.1352, L1: 3.5668, Perceptual: 7.8655)\n",
      "Epoch [31/50], Batch [200/250] | D Loss: 0.6378 | G Loss: 12.5676 (GAN: 1.1352, L1: 3.5668, Perceptual: 7.8655)\n",
      "Epoch [31/50], Batch [210/250] | D Loss: 0.5080 | G Loss: 11.1407 (GAN: 0.8005, L1: 3.2734, Perceptual: 7.0668)\n",
      "Epoch [31/50], Batch [210/250] | D Loss: 0.5080 | G Loss: 11.1407 (GAN: 0.8005, L1: 3.2734, Perceptual: 7.0668)\n",
      "Epoch [31/50], Batch [220/250] | D Loss: 0.8543 | G Loss: 10.5764 (GAN: 0.7362, L1: 3.1185, Perceptual: 6.7218)\n",
      "Epoch [31/50], Batch [220/250] | D Loss: 0.8543 | G Loss: 10.5764 (GAN: 0.7362, L1: 3.1185, Perceptual: 6.7218)\n",
      "Epoch [31/50], Batch [230/250] | D Loss: 0.8954 | G Loss: 10.7923 (GAN: 0.9007, L1: 3.3416, Perceptual: 6.5500)\n",
      "Epoch [31/50], Batch [230/250] | D Loss: 0.8954 | G Loss: 10.7923 (GAN: 0.9007, L1: 3.3416, Perceptual: 6.5500)\n",
      "Epoch [31/50], Batch [240/250] | D Loss: 0.5343 | G Loss: 11.9035 (GAN: 0.7535, L1: 3.5300, Perceptual: 7.6200)\n",
      "Epoch [31/50], Batch [240/250] | D Loss: 0.5343 | G Loss: 11.9035 (GAN: 0.7535, L1: 3.5300, Perceptual: 7.6200)\n",
      "Epoch [31/50], Batch [250/250] | D Loss: 0.5259 | G Loss: 11.4236 (GAN: 0.6004, L1: 3.5467, Perceptual: 7.2765)\n",
      "Epoch 31 completed. Current LR G: 0.000190, LR D: 0.000095\n",
      "Epoch [31/50], Batch [250/250] | D Loss: 0.5259 | G Loss: 11.4236 (GAN: 0.6004, L1: 3.5467, Perceptual: 7.2765)\n",
      "Epoch 31 completed. Current LR G: 0.000190, LR D: 0.000095\n",
      "Epoch [32/50], Batch [10/250] | D Loss: 0.7193 | G Loss: 10.7767 (GAN: 0.6668, L1: 3.1955, Perceptual: 6.9144)\n",
      "Epoch [32/50], Batch [10/250] | D Loss: 0.7193 | G Loss: 10.7767 (GAN: 0.6668, L1: 3.1955, Perceptual: 6.9144)\n",
      "Epoch [32/50], Batch [20/250] | D Loss: 0.6781 | G Loss: 11.9813 (GAN: 0.5516, L1: 3.8887, Perceptual: 7.5409)\n",
      "Epoch [32/50], Batch [20/250] | D Loss: 0.6781 | G Loss: 11.9813 (GAN: 0.5516, L1: 3.8887, Perceptual: 7.5409)\n",
      "Epoch [32/50], Batch [30/250] | D Loss: 0.5313 | G Loss: 10.1444 (GAN: 0.6212, L1: 2.7077, Perceptual: 6.8154)\n",
      "Epoch [32/50], Batch [30/250] | D Loss: 0.5313 | G Loss: 10.1444 (GAN: 0.6212, L1: 2.7077, Perceptual: 6.8154)\n",
      "Epoch [32/50], Batch [40/250] | D Loss: 0.5164 | G Loss: 10.7577 (GAN: 0.7746, L1: 3.0007, Perceptual: 6.9824)\n",
      "Epoch [32/50], Batch [40/250] | D Loss: 0.5164 | G Loss: 10.7577 (GAN: 0.7746, L1: 3.0007, Perceptual: 6.9824)\n",
      "Epoch [32/50], Batch [50/250] | D Loss: 0.4661 | G Loss: 11.6717 (GAN: 0.9659, L1: 3.3175, Perceptual: 7.3883)\n",
      "Epoch [32/50], Batch [50/250] | D Loss: 0.4661 | G Loss: 11.6717 (GAN: 0.9659, L1: 3.3175, Perceptual: 7.3883)\n",
      "Epoch [32/50], Batch [60/250] | D Loss: 0.5883 | G Loss: 11.9284 (GAN: 0.9425, L1: 3.8723, Perceptual: 7.1137)\n",
      "Epoch [32/50], Batch [60/250] | D Loss: 0.5883 | G Loss: 11.9284 (GAN: 0.9425, L1: 3.8723, Perceptual: 7.1137)\n",
      "Epoch [32/50], Batch [70/250] | D Loss: 0.6517 | G Loss: 10.8314 (GAN: 0.6845, L1: 3.2805, Perceptual: 6.8664)\n",
      "Epoch [32/50], Batch [70/250] | D Loss: 0.6517 | G Loss: 10.8314 (GAN: 0.6845, L1: 3.2805, Perceptual: 6.8664)\n",
      "Epoch [32/50], Batch [80/250] | D Loss: 0.8279 | G Loss: 10.8100 (GAN: 0.8952, L1: 3.1493, Perceptual: 6.7655)\n",
      "Epoch [32/50], Batch [80/250] | D Loss: 0.8279 | G Loss: 10.8100 (GAN: 0.8952, L1: 3.1493, Perceptual: 6.7655)\n",
      "Epoch [32/50], Batch [90/250] | D Loss: 0.5873 | G Loss: 12.4936 (GAN: 0.8294, L1: 3.9992, Perceptual: 7.6650)\n",
      "Epoch [32/50], Batch [90/250] | D Loss: 0.5873 | G Loss: 12.4936 (GAN: 0.8294, L1: 3.9992, Perceptual: 7.6650)\n",
      "Epoch [32/50], Batch [100/250] | D Loss: 0.6205 | G Loss: 12.1687 (GAN: 1.0370, L1: 3.4046, Perceptual: 7.7271)\n",
      "Epoch [32/50], Batch [100/250] | D Loss: 0.6205 | G Loss: 12.1687 (GAN: 1.0370, L1: 3.4046, Perceptual: 7.7271)\n",
      "Epoch [32/50], Batch [110/250] | D Loss: 0.8763 | G Loss: 10.8339 (GAN: 0.9547, L1: 3.0618, Perceptual: 6.8174)\n",
      "Epoch [32/50], Batch [110/250] | D Loss: 0.8763 | G Loss: 10.8339 (GAN: 0.9547, L1: 3.0618, Perceptual: 6.8174)\n",
      "Epoch [32/50], Batch [120/250] | D Loss: 0.9230 | G Loss: 11.2325 (GAN: 1.1522, L1: 3.4544, Perceptual: 6.6258)\n",
      "Epoch [32/50], Batch [120/250] | D Loss: 0.9230 | G Loss: 11.2325 (GAN: 1.1522, L1: 3.4544, Perceptual: 6.6258)\n",
      "Epoch [32/50], Batch [130/250] | D Loss: 0.6596 | G Loss: 12.2304 (GAN: 0.9043, L1: 3.3800, Perceptual: 7.9462)\n",
      "Epoch [32/50], Batch [130/250] | D Loss: 0.6596 | G Loss: 12.2304 (GAN: 0.9043, L1: 3.3800, Perceptual: 7.9462)\n",
      "Epoch [32/50], Batch [140/250] | D Loss: 0.8165 | G Loss: 11.3153 (GAN: 0.6474, L1: 3.3281, Perceptual: 7.3398)\n",
      "Epoch [32/50], Batch [140/250] | D Loss: 0.8165 | G Loss: 11.3153 (GAN: 0.6474, L1: 3.3281, Perceptual: 7.3398)\n",
      "Epoch [32/50], Batch [150/250] | D Loss: 0.6474 | G Loss: 11.2805 (GAN: 0.9033, L1: 3.1154, Perceptual: 7.2619)\n",
      "Epoch [32/50], Batch [150/250] | D Loss: 0.6474 | G Loss: 11.2805 (GAN: 0.9033, L1: 3.1154, Perceptual: 7.2619)\n",
      "Epoch [32/50], Batch [160/250] | D Loss: 0.5434 | G Loss: 11.0924 (GAN: 0.8159, L1: 3.1799, Perceptual: 7.0967)\n",
      "Epoch [32/50], Batch [160/250] | D Loss: 0.5434 | G Loss: 11.0924 (GAN: 0.8159, L1: 3.1799, Perceptual: 7.0967)\n",
      "Epoch [32/50], Batch [170/250] | D Loss: 0.8487 | G Loss: 12.9209 (GAN: 2.0848, L1: 3.6238, Perceptual: 7.2123)\n",
      "Epoch [32/50], Batch [170/250] | D Loss: 0.8487 | G Loss: 12.9209 (GAN: 2.0848, L1: 3.6238, Perceptual: 7.2123)\n",
      "Epoch [32/50], Batch [180/250] | D Loss: 0.6368 | G Loss: 11.6774 (GAN: 0.7451, L1: 3.3135, Perceptual: 7.6188)\n",
      "Epoch [32/50], Batch [180/250] | D Loss: 0.6368 | G Loss: 11.6774 (GAN: 0.7451, L1: 3.3135, Perceptual: 7.6188)\n",
      "Epoch [32/50], Batch [190/250] | D Loss: 0.6033 | G Loss: 11.1590 (GAN: 0.8049, L1: 3.5738, Perceptual: 6.7803)\n",
      "Epoch [32/50], Batch [190/250] | D Loss: 0.6033 | G Loss: 11.1590 (GAN: 0.8049, L1: 3.5738, Perceptual: 6.7803)\n",
      "Epoch [32/50], Batch [200/250] | D Loss: 0.6240 | G Loss: 11.4017 (GAN: 1.0570, L1: 3.1617, Perceptual: 7.1829)\n",
      "Epoch [32/50], Batch [200/250] | D Loss: 0.6240 | G Loss: 11.4017 (GAN: 1.0570, L1: 3.1617, Perceptual: 7.1829)\n",
      "Epoch [32/50], Batch [210/250] | D Loss: 0.6033 | G Loss: 11.3442 (GAN: 0.7651, L1: 3.1213, Perceptual: 7.4578)\n",
      "Epoch [32/50], Batch [210/250] | D Loss: 0.6033 | G Loss: 11.3442 (GAN: 0.7651, L1: 3.1213, Perceptual: 7.4578)\n",
      "Epoch [32/50], Batch [220/250] | D Loss: 0.6224 | G Loss: 12.4499 (GAN: 0.9025, L1: 3.7032, Perceptual: 7.8441)\n",
      "Epoch [32/50], Batch [220/250] | D Loss: 0.6224 | G Loss: 12.4499 (GAN: 0.9025, L1: 3.7032, Perceptual: 7.8441)\n",
      "Epoch [32/50], Batch [230/250] | D Loss: 0.6113 | G Loss: 13.1025 (GAN: 0.8066, L1: 4.1553, Perceptual: 8.1406)\n",
      "Epoch [32/50], Batch [230/250] | D Loss: 0.6113 | G Loss: 13.1025 (GAN: 0.8066, L1: 4.1553, Perceptual: 8.1406)\n",
      "Epoch [32/50], Batch [240/250] | D Loss: 0.6394 | G Loss: 11.3299 (GAN: 0.6643, L1: 3.3482, Perceptual: 7.3175)\n",
      "Epoch [32/50], Batch [240/250] | D Loss: 0.6394 | G Loss: 11.3299 (GAN: 0.6643, L1: 3.3482, Perceptual: 7.3175)\n",
      "Epoch [32/50], Batch [250/250] | D Loss: 0.5652 | G Loss: 10.9176 (GAN: 0.6504, L1: 3.5017, Perceptual: 6.7655)\n",
      "Epoch 32 completed. Current LR G: 0.000180, LR D: 0.000090\n",
      "Epoch [32/50], Batch [250/250] | D Loss: 0.5652 | G Loss: 10.9176 (GAN: 0.6504, L1: 3.5017, Perceptual: 6.7655)\n",
      "Epoch 32 completed. Current LR G: 0.000180, LR D: 0.000090\n",
      "Epoch [33/50], Batch [10/250] | D Loss: 0.5852 | G Loss: 12.4742 (GAN: 0.8685, L1: 3.8263, Perceptual: 7.7794)\n",
      "Epoch [33/50], Batch [10/250] | D Loss: 0.5852 | G Loss: 12.4742 (GAN: 0.8685, L1: 3.8263, Perceptual: 7.7794)\n",
      "Epoch [33/50], Batch [20/250] | D Loss: 0.6953 | G Loss: 10.9261 (GAN: 0.6738, L1: 3.1066, Perceptual: 7.1457)\n",
      "Epoch [33/50], Batch [20/250] | D Loss: 0.6953 | G Loss: 10.9261 (GAN: 0.6738, L1: 3.1066, Perceptual: 7.1457)\n",
      "Epoch [33/50], Batch [30/250] | D Loss: 0.6872 | G Loss: 9.9052 (GAN: 0.7246, L1: 2.7637, Perceptual: 6.4169)\n",
      "Epoch [33/50], Batch [30/250] | D Loss: 0.6872 | G Loss: 9.9052 (GAN: 0.7246, L1: 2.7637, Perceptual: 6.4169)\n",
      "Epoch [33/50], Batch [40/250] | D Loss: 0.5821 | G Loss: 11.1483 (GAN: 0.8990, L1: 3.2045, Perceptual: 7.0449)\n",
      "Epoch [33/50], Batch [40/250] | D Loss: 0.5821 | G Loss: 11.1483 (GAN: 0.8990, L1: 3.2045, Perceptual: 7.0449)\n",
      "Epoch [33/50], Batch [50/250] | D Loss: 0.5410 | G Loss: 12.3170 (GAN: 0.8624, L1: 3.7863, Perceptual: 7.6683)\n",
      "Epoch [33/50], Batch [50/250] | D Loss: 0.5410 | G Loss: 12.3170 (GAN: 0.8624, L1: 3.7863, Perceptual: 7.6683)\n",
      "Epoch [33/50], Batch [60/250] | D Loss: 0.5452 | G Loss: 11.3962 (GAN: 0.7238, L1: 3.4486, Perceptual: 7.2237)\n",
      "Epoch [33/50], Batch [60/250] | D Loss: 0.5452 | G Loss: 11.3962 (GAN: 0.7238, L1: 3.4486, Perceptual: 7.2237)\n",
      "Epoch [33/50], Batch [70/250] | D Loss: 0.5180 | G Loss: 12.3432 (GAN: 0.8590, L1: 3.6558, Perceptual: 7.8284)\n",
      "Epoch [33/50], Batch [70/250] | D Loss: 0.5180 | G Loss: 12.3432 (GAN: 0.8590, L1: 3.6558, Perceptual: 7.8284)\n",
      "Epoch [33/50], Batch [80/250] | D Loss: 0.6367 | G Loss: 11.3013 (GAN: 0.7161, L1: 3.5655, Perceptual: 7.0197)\n",
      "Epoch [33/50], Batch [80/250] | D Loss: 0.6367 | G Loss: 11.3013 (GAN: 0.7161, L1: 3.5655, Perceptual: 7.0197)\n",
      "Epoch [33/50], Batch [90/250] | D Loss: 0.6509 | G Loss: 11.5823 (GAN: 0.6600, L1: 3.5112, Perceptual: 7.4110)\n",
      "Epoch [33/50], Batch [90/250] | D Loss: 0.6509 | G Loss: 11.5823 (GAN: 0.6600, L1: 3.5112, Perceptual: 7.4110)\n",
      "Epoch [33/50], Batch [100/250] | D Loss: 0.6959 | G Loss: 11.0286 (GAN: 0.7556, L1: 3.2310, Perceptual: 7.0420)\n",
      "Epoch [33/50], Batch [100/250] | D Loss: 0.6959 | G Loss: 11.0286 (GAN: 0.7556, L1: 3.2310, Perceptual: 7.0420)\n",
      "Epoch [33/50], Batch [110/250] | D Loss: 0.5820 | G Loss: 11.7564 (GAN: 1.0707, L1: 3.3334, Perceptual: 7.3523)\n",
      "Epoch [33/50], Batch [110/250] | D Loss: 0.5820 | G Loss: 11.7564 (GAN: 1.0707, L1: 3.3334, Perceptual: 7.3523)\n",
      "Epoch [33/50], Batch [120/250] | D Loss: 0.6086 | G Loss: 11.8303 (GAN: 0.8886, L1: 3.6954, Perceptual: 7.2463)\n",
      "Epoch [33/50], Batch [120/250] | D Loss: 0.6086 | G Loss: 11.8303 (GAN: 0.8886, L1: 3.6954, Perceptual: 7.2463)\n",
      "Epoch [33/50], Batch [130/250] | D Loss: 0.6903 | G Loss: 11.3225 (GAN: 0.7692, L1: 3.3030, Perceptual: 7.2503)\n",
      "Epoch [33/50], Batch [130/250] | D Loss: 0.6903 | G Loss: 11.3225 (GAN: 0.7692, L1: 3.3030, Perceptual: 7.2503)\n",
      "Epoch [33/50], Batch [140/250] | D Loss: 0.4299 | G Loss: 11.4792 (GAN: 0.7638, L1: 3.4984, Perceptual: 7.2170)\n",
      "Epoch [33/50], Batch [140/250] | D Loss: 0.4299 | G Loss: 11.4792 (GAN: 0.7638, L1: 3.4984, Perceptual: 7.2170)\n",
      "Epoch [33/50], Batch [150/250] | D Loss: 0.5778 | G Loss: 12.4509 (GAN: 0.9256, L1: 3.7122, Perceptual: 7.8132)\n",
      "Epoch [33/50], Batch [150/250] | D Loss: 0.5778 | G Loss: 12.4509 (GAN: 0.9256, L1: 3.7122, Perceptual: 7.8132)\n",
      "Epoch [33/50], Batch [160/250] | D Loss: 0.5232 | G Loss: 12.6406 (GAN: 0.7863, L1: 3.8624, Perceptual: 7.9920)\n",
      "Epoch [33/50], Batch [160/250] | D Loss: 0.5232 | G Loss: 12.6406 (GAN: 0.7863, L1: 3.8624, Perceptual: 7.9920)\n",
      "Epoch [33/50], Batch [170/250] | D Loss: 0.7277 | G Loss: 11.9265 (GAN: 0.8322, L1: 3.5584, Perceptual: 7.5359)\n",
      "Epoch [33/50], Batch [170/250] | D Loss: 0.7277 | G Loss: 11.9265 (GAN: 0.8322, L1: 3.5584, Perceptual: 7.5359)\n",
      "Epoch [33/50], Batch [180/250] | D Loss: 0.5999 | G Loss: 11.1874 (GAN: 0.8484, L1: 3.1068, Perceptual: 7.2322)\n",
      "Epoch [33/50], Batch [180/250] | D Loss: 0.5999 | G Loss: 11.1874 (GAN: 0.8484, L1: 3.1068, Perceptual: 7.2322)\n",
      "Epoch [33/50], Batch [190/250] | D Loss: 0.5680 | G Loss: 11.5015 (GAN: 0.7872, L1: 3.3721, Perceptual: 7.3422)\n",
      "Epoch [33/50], Batch [190/250] | D Loss: 0.5680 | G Loss: 11.5015 (GAN: 0.7872, L1: 3.3721, Perceptual: 7.3422)\n",
      "Epoch [33/50], Batch [200/250] | D Loss: 0.6051 | G Loss: 11.5469 (GAN: 0.7964, L1: 3.2986, Perceptual: 7.4519)\n",
      "Epoch [33/50], Batch [200/250] | D Loss: 0.6051 | G Loss: 11.5469 (GAN: 0.7964, L1: 3.2986, Perceptual: 7.4519)\n",
      "Epoch [33/50], Batch [210/250] | D Loss: 0.5290 | G Loss: 11.5418 (GAN: 0.7266, L1: 3.3854, Perceptual: 7.4298)\n",
      "Epoch [33/50], Batch [210/250] | D Loss: 0.5290 | G Loss: 11.5418 (GAN: 0.7266, L1: 3.3854, Perceptual: 7.4298)\n",
      "Epoch [33/50], Batch [220/250] | D Loss: 0.5876 | G Loss: 9.8087 (GAN: 0.6861, L1: 2.8760, Perceptual: 6.2466)\n",
      "Epoch [33/50], Batch [220/250] | D Loss: 0.5876 | G Loss: 9.8087 (GAN: 0.6861, L1: 2.8760, Perceptual: 6.2466)\n",
      "Epoch [33/50], Batch [230/250] | D Loss: 0.7318 | G Loss: 10.4940 (GAN: 0.7430, L1: 2.9402, Perceptual: 6.8109)\n",
      "Epoch [33/50], Batch [230/250] | D Loss: 0.7318 | G Loss: 10.4940 (GAN: 0.7430, L1: 2.9402, Perceptual: 6.8109)\n",
      "Epoch [33/50], Batch [240/250] | D Loss: 0.5984 | G Loss: 12.2997 (GAN: 0.8445, L1: 3.7215, Perceptual: 7.7337)\n",
      "Epoch [33/50], Batch [240/250] | D Loss: 0.5984 | G Loss: 12.2997 (GAN: 0.8445, L1: 3.7215, Perceptual: 7.7337)\n",
      "Epoch [33/50], Batch [250/250] | D Loss: 0.6315 | G Loss: 10.7689 (GAN: 0.7618, L1: 3.0925, Perceptual: 6.9146)\n",
      "Epoch 33 completed. Current LR G: 0.000170, LR D: 0.000085\n",
      "Epoch [33/50], Batch [250/250] | D Loss: 0.6315 | G Loss: 10.7689 (GAN: 0.7618, L1: 3.0925, Perceptual: 6.9146)\n",
      "Epoch 33 completed. Current LR G: 0.000170, LR D: 0.000085\n",
      "Epoch [34/50], Batch [10/250] | D Loss: 0.6602 | G Loss: 11.1197 (GAN: 0.7686, L1: 3.2505, Perceptual: 7.1006)\n",
      "Epoch [34/50], Batch [10/250] | D Loss: 0.6602 | G Loss: 11.1197 (GAN: 0.7686, L1: 3.2505, Perceptual: 7.1006)\n",
      "Epoch [34/50], Batch [20/250] | D Loss: 0.8119 | G Loss: 12.0318 (GAN: 0.8891, L1: 3.4606, Perceptual: 7.6820)\n",
      "Epoch [34/50], Batch [20/250] | D Loss: 0.8119 | G Loss: 12.0318 (GAN: 0.8891, L1: 3.4606, Perceptual: 7.6820)\n",
      "Epoch [34/50], Batch [30/250] | D Loss: 0.6643 | G Loss: 9.7593 (GAN: 0.7301, L1: 2.6649, Perceptual: 6.3643)\n",
      "Epoch [34/50], Batch [30/250] | D Loss: 0.6643 | G Loss: 9.7593 (GAN: 0.7301, L1: 2.6649, Perceptual: 6.3643)\n",
      "Epoch [34/50], Batch [40/250] | D Loss: 0.6249 | G Loss: 11.8636 (GAN: 1.0811, L1: 3.4807, Perceptual: 7.3018)\n",
      "Epoch [34/50], Batch [40/250] | D Loss: 0.6249 | G Loss: 11.8636 (GAN: 1.0811, L1: 3.4807, Perceptual: 7.3018)\n",
      "Epoch [34/50], Batch [50/250] | D Loss: 0.7087 | G Loss: 11.5144 (GAN: 0.9568, L1: 3.4348, Perceptual: 7.1227)\n",
      "Epoch [34/50], Batch [50/250] | D Loss: 0.7087 | G Loss: 11.5144 (GAN: 0.9568, L1: 3.4348, Perceptual: 7.1227)\n",
      "Epoch [34/50], Batch [60/250] | D Loss: 0.6784 | G Loss: 11.3106 (GAN: 0.8840, L1: 3.1249, Perceptual: 7.3017)\n",
      "Epoch [34/50], Batch [60/250] | D Loss: 0.6784 | G Loss: 11.3106 (GAN: 0.8840, L1: 3.1249, Perceptual: 7.3017)\n",
      "Epoch [34/50], Batch [70/250] | D Loss: 0.6396 | G Loss: 10.0237 (GAN: 0.6165, L1: 2.6802, Perceptual: 6.7270)\n",
      "Epoch [34/50], Batch [70/250] | D Loss: 0.6396 | G Loss: 10.0237 (GAN: 0.6165, L1: 2.6802, Perceptual: 6.7270)\n",
      "Epoch [34/50], Batch [80/250] | D Loss: 0.3689 | G Loss: 12.6635 (GAN: 0.8056, L1: 3.8698, Perceptual: 7.9881)\n",
      "Epoch [34/50], Batch [80/250] | D Loss: 0.3689 | G Loss: 12.6635 (GAN: 0.8056, L1: 3.8698, Perceptual: 7.9881)\n",
      "Epoch [34/50], Batch [90/250] | D Loss: 0.5782 | G Loss: 10.7614 (GAN: 0.7075, L1: 3.3908, Perceptual: 6.6631)\n",
      "Epoch [34/50], Batch [90/250] | D Loss: 0.5782 | G Loss: 10.7614 (GAN: 0.7075, L1: 3.3908, Perceptual: 6.6631)\n",
      "Epoch [34/50], Batch [100/250] | D Loss: 0.5578 | G Loss: 11.5175 (GAN: 0.9387, L1: 3.3554, Perceptual: 7.2234)\n",
      "Epoch [34/50], Batch [100/250] | D Loss: 0.5578 | G Loss: 11.5175 (GAN: 0.9387, L1: 3.3554, Perceptual: 7.2234)\n",
      "Epoch [34/50], Batch [110/250] | D Loss: 0.5604 | G Loss: 11.7227 (GAN: 0.8007, L1: 3.5260, Perceptual: 7.3961)\n",
      "Epoch [34/50], Batch [110/250] | D Loss: 0.5604 | G Loss: 11.7227 (GAN: 0.8007, L1: 3.5260, Perceptual: 7.3961)\n",
      "Epoch [34/50], Batch [120/250] | D Loss: 0.6067 | G Loss: 11.4368 (GAN: 0.8540, L1: 3.5082, Perceptual: 7.0746)\n",
      "Epoch [34/50], Batch [120/250] | D Loss: 0.6067 | G Loss: 11.4368 (GAN: 0.8540, L1: 3.5082, Perceptual: 7.0746)\n",
      "Epoch [34/50], Batch [130/250] | D Loss: 0.6481 | G Loss: 11.4031 (GAN: 0.8126, L1: 3.5900, Perceptual: 7.0005)\n",
      "Epoch [34/50], Batch [130/250] | D Loss: 0.6481 | G Loss: 11.4031 (GAN: 0.8126, L1: 3.5900, Perceptual: 7.0005)\n",
      "Epoch [34/50], Batch [140/250] | D Loss: 0.6792 | G Loss: 11.2075 (GAN: 0.7240, L1: 3.1939, Perceptual: 7.2896)\n",
      "Epoch [34/50], Batch [140/250] | D Loss: 0.6792 | G Loss: 11.2075 (GAN: 0.7240, L1: 3.1939, Perceptual: 7.2896)\n",
      "Epoch [34/50], Batch [150/250] | D Loss: 0.5502 | G Loss: 11.7171 (GAN: 0.7463, L1: 3.6291, Perceptual: 7.3417)\n",
      "Epoch [34/50], Batch [150/250] | D Loss: 0.5502 | G Loss: 11.7171 (GAN: 0.7463, L1: 3.6291, Perceptual: 7.3417)\n",
      "Epoch [34/50], Batch [160/250] | D Loss: 0.4914 | G Loss: 11.9269 (GAN: 0.8340, L1: 3.5199, Perceptual: 7.5730)\n",
      "Epoch [34/50], Batch [160/250] | D Loss: 0.4914 | G Loss: 11.9269 (GAN: 0.8340, L1: 3.5199, Perceptual: 7.5730)\n",
      "Epoch [34/50], Batch [170/250] | D Loss: 0.6150 | G Loss: 11.0417 (GAN: 0.9188, L1: 3.1226, Perceptual: 7.0002)\n",
      "Epoch [34/50], Batch [170/250] | D Loss: 0.6150 | G Loss: 11.0417 (GAN: 0.9188, L1: 3.1226, Perceptual: 7.0002)\n",
      "Epoch [34/50], Batch [180/250] | D Loss: 0.5877 | G Loss: 9.6362 (GAN: 0.7366, L1: 2.7878, Perceptual: 6.1119)\n",
      "Epoch [34/50], Batch [180/250] | D Loss: 0.5877 | G Loss: 9.6362 (GAN: 0.7366, L1: 2.7878, Perceptual: 6.1119)\n",
      "Epoch [34/50], Batch [190/250] | D Loss: 0.5142 | G Loss: 11.2965 (GAN: 0.7093, L1: 3.2751, Perceptual: 7.3121)\n",
      "Epoch [34/50], Batch [190/250] | D Loss: 0.5142 | G Loss: 11.2965 (GAN: 0.7093, L1: 3.2751, Perceptual: 7.3121)\n",
      "Epoch [34/50], Batch [200/250] | D Loss: 0.8069 | G Loss: 10.9928 (GAN: 1.1563, L1: 3.0887, Perceptual: 6.7479)\n",
      "Epoch [34/50], Batch [200/250] | D Loss: 0.8069 | G Loss: 10.9928 (GAN: 1.1563, L1: 3.0887, Perceptual: 6.7479)\n",
      "Epoch [34/50], Batch [210/250] | D Loss: 0.7798 | G Loss: 11.1823 (GAN: 0.6814, L1: 3.2220, Perceptual: 7.2788)\n",
      "Epoch [34/50], Batch [210/250] | D Loss: 0.7798 | G Loss: 11.1823 (GAN: 0.6814, L1: 3.2220, Perceptual: 7.2788)\n",
      "Epoch [34/50], Batch [220/250] | D Loss: 0.4647 | G Loss: 12.8655 (GAN: 0.9567, L1: 3.7052, Perceptual: 8.2036)\n",
      "Epoch [34/50], Batch [220/250] | D Loss: 0.4647 | G Loss: 12.8655 (GAN: 0.9567, L1: 3.7052, Perceptual: 8.2036)\n",
      "Epoch [34/50], Batch [230/250] | D Loss: 0.6664 | G Loss: 11.2611 (GAN: 0.6910, L1: 3.5122, Perceptual: 7.0579)\n",
      "Epoch [34/50], Batch [230/250] | D Loss: 0.6664 | G Loss: 11.2611 (GAN: 0.6910, L1: 3.5122, Perceptual: 7.0579)\n",
      "Epoch [34/50], Batch [240/250] | D Loss: 0.5630 | G Loss: 10.7944 (GAN: 0.6252, L1: 3.3361, Perceptual: 6.8331)\n",
      "Epoch [34/50], Batch [240/250] | D Loss: 0.5630 | G Loss: 10.7944 (GAN: 0.6252, L1: 3.3361, Perceptual: 6.8331)\n",
      "Epoch [34/50], Batch [250/250] | D Loss: 0.6242 | G Loss: 10.1809 (GAN: 0.7670, L1: 2.9865, Perceptual: 6.4274)\n",
      "Epoch 34 completed. Current LR G: 0.000160, LR D: 0.000080\n",
      "Epoch [34/50], Batch [250/250] | D Loss: 0.6242 | G Loss: 10.1809 (GAN: 0.7670, L1: 2.9865, Perceptual: 6.4274)\n",
      "Epoch 34 completed. Current LR G: 0.000160, LR D: 0.000080\n",
      "Epoch [35/50], Batch [10/250] | D Loss: 0.6670 | G Loss: 11.5623 (GAN: 0.7792, L1: 3.2510, Perceptual: 7.5322)\n",
      "Epoch [35/50], Batch [10/250] | D Loss: 0.6670 | G Loss: 11.5623 (GAN: 0.7792, L1: 3.2510, Perceptual: 7.5322)\n",
      "Epoch [35/50], Batch [20/250] | D Loss: 0.6625 | G Loss: 10.0151 (GAN: 0.8695, L1: 2.7200, Perceptual: 6.4256)\n",
      "Epoch [35/50], Batch [20/250] | D Loss: 0.6625 | G Loss: 10.0151 (GAN: 0.8695, L1: 2.7200, Perceptual: 6.4256)\n",
      "Epoch [35/50], Batch [30/250] | D Loss: 0.5490 | G Loss: 10.8327 (GAN: 0.8626, L1: 3.0847, Perceptual: 6.8854)\n",
      "Epoch [35/50], Batch [30/250] | D Loss: 0.5490 | G Loss: 10.8327 (GAN: 0.8626, L1: 3.0847, Perceptual: 6.8854)\n",
      "Epoch [35/50], Batch [40/250] | D Loss: 0.7074 | G Loss: 9.7490 (GAN: 0.6600, L1: 2.7995, Perceptual: 6.2895)\n",
      "Epoch [35/50], Batch [40/250] | D Loss: 0.7074 | G Loss: 9.7490 (GAN: 0.6600, L1: 2.7995, Perceptual: 6.2895)\n",
      "Epoch [35/50], Batch [50/250] | D Loss: 0.6759 | G Loss: 10.8300 (GAN: 0.8425, L1: 3.0092, Perceptual: 6.9783)\n",
      "Epoch [35/50], Batch [50/250] | D Loss: 0.6759 | G Loss: 10.8300 (GAN: 0.8425, L1: 3.0092, Perceptual: 6.9783)\n",
      "Epoch [35/50], Batch [60/250] | D Loss: 0.5974 | G Loss: 11.5654 (GAN: 0.8888, L1: 3.5405, Perceptual: 7.1362)\n",
      "Epoch [35/50], Batch [60/250] | D Loss: 0.5974 | G Loss: 11.5654 (GAN: 0.8888, L1: 3.5405, Perceptual: 7.1362)\n",
      "Epoch [35/50], Batch [70/250] | D Loss: 0.9114 | G Loss: 10.6615 (GAN: 0.6348, L1: 3.3350, Perceptual: 6.6918)\n",
      "Epoch [35/50], Batch [70/250] | D Loss: 0.9114 | G Loss: 10.6615 (GAN: 0.6348, L1: 3.3350, Perceptual: 6.6918)\n",
      "Epoch [35/50], Batch [80/250] | D Loss: 0.5072 | G Loss: 10.8678 (GAN: 0.7941, L1: 3.2421, Perceptual: 6.8316)\n",
      "Epoch [35/50], Batch [80/250] | D Loss: 0.5072 | G Loss: 10.8678 (GAN: 0.7941, L1: 3.2421, Perceptual: 6.8316)\n",
      "Epoch [35/50], Batch [90/250] | D Loss: 0.5007 | G Loss: 11.6795 (GAN: 0.8121, L1: 3.3619, Perceptual: 7.5055)\n",
      "Epoch [35/50], Batch [90/250] | D Loss: 0.5007 | G Loss: 11.6795 (GAN: 0.8121, L1: 3.3619, Perceptual: 7.5055)\n",
      "Epoch [35/50], Batch [100/250] | D Loss: 0.5861 | G Loss: 12.8435 (GAN: 0.9562, L1: 4.2085, Perceptual: 7.6788)\n",
      "Epoch [35/50], Batch [100/250] | D Loss: 0.5861 | G Loss: 12.8435 (GAN: 0.9562, L1: 4.2085, Perceptual: 7.6788)\n",
      "Epoch [35/50], Batch [110/250] | D Loss: 0.6537 | G Loss: 11.2760 (GAN: 0.7766, L1: 3.3431, Perceptual: 7.1564)\n",
      "Epoch [35/50], Batch [110/250] | D Loss: 0.6537 | G Loss: 11.2760 (GAN: 0.7766, L1: 3.3431, Perceptual: 7.1564)\n",
      "Epoch [35/50], Batch [120/250] | D Loss: 0.5619 | G Loss: 10.7871 (GAN: 0.8021, L1: 2.9909, Perceptual: 6.9941)\n",
      "Epoch [35/50], Batch [120/250] | D Loss: 0.5619 | G Loss: 10.7871 (GAN: 0.8021, L1: 2.9909, Perceptual: 6.9941)\n",
      "Epoch [35/50], Batch [130/250] | D Loss: 0.4288 | G Loss: 12.1330 (GAN: 0.7246, L1: 3.4355, Perceptual: 7.9729)\n",
      "Epoch [35/50], Batch [130/250] | D Loss: 0.4288 | G Loss: 12.1330 (GAN: 0.7246, L1: 3.4355, Perceptual: 7.9729)\n",
      "Epoch [35/50], Batch [140/250] | D Loss: 0.5957 | G Loss: 10.9186 (GAN: 0.7792, L1: 3.2612, Perceptual: 6.8782)\n",
      "Epoch [35/50], Batch [140/250] | D Loss: 0.5957 | G Loss: 10.9186 (GAN: 0.7792, L1: 3.2612, Perceptual: 6.8782)\n",
      "Epoch [35/50], Batch [150/250] | D Loss: 0.6712 | G Loss: 11.6595 (GAN: 1.0270, L1: 3.8261, Perceptual: 6.8064)\n",
      "Epoch [35/50], Batch [150/250] | D Loss: 0.6712 | G Loss: 11.6595 (GAN: 1.0270, L1: 3.8261, Perceptual: 6.8064)\n",
      "Epoch [35/50], Batch [160/250] | D Loss: 0.5541 | G Loss: 12.2944 (GAN: 0.7187, L1: 3.7809, Perceptual: 7.7948)\n",
      "Epoch [35/50], Batch [160/250] | D Loss: 0.5541 | G Loss: 12.2944 (GAN: 0.7187, L1: 3.7809, Perceptual: 7.7948)\n",
      "Epoch [35/50], Batch [170/250] | D Loss: 0.5053 | G Loss: 11.8476 (GAN: 0.7752, L1: 3.6687, Perceptual: 7.4037)\n",
      "Epoch [35/50], Batch [170/250] | D Loss: 0.5053 | G Loss: 11.8476 (GAN: 0.7752, L1: 3.6687, Perceptual: 7.4037)\n",
      "Epoch [35/50], Batch [180/250] | D Loss: 0.6994 | G Loss: 10.3875 (GAN: 0.8155, L1: 2.9455, Perceptual: 6.6265)\n",
      "Epoch [35/50], Batch [180/250] | D Loss: 0.6994 | G Loss: 10.3875 (GAN: 0.8155, L1: 2.9455, Perceptual: 6.6265)\n",
      "Epoch [35/50], Batch [190/250] | D Loss: 0.6234 | G Loss: 10.7812 (GAN: 0.8380, L1: 2.8810, Perceptual: 7.0622)\n",
      "Epoch [35/50], Batch [190/250] | D Loss: 0.6234 | G Loss: 10.7812 (GAN: 0.8380, L1: 2.8810, Perceptual: 7.0622)\n",
      "Epoch [35/50], Batch [200/250] | D Loss: 0.8222 | G Loss: 11.0134 (GAN: 1.1909, L1: 2.9994, Perceptual: 6.8232)\n",
      "Epoch [35/50], Batch [200/250] | D Loss: 0.8222 | G Loss: 11.0134 (GAN: 1.1909, L1: 2.9994, Perceptual: 6.8232)\n",
      "Epoch [35/50], Batch [210/250] | D Loss: 0.4894 | G Loss: 10.9113 (GAN: 0.7445, L1: 3.1734, Perceptual: 6.9933)\n",
      "Epoch [35/50], Batch [210/250] | D Loss: 0.4894 | G Loss: 10.9113 (GAN: 0.7445, L1: 3.1734, Perceptual: 6.9933)\n",
      "Epoch [35/50], Batch [220/250] | D Loss: 0.5721 | G Loss: 12.2588 (GAN: 0.8940, L1: 3.6222, Perceptual: 7.7426)\n",
      "Epoch [35/50], Batch [220/250] | D Loss: 0.5721 | G Loss: 12.2588 (GAN: 0.8940, L1: 3.6222, Perceptual: 7.7426)\n",
      "Epoch [35/50], Batch [230/250] | D Loss: 0.7146 | G Loss: 10.4459 (GAN: 0.7356, L1: 3.1333, Perceptual: 6.5769)\n",
      "Epoch [35/50], Batch [230/250] | D Loss: 0.7146 | G Loss: 10.4459 (GAN: 0.7356, L1: 3.1333, Perceptual: 6.5769)\n",
      "Epoch [35/50], Batch [240/250] | D Loss: 0.6456 | G Loss: 12.3911 (GAN: 0.7081, L1: 4.0555, Perceptual: 7.6275)\n",
      "Epoch [35/50], Batch [240/250] | D Loss: 0.6456 | G Loss: 12.3911 (GAN: 0.7081, L1: 4.0555, Perceptual: 7.6275)\n",
      "Epoch [35/50], Batch [250/250] | D Loss: 0.4895 | G Loss: 12.5673 (GAN: 0.8740, L1: 3.6632, Perceptual: 8.0300)\n",
      "Epoch 35 completed. Current LR G: 0.000150, LR D: 0.000075\n",
      "--- Entering save/checkpoint block for epoch 35 ---\n",
      "Saving samples and checkpoint for epoch 35...\n",
      "Epoch [35/50], Batch [250/250] | D Loss: 0.4895 | G Loss: 12.5673 (GAN: 0.8740, L1: 3.6632, Perceptual: 8.0300)\n",
      "Epoch 35 completed. Current LR G: 0.000150, LR D: 0.000075\n",
      "--- Entering save/checkpoint block for epoch 35 ---\n",
      "Saving samples and checkpoint for epoch 35...\n",
      "Successfully saved image to: output/generated_images\\epoch_35_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_35_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [36/50], Batch [10/250] | D Loss: 0.6285 | G Loss: 11.3699 (GAN: 0.8123, L1: 3.5316, Perceptual: 7.0261)\n",
      "Epoch [36/50], Batch [10/250] | D Loss: 0.6285 | G Loss: 11.3699 (GAN: 0.8123, L1: 3.5316, Perceptual: 7.0261)\n",
      "Epoch [36/50], Batch [20/250] | D Loss: 0.5868 | G Loss: 11.1404 (GAN: 0.8116, L1: 3.2711, Perceptual: 7.0577)\n",
      "Epoch [36/50], Batch [20/250] | D Loss: 0.5868 | G Loss: 11.1404 (GAN: 0.8116, L1: 3.2711, Perceptual: 7.0577)\n",
      "Epoch [36/50], Batch [30/250] | D Loss: 0.8149 | G Loss: 11.5236 (GAN: 1.0418, L1: 3.3272, Perceptual: 7.1546)\n",
      "Epoch [36/50], Batch [30/250] | D Loss: 0.8149 | G Loss: 11.5236 (GAN: 1.0418, L1: 3.3272, Perceptual: 7.1546)\n",
      "Epoch [36/50], Batch [40/250] | D Loss: 0.5998 | G Loss: 11.9001 (GAN: 0.6285, L1: 3.6334, Perceptual: 7.6383)\n",
      "Epoch [36/50], Batch [40/250] | D Loss: 0.5998 | G Loss: 11.9001 (GAN: 0.6285, L1: 3.6334, Perceptual: 7.6383)\n",
      "Epoch [36/50], Batch [50/250] | D Loss: 0.6780 | G Loss: 12.2350 (GAN: 0.7061, L1: 3.9207, Perceptual: 7.6082)\n",
      "Epoch [36/50], Batch [50/250] | D Loss: 0.6780 | G Loss: 12.2350 (GAN: 0.7061, L1: 3.9207, Perceptual: 7.6082)\n",
      "Epoch [36/50], Batch [60/250] | D Loss: 0.6572 | G Loss: 10.8850 (GAN: 0.9603, L1: 3.1463, Perceptual: 6.7784)\n",
      "Epoch [36/50], Batch [60/250] | D Loss: 0.6572 | G Loss: 10.8850 (GAN: 0.9603, L1: 3.1463, Perceptual: 6.7784)\n",
      "Epoch [36/50], Batch [70/250] | D Loss: 0.7495 | G Loss: 11.0570 (GAN: 0.8582, L1: 3.2198, Perceptual: 6.9789)\n",
      "Epoch [36/50], Batch [70/250] | D Loss: 0.7495 | G Loss: 11.0570 (GAN: 0.8582, L1: 3.2198, Perceptual: 6.9789)\n",
      "Epoch [36/50], Batch [80/250] | D Loss: 0.8357 | G Loss: 10.9976 (GAN: 0.8473, L1: 3.1962, Perceptual: 6.9541)\n",
      "Epoch [36/50], Batch [80/250] | D Loss: 0.8357 | G Loss: 10.9976 (GAN: 0.8473, L1: 3.1962, Perceptual: 6.9541)\n",
      "Epoch [36/50], Batch [90/250] | D Loss: 0.6444 | G Loss: 12.3543 (GAN: 0.7747, L1: 4.0192, Perceptual: 7.5604)\n",
      "Epoch [36/50], Batch [90/250] | D Loss: 0.6444 | G Loss: 12.3543 (GAN: 0.7747, L1: 4.0192, Perceptual: 7.5604)\n",
      "Epoch [36/50], Batch [100/250] | D Loss: 0.6644 | G Loss: 11.4418 (GAN: 0.7979, L1: 3.6187, Perceptual: 7.0252)\n",
      "Epoch [36/50], Batch [100/250] | D Loss: 0.6644 | G Loss: 11.4418 (GAN: 0.7979, L1: 3.6187, Perceptual: 7.0252)\n",
      "Epoch [36/50], Batch [110/250] | D Loss: 0.5683 | G Loss: 11.5674 (GAN: 0.9449, L1: 3.2678, Perceptual: 7.3548)\n",
      "Epoch [36/50], Batch [110/250] | D Loss: 0.5683 | G Loss: 11.5674 (GAN: 0.9449, L1: 3.2678, Perceptual: 7.3548)\n",
      "Epoch [36/50], Batch [120/250] | D Loss: 0.5806 | G Loss: 11.8090 (GAN: 0.9107, L1: 3.6156, Perceptual: 7.2827)\n",
      "Epoch [36/50], Batch [120/250] | D Loss: 0.5806 | G Loss: 11.8090 (GAN: 0.9107, L1: 3.6156, Perceptual: 7.2827)\n",
      "Epoch [36/50], Batch [130/250] | D Loss: 0.6610 | G Loss: 11.1530 (GAN: 0.8502, L1: 3.2241, Perceptual: 7.0787)\n",
      "Epoch [36/50], Batch [130/250] | D Loss: 0.6610 | G Loss: 11.1530 (GAN: 0.8502, L1: 3.2241, Perceptual: 7.0787)\n",
      "Epoch [36/50], Batch [140/250] | D Loss: 0.7110 | G Loss: 10.1154 (GAN: 0.8100, L1: 2.8590, Perceptual: 6.4464)\n",
      "Epoch [36/50], Batch [140/250] | D Loss: 0.7110 | G Loss: 10.1154 (GAN: 0.8100, L1: 2.8590, Perceptual: 6.4464)\n",
      "Epoch [36/50], Batch [150/250] | D Loss: 0.6198 | G Loss: 12.1917 (GAN: 0.7884, L1: 3.5437, Perceptual: 7.8597)\n",
      "Epoch [36/50], Batch [150/250] | D Loss: 0.6198 | G Loss: 12.1917 (GAN: 0.7884, L1: 3.5437, Perceptual: 7.8597)\n",
      "Epoch [36/50], Batch [160/250] | D Loss: 0.7188 | G Loss: 10.0777 (GAN: 0.6813, L1: 2.5969, Perceptual: 6.7995)\n",
      "Epoch [36/50], Batch [160/250] | D Loss: 0.7188 | G Loss: 10.0777 (GAN: 0.6813, L1: 2.5969, Perceptual: 6.7995)\n",
      "Epoch [36/50], Batch [170/250] | D Loss: 0.6216 | G Loss: 11.4084 (GAN: 0.7688, L1: 3.3266, Perceptual: 7.3129)\n",
      "Epoch [36/50], Batch [170/250] | D Loss: 0.6216 | G Loss: 11.4084 (GAN: 0.7688, L1: 3.3266, Perceptual: 7.3129)\n",
      "Epoch [36/50], Batch [180/250] | D Loss: 0.6300 | G Loss: 12.4856 (GAN: 0.8427, L1: 3.8692, Perceptual: 7.7737)\n",
      "Epoch [36/50], Batch [180/250] | D Loss: 0.6300 | G Loss: 12.4856 (GAN: 0.8427, L1: 3.8692, Perceptual: 7.7737)\n",
      "Epoch [36/50], Batch [190/250] | D Loss: 0.5742 | G Loss: 12.3098 (GAN: 0.8357, L1: 3.9787, Perceptual: 7.4955)\n",
      "Epoch [36/50], Batch [190/250] | D Loss: 0.5742 | G Loss: 12.3098 (GAN: 0.8357, L1: 3.9787, Perceptual: 7.4955)\n",
      "Epoch [36/50], Batch [200/250] | D Loss: 0.6830 | G Loss: 10.9788 (GAN: 0.7889, L1: 3.3932, Perceptual: 6.7966)\n",
      "Epoch [36/50], Batch [200/250] | D Loss: 0.6830 | G Loss: 10.9788 (GAN: 0.7889, L1: 3.3932, Perceptual: 6.7966)\n",
      "Epoch [36/50], Batch [210/250] | D Loss: 0.5815 | G Loss: 10.7185 (GAN: 0.7141, L1: 3.1014, Perceptual: 6.9030)\n",
      "Epoch [36/50], Batch [210/250] | D Loss: 0.5815 | G Loss: 10.7185 (GAN: 0.7141, L1: 3.1014, Perceptual: 6.9030)\n",
      "Epoch [36/50], Batch [220/250] | D Loss: 0.5525 | G Loss: 11.8538 (GAN: 0.6986, L1: 3.6108, Perceptual: 7.5444)\n",
      "Epoch [36/50], Batch [220/250] | D Loss: 0.5525 | G Loss: 11.8538 (GAN: 0.6986, L1: 3.6108, Perceptual: 7.5444)\n",
      "Epoch [36/50], Batch [230/250] | D Loss: 0.5614 | G Loss: 12.4151 (GAN: 0.8458, L1: 3.7812, Perceptual: 7.7882)\n",
      "Epoch [36/50], Batch [230/250] | D Loss: 0.5614 | G Loss: 12.4151 (GAN: 0.8458, L1: 3.7812, Perceptual: 7.7882)\n",
      "Epoch [36/50], Batch [240/250] | D Loss: 0.6700 | G Loss: 11.6151 (GAN: 0.8526, L1: 3.6129, Perceptual: 7.1496)\n",
      "Epoch [36/50], Batch [240/250] | D Loss: 0.6700 | G Loss: 11.6151 (GAN: 0.8526, L1: 3.6129, Perceptual: 7.1496)\n",
      "Epoch [36/50], Batch [250/250] | D Loss: 0.5414 | G Loss: 11.8863 (GAN: 0.7668, L1: 3.4711, Perceptual: 7.6484)\n",
      "Epoch 36 completed. Current LR G: 0.000140, LR D: 0.000070\n",
      "Epoch [36/50], Batch [250/250] | D Loss: 0.5414 | G Loss: 11.8863 (GAN: 0.7668, L1: 3.4711, Perceptual: 7.6484)\n",
      "Epoch 36 completed. Current LR G: 0.000140, LR D: 0.000070\n",
      "Epoch [37/50], Batch [10/250] | D Loss: 0.5249 | G Loss: 10.8130 (GAN: 0.8306, L1: 3.1018, Perceptual: 6.8806)\n",
      "Epoch [37/50], Batch [10/250] | D Loss: 0.5249 | G Loss: 10.8130 (GAN: 0.8306, L1: 3.1018, Perceptual: 6.8806)\n",
      "Epoch [37/50], Batch [20/250] | D Loss: 0.4984 | G Loss: 11.6737 (GAN: 0.7946, L1: 3.3023, Perceptual: 7.5769)\n",
      "Epoch [37/50], Batch [20/250] | D Loss: 0.4984 | G Loss: 11.6737 (GAN: 0.7946, L1: 3.3023, Perceptual: 7.5769)\n",
      "Epoch [37/50], Batch [30/250] | D Loss: 0.5714 | G Loss: 11.7786 (GAN: 0.8007, L1: 3.5148, Perceptual: 7.4631)\n",
      "Epoch [37/50], Batch [30/250] | D Loss: 0.5714 | G Loss: 11.7786 (GAN: 0.8007, L1: 3.5148, Perceptual: 7.4631)\n",
      "Epoch [37/50], Batch [40/250] | D Loss: 0.5764 | G Loss: 11.6228 (GAN: 0.9526, L1: 3.5190, Perceptual: 7.1512)\n",
      "Epoch [37/50], Batch [40/250] | D Loss: 0.5764 | G Loss: 11.6228 (GAN: 0.9526, L1: 3.5190, Perceptual: 7.1512)\n",
      "Epoch [37/50], Batch [50/250] | D Loss: 0.5635 | G Loss: 10.7414 (GAN: 0.6504, L1: 3.1164, Perceptual: 6.9747)\n",
      "Epoch [37/50], Batch [50/250] | D Loss: 0.5635 | G Loss: 10.7414 (GAN: 0.6504, L1: 3.1164, Perceptual: 6.9747)\n",
      "Epoch [37/50], Batch [60/250] | D Loss: 0.4597 | G Loss: 12.7232 (GAN: 0.8531, L1: 4.0355, Perceptual: 7.8347)\n",
      "Epoch [37/50], Batch [60/250] | D Loss: 0.4597 | G Loss: 12.7232 (GAN: 0.8531, L1: 4.0355, Perceptual: 7.8347)\n",
      "Epoch [37/50], Batch [70/250] | D Loss: 0.5730 | G Loss: 12.2983 (GAN: 0.7511, L1: 3.5669, Perceptual: 7.9802)\n",
      "Epoch [37/50], Batch [70/250] | D Loss: 0.5730 | G Loss: 12.2983 (GAN: 0.7511, L1: 3.5669, Perceptual: 7.9802)\n",
      "Epoch [37/50], Batch [80/250] | D Loss: 0.6733 | G Loss: 10.5182 (GAN: 0.7220, L1: 3.0070, Perceptual: 6.7892)\n",
      "Epoch [37/50], Batch [80/250] | D Loss: 0.6733 | G Loss: 10.5182 (GAN: 0.7220, L1: 3.0070, Perceptual: 6.7892)\n",
      "Epoch [37/50], Batch [90/250] | D Loss: 0.5602 | G Loss: 11.1428 (GAN: 0.7297, L1: 3.1446, Perceptual: 7.2685)\n",
      "Epoch [37/50], Batch [90/250] | D Loss: 0.5602 | G Loss: 11.1428 (GAN: 0.7297, L1: 3.1446, Perceptual: 7.2685)\n",
      "Epoch [37/50], Batch [100/250] | D Loss: 0.5668 | G Loss: 10.9527 (GAN: 0.6946, L1: 3.2172, Perceptual: 7.0409)\n",
      "Epoch [37/50], Batch [100/250] | D Loss: 0.5668 | G Loss: 10.9527 (GAN: 0.6946, L1: 3.2172, Perceptual: 7.0409)\n",
      "Epoch [37/50], Batch [110/250] | D Loss: 0.6946 | G Loss: 11.8913 (GAN: 0.6212, L1: 3.7932, Perceptual: 7.4769)\n",
      "Epoch [37/50], Batch [110/250] | D Loss: 0.6946 | G Loss: 11.8913 (GAN: 0.6212, L1: 3.7932, Perceptual: 7.4769)\n",
      "Epoch [37/50], Batch [120/250] | D Loss: 0.7655 | G Loss: 11.4977 (GAN: 0.9019, L1: 3.4373, Perceptual: 7.1585)\n",
      "Epoch [37/50], Batch [120/250] | D Loss: 0.7655 | G Loss: 11.4977 (GAN: 0.9019, L1: 3.4373, Perceptual: 7.1585)\n",
      "Epoch [37/50], Batch [130/250] | D Loss: 0.5765 | G Loss: 11.7007 (GAN: 0.8844, L1: 3.2771, Perceptual: 7.5392)\n",
      "Epoch [37/50], Batch [130/250] | D Loss: 0.5765 | G Loss: 11.7007 (GAN: 0.8844, L1: 3.2771, Perceptual: 7.5392)\n",
      "Epoch [37/50], Batch [140/250] | D Loss: 0.6292 | G Loss: 10.8582 (GAN: 0.8945, L1: 3.0149, Perceptual: 6.9488)\n",
      "Epoch [37/50], Batch [140/250] | D Loss: 0.6292 | G Loss: 10.8582 (GAN: 0.8945, L1: 3.0149, Perceptual: 6.9488)\n",
      "Epoch [37/50], Batch [150/250] | D Loss: 0.6552 | G Loss: 10.2350 (GAN: 0.7471, L1: 2.8492, Perceptual: 6.6387)\n",
      "Epoch [37/50], Batch [150/250] | D Loss: 0.6552 | G Loss: 10.2350 (GAN: 0.7471, L1: 2.8492, Perceptual: 6.6387)\n",
      "Epoch [37/50], Batch [160/250] | D Loss: 0.4219 | G Loss: 11.9538 (GAN: 0.9206, L1: 3.7299, Perceptual: 7.3032)\n",
      "Epoch [37/50], Batch [160/250] | D Loss: 0.4219 | G Loss: 11.9538 (GAN: 0.9206, L1: 3.7299, Perceptual: 7.3032)\n",
      "Epoch [37/50], Batch [170/250] | D Loss: 0.4795 | G Loss: 12.3503 (GAN: 0.8062, L1: 4.0177, Perceptual: 7.5263)\n",
      "Epoch [37/50], Batch [170/250] | D Loss: 0.4795 | G Loss: 12.3503 (GAN: 0.8062, L1: 4.0177, Perceptual: 7.5263)\n",
      "Epoch [37/50], Batch [180/250] | D Loss: 0.6040 | G Loss: 11.1951 (GAN: 0.6222, L1: 3.2715, Perceptual: 7.3014)\n",
      "Epoch [37/50], Batch [180/250] | D Loss: 0.6040 | G Loss: 11.1951 (GAN: 0.6222, L1: 3.2715, Perceptual: 7.3014)\n",
      "Epoch [37/50], Batch [190/250] | D Loss: 0.5989 | G Loss: 10.7872 (GAN: 0.7725, L1: 3.2590, Perceptual: 6.7557)\n",
      "Epoch [37/50], Batch [190/250] | D Loss: 0.5989 | G Loss: 10.7872 (GAN: 0.7725, L1: 3.2590, Perceptual: 6.7557)\n",
      "Epoch [37/50], Batch [200/250] | D Loss: 0.7711 | G Loss: 11.2093 (GAN: 0.9029, L1: 3.2453, Perceptual: 7.0611)\n",
      "Epoch [37/50], Batch [200/250] | D Loss: 0.7711 | G Loss: 11.2093 (GAN: 0.9029, L1: 3.2453, Perceptual: 7.0611)\n",
      "Epoch [37/50], Batch [210/250] | D Loss: 0.5109 | G Loss: 11.8803 (GAN: 0.9864, L1: 3.4857, Perceptual: 7.4082)\n",
      "Epoch [37/50], Batch [210/250] | D Loss: 0.5109 | G Loss: 11.8803 (GAN: 0.9864, L1: 3.4857, Perceptual: 7.4082)\n",
      "Epoch [37/50], Batch [220/250] | D Loss: 0.7637 | G Loss: 12.5313 (GAN: 1.0190, L1: 3.7564, Perceptual: 7.7559)\n",
      "Epoch [37/50], Batch [220/250] | D Loss: 0.7637 | G Loss: 12.5313 (GAN: 1.0190, L1: 3.7564, Perceptual: 7.7559)\n",
      "Epoch [37/50], Batch [230/250] | D Loss: 0.6129 | G Loss: 11.7139 (GAN: 0.6915, L1: 3.5675, Perceptual: 7.4549)\n",
      "Epoch [37/50], Batch [230/250] | D Loss: 0.6129 | G Loss: 11.7139 (GAN: 0.6915, L1: 3.5675, Perceptual: 7.4549)\n",
      "Epoch [37/50], Batch [240/250] | D Loss: 0.6123 | G Loss: 11.8523 (GAN: 0.8627, L1: 3.2990, Perceptual: 7.6906)\n",
      "Epoch [37/50], Batch [240/250] | D Loss: 0.6123 | G Loss: 11.8523 (GAN: 0.8627, L1: 3.2990, Perceptual: 7.6906)\n",
      "Epoch [37/50], Batch [250/250] | D Loss: 0.6000 | G Loss: 11.9317 (GAN: 0.7435, L1: 3.7260, Perceptual: 7.4622)\n",
      "Epoch 37 completed. Current LR G: 0.000130, LR D: 0.000065\n",
      "Epoch [37/50], Batch [250/250] | D Loss: 0.6000 | G Loss: 11.9317 (GAN: 0.7435, L1: 3.7260, Perceptual: 7.4622)\n",
      "Epoch 37 completed. Current LR G: 0.000130, LR D: 0.000065\n",
      "Epoch [38/50], Batch [10/250] | D Loss: 0.5648 | G Loss: 12.1697 (GAN: 0.8102, L1: 3.6286, Perceptual: 7.7309)\n",
      "Epoch [38/50], Batch [10/250] | D Loss: 0.5648 | G Loss: 12.1697 (GAN: 0.8102, L1: 3.6286, Perceptual: 7.7309)\n",
      "Epoch [38/50], Batch [20/250] | D Loss: 0.5796 | G Loss: 10.7849 (GAN: 0.8819, L1: 3.1821, Perceptual: 6.7209)\n",
      "Epoch [38/50], Batch [20/250] | D Loss: 0.5796 | G Loss: 10.7849 (GAN: 0.8819, L1: 3.1821, Perceptual: 6.7209)\n",
      "Epoch [38/50], Batch [30/250] | D Loss: 0.6027 | G Loss: 12.1708 (GAN: 0.9508, L1: 3.8328, Perceptual: 7.3872)\n",
      "Epoch [38/50], Batch [30/250] | D Loss: 0.6027 | G Loss: 12.1708 (GAN: 0.9508, L1: 3.8328, Perceptual: 7.3872)\n",
      "Epoch [38/50], Batch [40/250] | D Loss: 0.6465 | G Loss: 11.8482 (GAN: 0.8676, L1: 3.5697, Perceptual: 7.4109)\n",
      "Epoch [38/50], Batch [40/250] | D Loss: 0.6465 | G Loss: 11.8482 (GAN: 0.8676, L1: 3.5697, Perceptual: 7.4109)\n",
      "Epoch [38/50], Batch [50/250] | D Loss: 0.5125 | G Loss: 11.9345 (GAN: 0.6017, L1: 3.4441, Perceptual: 7.8888)\n",
      "Epoch [38/50], Batch [50/250] | D Loss: 0.5125 | G Loss: 11.9345 (GAN: 0.6017, L1: 3.4441, Perceptual: 7.8888)\n",
      "Epoch [38/50], Batch [60/250] | D Loss: 0.5842 | G Loss: 10.8883 (GAN: 0.7497, L1: 3.2472, Perceptual: 6.8913)\n",
      "Epoch [38/50], Batch [60/250] | D Loss: 0.5842 | G Loss: 10.8883 (GAN: 0.7497, L1: 3.2472, Perceptual: 6.8913)\n",
      "Epoch [38/50], Batch [70/250] | D Loss: 0.5585 | G Loss: 12.3614 (GAN: 0.9759, L1: 3.6698, Perceptual: 7.7157)\n",
      "Epoch [38/50], Batch [70/250] | D Loss: 0.5585 | G Loss: 12.3614 (GAN: 0.9759, L1: 3.6698, Perceptual: 7.7157)\n",
      "Epoch [38/50], Batch [80/250] | D Loss: 0.6329 | G Loss: 11.7422 (GAN: 0.7951, L1: 3.6978, Perceptual: 7.2493)\n",
      "Epoch [38/50], Batch [80/250] | D Loss: 0.6329 | G Loss: 11.7422 (GAN: 0.7951, L1: 3.6978, Perceptual: 7.2493)\n",
      "Epoch [38/50], Batch [90/250] | D Loss: 0.6907 | G Loss: 11.0596 (GAN: 0.7759, L1: 3.2594, Perceptual: 7.0244)\n",
      "Epoch [38/50], Batch [90/250] | D Loss: 0.6907 | G Loss: 11.0596 (GAN: 0.7759, L1: 3.2594, Perceptual: 7.0244)\n",
      "Epoch [38/50], Batch [100/250] | D Loss: 0.5218 | G Loss: 11.6807 (GAN: 0.8530, L1: 3.3934, Perceptual: 7.4344)\n",
      "Epoch [38/50], Batch [100/250] | D Loss: 0.5218 | G Loss: 11.6807 (GAN: 0.8530, L1: 3.3934, Perceptual: 7.4344)\n",
      "Epoch [38/50], Batch [110/250] | D Loss: 0.6329 | G Loss: 10.6786 (GAN: 0.7932, L1: 2.8762, Perceptual: 7.0093)\n",
      "Epoch [38/50], Batch [110/250] | D Loss: 0.6329 | G Loss: 10.6786 (GAN: 0.7932, L1: 2.8762, Perceptual: 7.0093)\n",
      "Epoch [38/50], Batch [120/250] | D Loss: 0.6083 | G Loss: 11.9783 (GAN: 0.7363, L1: 3.6662, Perceptual: 7.5758)\n",
      "Epoch [38/50], Batch [120/250] | D Loss: 0.6083 | G Loss: 11.9783 (GAN: 0.7363, L1: 3.6662, Perceptual: 7.5758)\n",
      "Epoch [38/50], Batch [130/250] | D Loss: 0.7610 | G Loss: 11.1003 (GAN: 0.8086, L1: 3.4044, Perceptual: 6.8873)\n",
      "Epoch [38/50], Batch [130/250] | D Loss: 0.7610 | G Loss: 11.1003 (GAN: 0.8086, L1: 3.4044, Perceptual: 6.8873)\n",
      "Epoch [38/50], Batch [140/250] | D Loss: 0.6553 | G Loss: 11.9000 (GAN: 0.8440, L1: 3.4372, Perceptual: 7.6188)\n",
      "Epoch [38/50], Batch [140/250] | D Loss: 0.6553 | G Loss: 11.9000 (GAN: 0.8440, L1: 3.4372, Perceptual: 7.6188)\n",
      "Epoch [38/50], Batch [150/250] | D Loss: 0.6466 | G Loss: 11.8651 (GAN: 0.8175, L1: 3.5355, Perceptual: 7.5121)\n",
      "Epoch [38/50], Batch [150/250] | D Loss: 0.6466 | G Loss: 11.8651 (GAN: 0.8175, L1: 3.5355, Perceptual: 7.5121)\n",
      "Epoch [38/50], Batch [160/250] | D Loss: 0.6447 | G Loss: 10.9208 (GAN: 0.7951, L1: 2.9541, Perceptual: 7.1716)\n",
      "Epoch [38/50], Batch [160/250] | D Loss: 0.6447 | G Loss: 10.9208 (GAN: 0.7951, L1: 2.9541, Perceptual: 7.1716)\n",
      "Epoch [38/50], Batch [170/250] | D Loss: 0.6039 | G Loss: 11.3535 (GAN: 0.6902, L1: 3.5376, Perceptual: 7.1257)\n",
      "Epoch [38/50], Batch [170/250] | D Loss: 0.6039 | G Loss: 11.3535 (GAN: 0.6902, L1: 3.5376, Perceptual: 7.1257)\n",
      "Epoch [38/50], Batch [180/250] | D Loss: 0.6490 | G Loss: 12.2680 (GAN: 0.8517, L1: 3.5483, Perceptual: 7.8679)\n",
      "Epoch [38/50], Batch [180/250] | D Loss: 0.6490 | G Loss: 12.2680 (GAN: 0.8517, L1: 3.5483, Perceptual: 7.8679)\n",
      "Epoch [38/50], Batch [190/250] | D Loss: 0.6010 | G Loss: 10.2614 (GAN: 0.6450, L1: 2.8124, Perceptual: 6.8040)\n",
      "Epoch [38/50], Batch [190/250] | D Loss: 0.6010 | G Loss: 10.2614 (GAN: 0.6450, L1: 2.8124, Perceptual: 6.8040)\n",
      "Epoch [38/50], Batch [200/250] | D Loss: 0.5439 | G Loss: 11.8469 (GAN: 0.8615, L1: 3.5573, Perceptual: 7.4281)\n",
      "Epoch [38/50], Batch [200/250] | D Loss: 0.5439 | G Loss: 11.8469 (GAN: 0.8615, L1: 3.5573, Perceptual: 7.4281)\n",
      "Epoch [38/50], Batch [210/250] | D Loss: 0.5995 | G Loss: 11.1189 (GAN: 0.7370, L1: 3.4004, Perceptual: 6.9815)\n",
      "Epoch [38/50], Batch [210/250] | D Loss: 0.5995 | G Loss: 11.1189 (GAN: 0.7370, L1: 3.4004, Perceptual: 6.9815)\n",
      "Epoch [38/50], Batch [220/250] | D Loss: 0.6487 | G Loss: 10.6165 (GAN: 0.8824, L1: 2.8159, Perceptual: 6.9182)\n",
      "Epoch [38/50], Batch [220/250] | D Loss: 0.6487 | G Loss: 10.6165 (GAN: 0.8824, L1: 2.8159, Perceptual: 6.9182)\n",
      "Epoch [38/50], Batch [230/250] | D Loss: 0.6674 | G Loss: 10.9367 (GAN: 0.8103, L1: 3.3446, Perceptual: 6.7818)\n",
      "Epoch [38/50], Batch [230/250] | D Loss: 0.6674 | G Loss: 10.9367 (GAN: 0.8103, L1: 3.3446, Perceptual: 6.7818)\n",
      "Epoch [38/50], Batch [240/250] | D Loss: 0.6499 | G Loss: 12.4679 (GAN: 0.9146, L1: 3.7489, Perceptual: 7.8045)\n",
      "Epoch [38/50], Batch [240/250] | D Loss: 0.6499 | G Loss: 12.4679 (GAN: 0.9146, L1: 3.7489, Perceptual: 7.8045)\n",
      "Epoch [38/50], Batch [250/250] | D Loss: 0.7667 | G Loss: 10.1052 (GAN: 0.8036, L1: 2.7884, Perceptual: 6.5132)\n",
      "Epoch 38 completed. Current LR G: 0.000120, LR D: 0.000060\n",
      "Epoch [38/50], Batch [250/250] | D Loss: 0.7667 | G Loss: 10.1052 (GAN: 0.8036, L1: 2.7884, Perceptual: 6.5132)\n",
      "Epoch 38 completed. Current LR G: 0.000120, LR D: 0.000060\n",
      "Epoch [39/50], Batch [10/250] | D Loss: 0.5368 | G Loss: 11.5483 (GAN: 0.6834, L1: 3.3663, Perceptual: 7.4986)\n",
      "Epoch [39/50], Batch [10/250] | D Loss: 0.5368 | G Loss: 11.5483 (GAN: 0.6834, L1: 3.3663, Perceptual: 7.4986)\n",
      "Epoch [39/50], Batch [20/250] | D Loss: 0.5714 | G Loss: 11.9017 (GAN: 0.7739, L1: 3.5435, Perceptual: 7.5843)\n",
      "Epoch [39/50], Batch [20/250] | D Loss: 0.5714 | G Loss: 11.9017 (GAN: 0.7739, L1: 3.5435, Perceptual: 7.5843)\n",
      "Epoch [39/50], Batch [30/250] | D Loss: 0.7146 | G Loss: 9.7385 (GAN: 0.7144, L1: 2.5985, Perceptual: 6.4256)\n",
      "Epoch [39/50], Batch [30/250] | D Loss: 0.7146 | G Loss: 9.7385 (GAN: 0.7144, L1: 2.5985, Perceptual: 6.4256)\n",
      "Epoch [39/50], Batch [40/250] | D Loss: 0.6386 | G Loss: 11.6160 (GAN: 0.8880, L1: 3.5483, Perceptual: 7.1797)\n",
      "Epoch [39/50], Batch [40/250] | D Loss: 0.6386 | G Loss: 11.6160 (GAN: 0.8880, L1: 3.5483, Perceptual: 7.1797)\n",
      "Epoch [39/50], Batch [50/250] | D Loss: 0.5112 | G Loss: 11.5792 (GAN: 0.8437, L1: 3.3371, Perceptual: 7.3984)\n",
      "Epoch [39/50], Batch [50/250] | D Loss: 0.5112 | G Loss: 11.5792 (GAN: 0.8437, L1: 3.3371, Perceptual: 7.3984)\n",
      "Epoch [39/50], Batch [60/250] | D Loss: 0.5476 | G Loss: 10.9798 (GAN: 0.8417, L1: 3.1967, Perceptual: 6.9415)\n",
      "Epoch [39/50], Batch [60/250] | D Loss: 0.5476 | G Loss: 10.9798 (GAN: 0.8417, L1: 3.1967, Perceptual: 6.9415)\n",
      "Epoch [39/50], Batch [70/250] | D Loss: 0.5530 | G Loss: 12.1728 (GAN: 0.7978, L1: 3.6513, Perceptual: 7.7238)\n",
      "Epoch [39/50], Batch [70/250] | D Loss: 0.5530 | G Loss: 12.1728 (GAN: 0.7978, L1: 3.6513, Perceptual: 7.7238)\n",
      "Epoch [39/50], Batch [80/250] | D Loss: 0.6912 | G Loss: 11.5077 (GAN: 0.9914, L1: 3.1947, Perceptual: 7.3216)\n",
      "Epoch [39/50], Batch [80/250] | D Loss: 0.6912 | G Loss: 11.5077 (GAN: 0.9914, L1: 3.1947, Perceptual: 7.3216)\n",
      "Epoch [39/50], Batch [90/250] | D Loss: 0.7306 | G Loss: 12.5084 (GAN: 0.7866, L1: 4.1049, Perceptual: 7.6168)\n",
      "Epoch [39/50], Batch [90/250] | D Loss: 0.7306 | G Loss: 12.5084 (GAN: 0.7866, L1: 4.1049, Perceptual: 7.6168)\n",
      "Epoch [39/50], Batch [100/250] | D Loss: 0.6473 | G Loss: 11.0830 (GAN: 0.8422, L1: 3.1685, Perceptual: 7.0723)\n",
      "Epoch [39/50], Batch [100/250] | D Loss: 0.6473 | G Loss: 11.0830 (GAN: 0.8422, L1: 3.1685, Perceptual: 7.0723)\n",
      "Epoch [39/50], Batch [110/250] | D Loss: 0.5782 | G Loss: 12.6652 (GAN: 0.8831, L1: 3.6799, Perceptual: 8.1021)\n",
      "Epoch [39/50], Batch [110/250] | D Loss: 0.5782 | G Loss: 12.6652 (GAN: 0.8831, L1: 3.6799, Perceptual: 8.1021)\n",
      "Epoch [39/50], Batch [120/250] | D Loss: 0.5331 | G Loss: 11.6501 (GAN: 0.8770, L1: 3.2350, Perceptual: 7.5381)\n",
      "Epoch [39/50], Batch [120/250] | D Loss: 0.5331 | G Loss: 11.6501 (GAN: 0.8770, L1: 3.2350, Perceptual: 7.5381)\n",
      "Epoch [39/50], Batch [130/250] | D Loss: 0.7133 | G Loss: 11.7896 (GAN: 0.7896, L1: 3.7883, Perceptual: 7.2118)\n",
      "Epoch [39/50], Batch [130/250] | D Loss: 0.7133 | G Loss: 11.7896 (GAN: 0.7896, L1: 3.7883, Perceptual: 7.2118)\n",
      "Epoch [39/50], Batch [140/250] | D Loss: 0.5329 | G Loss: 10.8366 (GAN: 0.7276, L1: 3.3664, Perceptual: 6.7426)\n",
      "Epoch [39/50], Batch [140/250] | D Loss: 0.5329 | G Loss: 10.8366 (GAN: 0.7276, L1: 3.3664, Perceptual: 6.7426)\n",
      "Epoch [39/50], Batch [150/250] | D Loss: 0.5682 | G Loss: 11.5157 (GAN: 0.8508, L1: 3.4744, Perceptual: 7.1905)\n",
      "Epoch [39/50], Batch [150/250] | D Loss: 0.5682 | G Loss: 11.5157 (GAN: 0.8508, L1: 3.4744, Perceptual: 7.1905)\n",
      "Epoch [39/50], Batch [160/250] | D Loss: 0.5249 | G Loss: 11.2672 (GAN: 0.7987, L1: 3.2171, Perceptual: 7.2515)\n",
      "Epoch [39/50], Batch [160/250] | D Loss: 0.5249 | G Loss: 11.2672 (GAN: 0.7987, L1: 3.2171, Perceptual: 7.2515)\n",
      "Epoch [39/50], Batch [170/250] | D Loss: 0.6835 | G Loss: 11.4057 (GAN: 0.6302, L1: 3.4237, Perceptual: 7.3518)\n",
      "Epoch [39/50], Batch [170/250] | D Loss: 0.6835 | G Loss: 11.4057 (GAN: 0.6302, L1: 3.4237, Perceptual: 7.3518)\n",
      "Epoch [39/50], Batch [180/250] | D Loss: 0.6257 | G Loss: 11.3016 (GAN: 0.8630, L1: 3.1653, Perceptual: 7.2733)\n",
      "Epoch [39/50], Batch [180/250] | D Loss: 0.6257 | G Loss: 11.3016 (GAN: 0.8630, L1: 3.1653, Perceptual: 7.2733)\n",
      "Epoch [39/50], Batch [190/250] | D Loss: 0.5923 | G Loss: 12.2101 (GAN: 0.8276, L1: 3.7175, Perceptual: 7.6650)\n",
      "Epoch [39/50], Batch [190/250] | D Loss: 0.5923 | G Loss: 12.2101 (GAN: 0.8276, L1: 3.7175, Perceptual: 7.6650)\n",
      "Epoch [39/50], Batch [200/250] | D Loss: 0.5563 | G Loss: 12.2523 (GAN: 0.7971, L1: 3.6627, Perceptual: 7.7924)\n",
      "Epoch [39/50], Batch [200/250] | D Loss: 0.5563 | G Loss: 12.2523 (GAN: 0.7971, L1: 3.6627, Perceptual: 7.7924)\n",
      "Epoch [39/50], Batch [210/250] | D Loss: 0.5479 | G Loss: 12.4796 (GAN: 0.7949, L1: 3.6848, Perceptual: 7.9999)\n",
      "Epoch [39/50], Batch [210/250] | D Loss: 0.5479 | G Loss: 12.4796 (GAN: 0.7949, L1: 3.6848, Perceptual: 7.9999)\n",
      "Epoch [39/50], Batch [220/250] | D Loss: 0.6851 | G Loss: 11.9501 (GAN: 0.6260, L1: 3.5806, Perceptual: 7.7435)\n",
      "Epoch [39/50], Batch [220/250] | D Loss: 0.6851 | G Loss: 11.9501 (GAN: 0.6260, L1: 3.5806, Perceptual: 7.7435)\n",
      "Epoch [39/50], Batch [230/250] | D Loss: 0.5782 | G Loss: 10.8002 (GAN: 0.7984, L1: 3.2127, Perceptual: 6.7891)\n",
      "Epoch [39/50], Batch [230/250] | D Loss: 0.5782 | G Loss: 10.8002 (GAN: 0.7984, L1: 3.2127, Perceptual: 6.7891)\n",
      "Epoch [39/50], Batch [240/250] | D Loss: 0.7937 | G Loss: 11.7586 (GAN: 0.7827, L1: 3.4698, Perceptual: 7.5062)\n",
      "Epoch [39/50], Batch [240/250] | D Loss: 0.7937 | G Loss: 11.7586 (GAN: 0.7827, L1: 3.4698, Perceptual: 7.5062)\n",
      "Epoch [39/50], Batch [250/250] | D Loss: 0.7040 | G Loss: 11.5031 (GAN: 0.7893, L1: 3.6875, Perceptual: 7.0262)\n",
      "Epoch 39 completed. Current LR G: 0.000110, LR D: 0.000055\n",
      "Epoch [39/50], Batch [250/250] | D Loss: 0.7040 | G Loss: 11.5031 (GAN: 0.7893, L1: 3.6875, Perceptual: 7.0262)\n",
      "Epoch 39 completed. Current LR G: 0.000110, LR D: 0.000055\n",
      "Epoch [40/50], Batch [10/250] | D Loss: 0.4829 | G Loss: 12.4489 (GAN: 0.7284, L1: 3.6513, Perceptual: 8.0692)\n",
      "Epoch [40/50], Batch [10/250] | D Loss: 0.4829 | G Loss: 12.4489 (GAN: 0.7284, L1: 3.6513, Perceptual: 8.0692)\n",
      "Epoch [40/50], Batch [20/250] | D Loss: 0.5043 | G Loss: 13.0827 (GAN: 0.8105, L1: 4.0269, Perceptual: 8.2453)\n",
      "Epoch [40/50], Batch [20/250] | D Loss: 0.5043 | G Loss: 13.0827 (GAN: 0.8105, L1: 4.0269, Perceptual: 8.2453)\n",
      "Epoch [40/50], Batch [30/250] | D Loss: 0.5949 | G Loss: 11.7736 (GAN: 0.7978, L1: 3.8019, Perceptual: 7.1739)\n",
      "Epoch [40/50], Batch [30/250] | D Loss: 0.5949 | G Loss: 11.7736 (GAN: 0.7978, L1: 3.8019, Perceptual: 7.1739)\n",
      "Epoch [40/50], Batch [40/250] | D Loss: 0.6458 | G Loss: 11.2996 (GAN: 0.8149, L1: 3.5236, Perceptual: 6.9610)\n",
      "Epoch [40/50], Batch [40/250] | D Loss: 0.6458 | G Loss: 11.2996 (GAN: 0.8149, L1: 3.5236, Perceptual: 6.9610)\n",
      "Epoch [40/50], Batch [50/250] | D Loss: 0.6658 | G Loss: 11.1930 (GAN: 0.8665, L1: 3.2508, Perceptual: 7.0758)\n",
      "Epoch [40/50], Batch [50/250] | D Loss: 0.6658 | G Loss: 11.1930 (GAN: 0.8665, L1: 3.2508, Perceptual: 7.0758)\n",
      "Epoch [40/50], Batch [60/250] | D Loss: 0.6480 | G Loss: 10.5962 (GAN: 0.8832, L1: 3.0431, Perceptual: 6.6699)\n",
      "Epoch [40/50], Batch [60/250] | D Loss: 0.6480 | G Loss: 10.5962 (GAN: 0.8832, L1: 3.0431, Perceptual: 6.6699)\n",
      "Epoch [40/50], Batch [70/250] | D Loss: 0.6605 | G Loss: 11.0597 (GAN: 0.8756, L1: 3.2544, Perceptual: 6.9297)\n",
      "Epoch [40/50], Batch [70/250] | D Loss: 0.6605 | G Loss: 11.0597 (GAN: 0.8756, L1: 3.2544, Perceptual: 6.9297)\n",
      "Epoch [40/50], Batch [80/250] | D Loss: 0.5707 | G Loss: 11.5562 (GAN: 0.8285, L1: 3.4387, Perceptual: 7.2890)\n",
      "Epoch [40/50], Batch [80/250] | D Loss: 0.5707 | G Loss: 11.5562 (GAN: 0.8285, L1: 3.4387, Perceptual: 7.2890)\n",
      "Epoch [40/50], Batch [90/250] | D Loss: 0.6919 | G Loss: 10.6575 (GAN: 0.7517, L1: 3.0814, Perceptual: 6.8244)\n",
      "Epoch [40/50], Batch [90/250] | D Loss: 0.6919 | G Loss: 10.6575 (GAN: 0.7517, L1: 3.0814, Perceptual: 6.8244)\n",
      "Epoch [40/50], Batch [100/250] | D Loss: 0.5459 | G Loss: 11.5381 (GAN: 0.8414, L1: 3.5115, Perceptual: 7.1851)\n",
      "Epoch [40/50], Batch [100/250] | D Loss: 0.5459 | G Loss: 11.5381 (GAN: 0.8414, L1: 3.5115, Perceptual: 7.1851)\n",
      "Epoch [40/50], Batch [110/250] | D Loss: 0.5798 | G Loss: 10.7333 (GAN: 0.7911, L1: 2.9460, Perceptual: 6.9962)\n",
      "Epoch [40/50], Batch [110/250] | D Loss: 0.5798 | G Loss: 10.7333 (GAN: 0.7911, L1: 2.9460, Perceptual: 6.9962)\n",
      "Epoch [40/50], Batch [120/250] | D Loss: 0.8104 | G Loss: 11.8119 (GAN: 0.8997, L1: 3.5351, Perceptual: 7.3771)\n",
      "Epoch [40/50], Batch [120/250] | D Loss: 0.8104 | G Loss: 11.8119 (GAN: 0.8997, L1: 3.5351, Perceptual: 7.3771)\n",
      "Epoch [40/50], Batch [130/250] | D Loss: 0.5362 | G Loss: 12.1534 (GAN: 0.8399, L1: 3.7609, Perceptual: 7.5526)\n",
      "Epoch [40/50], Batch [130/250] | D Loss: 0.5362 | G Loss: 12.1534 (GAN: 0.8399, L1: 3.7609, Perceptual: 7.5526)\n",
      "Epoch [40/50], Batch [140/250] | D Loss: 0.5871 | G Loss: 11.8310 (GAN: 0.7418, L1: 3.5929, Perceptual: 7.4963)\n",
      "Epoch [40/50], Batch [140/250] | D Loss: 0.5871 | G Loss: 11.8310 (GAN: 0.7418, L1: 3.5929, Perceptual: 7.4963)\n",
      "Epoch [40/50], Batch [150/250] | D Loss: 0.6332 | G Loss: 11.9685 (GAN: 0.9281, L1: 3.4004, Perceptual: 7.6400)\n",
      "Epoch [40/50], Batch [150/250] | D Loss: 0.6332 | G Loss: 11.9685 (GAN: 0.9281, L1: 3.4004, Perceptual: 7.6400)\n",
      "Epoch [40/50], Batch [160/250] | D Loss: 0.6894 | G Loss: 11.2555 (GAN: 0.8044, L1: 3.3977, Perceptual: 7.0535)\n",
      "Epoch [40/50], Batch [160/250] | D Loss: 0.6894 | G Loss: 11.2555 (GAN: 0.8044, L1: 3.3977, Perceptual: 7.0535)\n",
      "Epoch [40/50], Batch [170/250] | D Loss: 0.7646 | G Loss: 11.6154 (GAN: 0.8770, L1: 3.5393, Perceptual: 7.1991)\n",
      "Epoch [40/50], Batch [170/250] | D Loss: 0.7646 | G Loss: 11.6154 (GAN: 0.8770, L1: 3.5393, Perceptual: 7.1991)\n",
      "Epoch [40/50], Batch [180/250] | D Loss: 0.5033 | G Loss: 11.7779 (GAN: 0.7810, L1: 3.6453, Perceptual: 7.3516)\n",
      "Epoch [40/50], Batch [180/250] | D Loss: 0.5033 | G Loss: 11.7779 (GAN: 0.7810, L1: 3.6453, Perceptual: 7.3516)\n",
      "Epoch [40/50], Batch [190/250] | D Loss: 0.5151 | G Loss: 12.2019 (GAN: 0.8740, L1: 3.4531, Perceptual: 7.8748)\n",
      "Epoch [40/50], Batch [190/250] | D Loss: 0.5151 | G Loss: 12.2019 (GAN: 0.8740, L1: 3.4531, Perceptual: 7.8748)\n",
      "Epoch [40/50], Batch [200/250] | D Loss: 0.5466 | G Loss: 12.2574 (GAN: 0.7152, L1: 3.8080, Perceptual: 7.7342)\n",
      "Epoch [40/50], Batch [200/250] | D Loss: 0.5466 | G Loss: 12.2574 (GAN: 0.7152, L1: 3.8080, Perceptual: 7.7342)\n",
      "Epoch [40/50], Batch [210/250] | D Loss: 0.5667 | G Loss: 10.7821 (GAN: 0.8591, L1: 2.9452, Perceptual: 6.9778)\n",
      "Epoch [40/50], Batch [210/250] | D Loss: 0.5667 | G Loss: 10.7821 (GAN: 0.8591, L1: 2.9452, Perceptual: 6.9778)\n",
      "Epoch [40/50], Batch [220/250] | D Loss: 0.6324 | G Loss: 11.5154 (GAN: 0.9975, L1: 3.2132, Perceptual: 7.3047)\n",
      "Epoch [40/50], Batch [220/250] | D Loss: 0.6324 | G Loss: 11.5154 (GAN: 0.9975, L1: 3.2132, Perceptual: 7.3047)\n",
      "Epoch [40/50], Batch [230/250] | D Loss: 0.4880 | G Loss: 11.5290 (GAN: 0.7564, L1: 3.3139, Perceptual: 7.4586)\n",
      "Epoch [40/50], Batch [230/250] | D Loss: 0.4880 | G Loss: 11.5290 (GAN: 0.7564, L1: 3.3139, Perceptual: 7.4586)\n",
      "Epoch [40/50], Batch [240/250] | D Loss: 0.6364 | G Loss: 11.1001 (GAN: 0.7921, L1: 3.1009, Perceptual: 7.2071)\n",
      "Epoch [40/50], Batch [240/250] | D Loss: 0.6364 | G Loss: 11.1001 (GAN: 0.7921, L1: 3.1009, Perceptual: 7.2071)\n",
      "Epoch [40/50], Batch [250/250] | D Loss: 0.5723 | G Loss: 11.9756 (GAN: 0.8163, L1: 3.6898, Perceptual: 7.4696)\n",
      "Epoch 40 completed. Current LR G: 0.000100, LR D: 0.000050\n",
      "--- Entering save/checkpoint block for epoch 40 ---\n",
      "Saving samples and checkpoint for epoch 40...\n",
      "Epoch [40/50], Batch [250/250] | D Loss: 0.5723 | G Loss: 11.9756 (GAN: 0.8163, L1: 3.6898, Perceptual: 7.4696)\n",
      "Epoch 40 completed. Current LR G: 0.000100, LR D: 0.000050\n",
      "--- Entering save/checkpoint block for epoch 40 ---\n",
      "Saving samples and checkpoint for epoch 40...\n",
      "Successfully saved image to: output/generated_images\\epoch_40_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_40_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [41/50], Batch [10/250] | D Loss: 0.5463 | G Loss: 12.7030 (GAN: 0.9320, L1: 3.8645, Perceptual: 7.9064)\n",
      "Epoch [41/50], Batch [10/250] | D Loss: 0.5463 | G Loss: 12.7030 (GAN: 0.9320, L1: 3.8645, Perceptual: 7.9064)\n",
      "Epoch [41/50], Batch [20/250] | D Loss: 0.6027 | G Loss: 11.7362 (GAN: 0.7905, L1: 3.6670, Perceptual: 7.2787)\n",
      "Epoch [41/50], Batch [20/250] | D Loss: 0.6027 | G Loss: 11.7362 (GAN: 0.7905, L1: 3.6670, Perceptual: 7.2787)\n",
      "Epoch [41/50], Batch [30/250] | D Loss: 0.6252 | G Loss: 11.3859 (GAN: 0.9065, L1: 3.3808, Perceptual: 7.0985)\n",
      "Epoch [41/50], Batch [30/250] | D Loss: 0.6252 | G Loss: 11.3859 (GAN: 0.9065, L1: 3.3808, Perceptual: 7.0985)\n",
      "Epoch [41/50], Batch [40/250] | D Loss: 0.6441 | G Loss: 10.4204 (GAN: 0.8985, L1: 2.8872, Perceptual: 6.6347)\n",
      "Epoch [41/50], Batch [40/250] | D Loss: 0.6441 | G Loss: 10.4204 (GAN: 0.8985, L1: 2.8872, Perceptual: 6.6347)\n",
      "Epoch [41/50], Batch [50/250] | D Loss: 0.4363 | G Loss: 11.7203 (GAN: 0.9310, L1: 3.3382, Perceptual: 7.4511)\n",
      "Epoch [41/50], Batch [50/250] | D Loss: 0.4363 | G Loss: 11.7203 (GAN: 0.9310, L1: 3.3382, Perceptual: 7.4511)\n",
      "Epoch [41/50], Batch [60/250] | D Loss: 0.5467 | G Loss: 11.5987 (GAN: 0.7577, L1: 3.6132, Perceptual: 7.2278)\n",
      "Epoch [41/50], Batch [60/250] | D Loss: 0.5467 | G Loss: 11.5987 (GAN: 0.7577, L1: 3.6132, Perceptual: 7.2278)\n",
      "Epoch [41/50], Batch [70/250] | D Loss: 0.5760 | G Loss: 10.5308 (GAN: 0.7939, L1: 2.9836, Perceptual: 6.7533)\n",
      "Epoch [41/50], Batch [70/250] | D Loss: 0.5760 | G Loss: 10.5308 (GAN: 0.7939, L1: 2.9836, Perceptual: 6.7533)\n",
      "Epoch [41/50], Batch [80/250] | D Loss: 0.8765 | G Loss: 11.9907 (GAN: 0.8824, L1: 3.6272, Perceptual: 7.4810)\n",
      "Epoch [41/50], Batch [80/250] | D Loss: 0.8765 | G Loss: 11.9907 (GAN: 0.8824, L1: 3.6272, Perceptual: 7.4810)\n",
      "Epoch [41/50], Batch [90/250] | D Loss: 0.5043 | G Loss: 10.8822 (GAN: 0.6674, L1: 3.2515, Perceptual: 6.9633)\n",
      "Epoch [41/50], Batch [90/250] | D Loss: 0.5043 | G Loss: 10.8822 (GAN: 0.6674, L1: 3.2515, Perceptual: 6.9633)\n",
      "Epoch [41/50], Batch [100/250] | D Loss: 0.5987 | G Loss: 11.4750 (GAN: 0.8263, L1: 3.3888, Perceptual: 7.2599)\n",
      "Epoch [41/50], Batch [100/250] | D Loss: 0.5987 | G Loss: 11.4750 (GAN: 0.8263, L1: 3.3888, Perceptual: 7.2599)\n",
      "Epoch [41/50], Batch [110/250] | D Loss: 0.5398 | G Loss: 11.6701 (GAN: 0.7929, L1: 3.3328, Perceptual: 7.5444)\n",
      "Epoch [41/50], Batch [110/250] | D Loss: 0.5398 | G Loss: 11.6701 (GAN: 0.7929, L1: 3.3328, Perceptual: 7.5444)\n",
      "Epoch [41/50], Batch [120/250] | D Loss: 0.7889 | G Loss: 12.0535 (GAN: 1.0494, L1: 3.4561, Perceptual: 7.5480)\n",
      "Epoch [41/50], Batch [120/250] | D Loss: 0.7889 | G Loss: 12.0535 (GAN: 1.0494, L1: 3.4561, Perceptual: 7.5480)\n",
      "Epoch [41/50], Batch [130/250] | D Loss: 0.6298 | G Loss: 11.2631 (GAN: 0.6145, L1: 3.3823, Perceptual: 7.2662)\n",
      "Epoch [41/50], Batch [130/250] | D Loss: 0.6298 | G Loss: 11.2631 (GAN: 0.6145, L1: 3.3823, Perceptual: 7.2662)\n",
      "Epoch [41/50], Batch [140/250] | D Loss: 0.6773 | G Loss: 13.0515 (GAN: 0.8867, L1: 4.6876, Perceptual: 7.4772)\n",
      "Epoch [41/50], Batch [140/250] | D Loss: 0.6773 | G Loss: 13.0515 (GAN: 0.8867, L1: 4.6876, Perceptual: 7.4772)\n",
      "Epoch [41/50], Batch [150/250] | D Loss: 0.4724 | G Loss: 11.9177 (GAN: 0.8356, L1: 3.5357, Perceptual: 7.5465)\n",
      "Epoch [41/50], Batch [150/250] | D Loss: 0.4724 | G Loss: 11.9177 (GAN: 0.8356, L1: 3.5357, Perceptual: 7.5465)\n",
      "Epoch [41/50], Batch [160/250] | D Loss: 0.5606 | G Loss: 11.3204 (GAN: 0.9302, L1: 3.6201, Perceptual: 6.7701)\n",
      "Epoch [41/50], Batch [160/250] | D Loss: 0.5606 | G Loss: 11.3204 (GAN: 0.9302, L1: 3.6201, Perceptual: 6.7701)\n",
      "Epoch [41/50], Batch [170/250] | D Loss: 0.6036 | G Loss: 11.9482 (GAN: 0.9525, L1: 3.3827, Perceptual: 7.6131)\n",
      "Epoch [41/50], Batch [170/250] | D Loss: 0.6036 | G Loss: 11.9482 (GAN: 0.9525, L1: 3.3827, Perceptual: 7.6131)\n",
      "Epoch [41/50], Batch [180/250] | D Loss: 0.7168 | G Loss: 11.6146 (GAN: 0.7674, L1: 3.2719, Perceptual: 7.5753)\n",
      "Epoch [41/50], Batch [180/250] | D Loss: 0.7168 | G Loss: 11.6146 (GAN: 0.7674, L1: 3.2719, Perceptual: 7.5753)\n",
      "Epoch [41/50], Batch [190/250] | D Loss: 0.8005 | G Loss: 10.7371 (GAN: 0.7683, L1: 3.3305, Perceptual: 6.6383)\n",
      "Epoch [41/50], Batch [190/250] | D Loss: 0.8005 | G Loss: 10.7371 (GAN: 0.7683, L1: 3.3305, Perceptual: 6.6383)\n",
      "Epoch [41/50], Batch [200/250] | D Loss: 0.5675 | G Loss: 10.6909 (GAN: 0.7461, L1: 2.9545, Perceptual: 6.9902)\n",
      "Epoch [41/50], Batch [200/250] | D Loss: 0.5675 | G Loss: 10.6909 (GAN: 0.7461, L1: 2.9545, Perceptual: 6.9902)\n",
      "Epoch [41/50], Batch [210/250] | D Loss: 0.6689 | G Loss: 10.1612 (GAN: 0.8351, L1: 2.7556, Perceptual: 6.5706)\n",
      "Epoch [41/50], Batch [210/250] | D Loss: 0.6689 | G Loss: 10.1612 (GAN: 0.8351, L1: 2.7556, Perceptual: 6.5706)\n",
      "Epoch [41/50], Batch [220/250] | D Loss: 0.7017 | G Loss: 12.8131 (GAN: 0.8515, L1: 4.1649, Perceptual: 7.7966)\n",
      "Epoch [41/50], Batch [220/250] | D Loss: 0.7017 | G Loss: 12.8131 (GAN: 0.8515, L1: 4.1649, Perceptual: 7.7966)\n",
      "Epoch [41/50], Batch [230/250] | D Loss: 0.6583 | G Loss: 11.5678 (GAN: 0.6872, L1: 3.4011, Perceptual: 7.4795)\n",
      "Epoch [41/50], Batch [230/250] | D Loss: 0.6583 | G Loss: 11.5678 (GAN: 0.6872, L1: 3.4011, Perceptual: 7.4795)\n",
      "Epoch [41/50], Batch [240/250] | D Loss: 0.5309 | G Loss: 12.4383 (GAN: 0.7499, L1: 3.9922, Perceptual: 7.6962)\n",
      "Epoch [41/50], Batch [240/250] | D Loss: 0.5309 | G Loss: 12.4383 (GAN: 0.7499, L1: 3.9922, Perceptual: 7.6962)\n",
      "Epoch [41/50], Batch [250/250] | D Loss: 0.5481 | G Loss: 10.9977 (GAN: 0.8567, L1: 3.3008, Perceptual: 6.8402)\n",
      "Epoch 41 completed. Current LR G: 0.000090, LR D: 0.000045\n",
      "Epoch [41/50], Batch [250/250] | D Loss: 0.5481 | G Loss: 10.9977 (GAN: 0.8567, L1: 3.3008, Perceptual: 6.8402)\n",
      "Epoch 41 completed. Current LR G: 0.000090, LR D: 0.000045\n",
      "Epoch [42/50], Batch [10/250] | D Loss: 0.6370 | G Loss: 10.6865 (GAN: 0.7973, L1: 3.0457, Perceptual: 6.8434)\n",
      "Epoch [42/50], Batch [10/250] | D Loss: 0.6370 | G Loss: 10.6865 (GAN: 0.7973, L1: 3.0457, Perceptual: 6.8434)\n",
      "Epoch [42/50], Batch [20/250] | D Loss: 0.5678 | G Loss: 11.4389 (GAN: 0.8017, L1: 3.4723, Perceptual: 7.1650)\n",
      "Epoch [42/50], Batch [20/250] | D Loss: 0.5678 | G Loss: 11.4389 (GAN: 0.8017, L1: 3.4723, Perceptual: 7.1650)\n",
      "Epoch [42/50], Batch [30/250] | D Loss: 0.8097 | G Loss: 11.8687 (GAN: 0.7215, L1: 3.5816, Perceptual: 7.5656)\n",
      "Epoch [42/50], Batch [30/250] | D Loss: 0.8097 | G Loss: 11.8687 (GAN: 0.7215, L1: 3.5816, Perceptual: 7.5656)\n",
      "Epoch [42/50], Batch [40/250] | D Loss: 0.7332 | G Loss: 11.5263 (GAN: 1.0399, L1: 3.1137, Perceptual: 7.3726)\n",
      "Epoch [42/50], Batch [40/250] | D Loss: 0.7332 | G Loss: 11.5263 (GAN: 1.0399, L1: 3.1137, Perceptual: 7.3726)\n",
      "Epoch [42/50], Batch [50/250] | D Loss: 0.6485 | G Loss: 11.6463 (GAN: 0.9127, L1: 3.2127, Perceptual: 7.5209)\n",
      "Epoch [42/50], Batch [50/250] | D Loss: 0.6485 | G Loss: 11.6463 (GAN: 0.9127, L1: 3.2127, Perceptual: 7.5209)\n",
      "Epoch [42/50], Batch [60/250] | D Loss: 0.4605 | G Loss: 12.6291 (GAN: 0.8584, L1: 3.7112, Perceptual: 8.0595)\n",
      "Epoch [42/50], Batch [60/250] | D Loss: 0.4605 | G Loss: 12.6291 (GAN: 0.8584, L1: 3.7112, Perceptual: 8.0595)\n",
      "Epoch [42/50], Batch [70/250] | D Loss: 0.5362 | G Loss: 12.4492 (GAN: 0.8650, L1: 3.7850, Perceptual: 7.7992)\n",
      "Epoch [42/50], Batch [70/250] | D Loss: 0.5362 | G Loss: 12.4492 (GAN: 0.8650, L1: 3.7850, Perceptual: 7.7992)\n",
      "Epoch [42/50], Batch [80/250] | D Loss: 0.4956 | G Loss: 12.2676 (GAN: 0.7643, L1: 3.8585, Perceptual: 7.6447)\n",
      "Epoch [42/50], Batch [80/250] | D Loss: 0.4956 | G Loss: 12.2676 (GAN: 0.7643, L1: 3.8585, Perceptual: 7.6447)\n",
      "Epoch [42/50], Batch [90/250] | D Loss: 0.6038 | G Loss: 12.1072 (GAN: 0.7609, L1: 3.7507, Perceptual: 7.5956)\n",
      "Epoch [42/50], Batch [90/250] | D Loss: 0.6038 | G Loss: 12.1072 (GAN: 0.7609, L1: 3.7507, Perceptual: 7.5956)\n",
      "Epoch [42/50], Batch [100/250] | D Loss: 0.5027 | G Loss: 12.3427 (GAN: 0.8077, L1: 3.8786, Perceptual: 7.6563)\n",
      "Epoch [42/50], Batch [100/250] | D Loss: 0.5027 | G Loss: 12.3427 (GAN: 0.8077, L1: 3.8786, Perceptual: 7.6563)\n",
      "Epoch [42/50], Batch [110/250] | D Loss: 0.6319 | G Loss: 12.3963 (GAN: 1.0398, L1: 3.7221, Perceptual: 7.6343)\n",
      "Epoch [42/50], Batch [110/250] | D Loss: 0.6319 | G Loss: 12.3963 (GAN: 1.0398, L1: 3.7221, Perceptual: 7.6343)\n",
      "Epoch [42/50], Batch [120/250] | D Loss: 0.6703 | G Loss: 11.1326 (GAN: 0.6643, L1: 3.1540, Perceptual: 7.3144)\n",
      "Epoch [42/50], Batch [120/250] | D Loss: 0.6703 | G Loss: 11.1326 (GAN: 0.6643, L1: 3.1540, Perceptual: 7.3144)\n",
      "Epoch [42/50], Batch [130/250] | D Loss: 0.6245 | G Loss: 11.4807 (GAN: 0.7087, L1: 3.5544, Perceptual: 7.2176)\n",
      "Epoch [42/50], Batch [130/250] | D Loss: 0.6245 | G Loss: 11.4807 (GAN: 0.7087, L1: 3.5544, Perceptual: 7.2176)\n",
      "Epoch [42/50], Batch [140/250] | D Loss: 0.4465 | G Loss: 13.1597 (GAN: 0.9396, L1: 4.3484, Perceptual: 7.8717)\n",
      "Epoch [42/50], Batch [140/250] | D Loss: 0.4465 | G Loss: 13.1597 (GAN: 0.9396, L1: 4.3484, Perceptual: 7.8717)\n",
      "Epoch [42/50], Batch [150/250] | D Loss: 0.4406 | G Loss: 12.6858 (GAN: 0.8706, L1: 3.9964, Perceptual: 7.8188)\n",
      "Epoch [42/50], Batch [150/250] | D Loss: 0.4406 | G Loss: 12.6858 (GAN: 0.8706, L1: 3.9964, Perceptual: 7.8188)\n",
      "Epoch [42/50], Batch [160/250] | D Loss: 0.7802 | G Loss: 11.4128 (GAN: 0.8335, L1: 3.8687, Perceptual: 6.7105)\n",
      "Epoch [42/50], Batch [160/250] | D Loss: 0.7802 | G Loss: 11.4128 (GAN: 0.8335, L1: 3.8687, Perceptual: 6.7105)\n",
      "Epoch [42/50], Batch [170/250] | D Loss: 0.5612 | G Loss: 12.3644 (GAN: 0.8248, L1: 3.8981, Perceptual: 7.6415)\n",
      "Epoch [42/50], Batch [170/250] | D Loss: 0.5612 | G Loss: 12.3644 (GAN: 0.8248, L1: 3.8981, Perceptual: 7.6415)\n",
      "Epoch [42/50], Batch [180/250] | D Loss: 0.7474 | G Loss: 11.2743 (GAN: 0.9030, L1: 3.1199, Perceptual: 7.2513)\n",
      "Epoch [42/50], Batch [180/250] | D Loss: 0.7474 | G Loss: 11.2743 (GAN: 0.9030, L1: 3.1199, Perceptual: 7.2513)\n",
      "Epoch [42/50], Batch [190/250] | D Loss: 0.6002 | G Loss: 12.3736 (GAN: 0.7573, L1: 3.8688, Perceptual: 7.7475)\n",
      "Epoch [42/50], Batch [190/250] | D Loss: 0.6002 | G Loss: 12.3736 (GAN: 0.7573, L1: 3.8688, Perceptual: 7.7475)\n",
      "Epoch [42/50], Batch [200/250] | D Loss: 0.5929 | G Loss: 10.0807 (GAN: 0.8812, L1: 2.7089, Perceptual: 6.4906)\n",
      "Epoch [42/50], Batch [200/250] | D Loss: 0.5929 | G Loss: 10.0807 (GAN: 0.8812, L1: 2.7089, Perceptual: 6.4906)\n",
      "Epoch [42/50], Batch [210/250] | D Loss: 0.7275 | G Loss: 11.0282 (GAN: 0.7863, L1: 3.1458, Perceptual: 7.0961)\n",
      "Epoch [42/50], Batch [210/250] | D Loss: 0.7275 | G Loss: 11.0282 (GAN: 0.7863, L1: 3.1458, Perceptual: 7.0961)\n",
      "Epoch [42/50], Batch [220/250] | D Loss: 0.6083 | G Loss: 10.9815 (GAN: 0.8189, L1: 3.1588, Perceptual: 7.0037)\n",
      "Epoch [42/50], Batch [220/250] | D Loss: 0.6083 | G Loss: 10.9815 (GAN: 0.8189, L1: 3.1588, Perceptual: 7.0037)\n",
      "Epoch [42/50], Batch [230/250] | D Loss: 0.6025 | G Loss: 11.4999 (GAN: 0.8378, L1: 3.5221, Perceptual: 7.1400)\n",
      "Epoch [42/50], Batch [230/250] | D Loss: 0.6025 | G Loss: 11.4999 (GAN: 0.8378, L1: 3.5221, Perceptual: 7.1400)\n",
      "Epoch [42/50], Batch [240/250] | D Loss: 0.6738 | G Loss: 11.7951 (GAN: 0.9718, L1: 3.6256, Perceptual: 7.1977)\n",
      "Epoch [42/50], Batch [240/250] | D Loss: 0.6738 | G Loss: 11.7951 (GAN: 0.9718, L1: 3.6256, Perceptual: 7.1977)\n",
      "Epoch [42/50], Batch [250/250] | D Loss: 0.5008 | G Loss: 11.7281 (GAN: 0.8800, L1: 3.5473, Perceptual: 7.3008)\n",
      "Epoch 42 completed. Current LR G: 0.000080, LR D: 0.000040\n",
      "Epoch [42/50], Batch [250/250] | D Loss: 0.5008 | G Loss: 11.7281 (GAN: 0.8800, L1: 3.5473, Perceptual: 7.3008)\n",
      "Epoch 42 completed. Current LR G: 0.000080, LR D: 0.000040\n",
      "Epoch [43/50], Batch [10/250] | D Loss: 0.6816 | G Loss: 10.9796 (GAN: 0.7015, L1: 3.4110, Perceptual: 6.8671)\n",
      "Epoch [43/50], Batch [10/250] | D Loss: 0.6816 | G Loss: 10.9796 (GAN: 0.7015, L1: 3.4110, Perceptual: 6.8671)\n",
      "Epoch [43/50], Batch [20/250] | D Loss: 0.7577 | G Loss: 12.0402 (GAN: 1.0666, L1: 3.4276, Perceptual: 7.5460)\n",
      "Epoch [43/50], Batch [20/250] | D Loss: 0.7577 | G Loss: 12.0402 (GAN: 1.0666, L1: 3.4276, Perceptual: 7.5460)\n",
      "Epoch [43/50], Batch [30/250] | D Loss: 0.6173 | G Loss: 11.5724 (GAN: 0.7656, L1: 3.6075, Perceptual: 7.1992)\n",
      "Epoch [43/50], Batch [30/250] | D Loss: 0.6173 | G Loss: 11.5724 (GAN: 0.7656, L1: 3.6075, Perceptual: 7.1992)\n",
      "Epoch [43/50], Batch [40/250] | D Loss: 0.8620 | G Loss: 10.4180 (GAN: 0.7438, L1: 2.9445, Perceptual: 6.7297)\n",
      "Epoch [43/50], Batch [40/250] | D Loss: 0.8620 | G Loss: 10.4180 (GAN: 0.7438, L1: 2.9445, Perceptual: 6.7297)\n",
      "Epoch [43/50], Batch [50/250] | D Loss: 0.5539 | G Loss: 11.5896 (GAN: 0.9367, L1: 3.3533, Perceptual: 7.2995)\n",
      "Epoch [43/50], Batch [50/250] | D Loss: 0.5539 | G Loss: 11.5896 (GAN: 0.9367, L1: 3.3533, Perceptual: 7.2995)\n",
      "Epoch [43/50], Batch [60/250] | D Loss: 0.6570 | G Loss: 11.6120 (GAN: 0.9444, L1: 3.2373, Perceptual: 7.4302)\n",
      "Epoch [43/50], Batch [60/250] | D Loss: 0.6570 | G Loss: 11.6120 (GAN: 0.9444, L1: 3.2373, Perceptual: 7.4302)\n",
      "Epoch [43/50], Batch [70/250] | D Loss: 0.5052 | G Loss: 12.3839 (GAN: 0.8589, L1: 3.6984, Perceptual: 7.8265)\n",
      "Epoch [43/50], Batch [70/250] | D Loss: 0.5052 | G Loss: 12.3839 (GAN: 0.8589, L1: 3.6984, Perceptual: 7.8265)\n",
      "Epoch [43/50], Batch [80/250] | D Loss: 0.5026 | G Loss: 11.5318 (GAN: 0.9496, L1: 3.4017, Perceptual: 7.1805)\n",
      "Epoch [43/50], Batch [80/250] | D Loss: 0.5026 | G Loss: 11.5318 (GAN: 0.9496, L1: 3.4017, Perceptual: 7.1805)\n",
      "Epoch [43/50], Batch [90/250] | D Loss: 0.5510 | G Loss: 11.2489 (GAN: 0.7122, L1: 3.2901, Perceptual: 7.2465)\n",
      "Epoch [43/50], Batch [90/250] | D Loss: 0.5510 | G Loss: 11.2489 (GAN: 0.7122, L1: 3.2901, Perceptual: 7.2465)\n",
      "Epoch [43/50], Batch [100/250] | D Loss: 0.4616 | G Loss: 13.2154 (GAN: 0.9160, L1: 4.0030, Perceptual: 8.2964)\n",
      "Epoch [43/50], Batch [100/250] | D Loss: 0.4616 | G Loss: 13.2154 (GAN: 0.9160, L1: 4.0030, Perceptual: 8.2964)\n",
      "Epoch [43/50], Batch [110/250] | D Loss: 0.6640 | G Loss: 12.0302 (GAN: 0.7560, L1: 3.7064, Perceptual: 7.5678)\n",
      "Epoch [43/50], Batch [110/250] | D Loss: 0.6640 | G Loss: 12.0302 (GAN: 0.7560, L1: 3.7064, Perceptual: 7.5678)\n",
      "Epoch [43/50], Batch [120/250] | D Loss: 0.6647 | G Loss: 10.8069 (GAN: 0.7025, L1: 3.2410, Perceptual: 6.8633)\n",
      "Epoch [43/50], Batch [120/250] | D Loss: 0.6647 | G Loss: 10.8069 (GAN: 0.7025, L1: 3.2410, Perceptual: 6.8633)\n",
      "Epoch [43/50], Batch [130/250] | D Loss: 0.5245 | G Loss: 11.4388 (GAN: 0.9237, L1: 3.0532, Perceptual: 7.4619)\n",
      "Epoch [43/50], Batch [130/250] | D Loss: 0.5245 | G Loss: 11.4388 (GAN: 0.9237, L1: 3.0532, Perceptual: 7.4619)\n",
      "Epoch [43/50], Batch [140/250] | D Loss: 0.6628 | G Loss: 11.1324 (GAN: 0.9785, L1: 3.2855, Perceptual: 6.8684)\n",
      "Epoch [43/50], Batch [140/250] | D Loss: 0.6628 | G Loss: 11.1324 (GAN: 0.9785, L1: 3.2855, Perceptual: 6.8684)\n",
      "Epoch [43/50], Batch [150/250] | D Loss: 0.7029 | G Loss: 11.3380 (GAN: 1.0680, L1: 3.1959, Perceptual: 7.0740)\n",
      "Epoch [43/50], Batch [150/250] | D Loss: 0.7029 | G Loss: 11.3380 (GAN: 1.0680, L1: 3.1959, Perceptual: 7.0740)\n",
      "Epoch [43/50], Batch [160/250] | D Loss: 0.5036 | G Loss: 10.9680 (GAN: 0.6663, L1: 3.4276, Perceptual: 6.8741)\n",
      "Epoch [43/50], Batch [160/250] | D Loss: 0.5036 | G Loss: 10.9680 (GAN: 0.6663, L1: 3.4276, Perceptual: 6.8741)\n",
      "Epoch [43/50], Batch [170/250] | D Loss: 0.6933 | G Loss: 11.8108 (GAN: 1.0372, L1: 3.4430, Perceptual: 7.3306)\n",
      "Epoch [43/50], Batch [170/250] | D Loss: 0.6933 | G Loss: 11.8108 (GAN: 1.0372, L1: 3.4430, Perceptual: 7.3306)\n",
      "Epoch [43/50], Batch [180/250] | D Loss: 0.5467 | G Loss: 11.3549 (GAN: 0.8620, L1: 3.3518, Perceptual: 7.1411)\n",
      "Epoch [43/50], Batch [180/250] | D Loss: 0.5467 | G Loss: 11.3549 (GAN: 0.8620, L1: 3.3518, Perceptual: 7.1411)\n",
      "Epoch [43/50], Batch [190/250] | D Loss: 0.6296 | G Loss: 11.5793 (GAN: 0.8072, L1: 3.4855, Perceptual: 7.2866)\n",
      "Epoch [43/50], Batch [190/250] | D Loss: 0.6296 | G Loss: 11.5793 (GAN: 0.8072, L1: 3.4855, Perceptual: 7.2866)\n",
      "Epoch [43/50], Batch [200/250] | D Loss: 0.5212 | G Loss: 13.4405 (GAN: 0.9730, L1: 4.2150, Perceptual: 8.2525)\n",
      "Epoch [43/50], Batch [200/250] | D Loss: 0.5212 | G Loss: 13.4405 (GAN: 0.9730, L1: 4.2150, Perceptual: 8.2525)\n",
      "Epoch [43/50], Batch [210/250] | D Loss: 0.3963 | G Loss: 11.9990 (GAN: 0.7834, L1: 3.6149, Perceptual: 7.6007)\n",
      "Epoch [43/50], Batch [210/250] | D Loss: 0.3963 | G Loss: 11.9990 (GAN: 0.7834, L1: 3.6149, Perceptual: 7.6007)\n",
      "Epoch [43/50], Batch [220/250] | D Loss: 0.5330 | G Loss: 12.1199 (GAN: 0.8926, L1: 3.6705, Perceptual: 7.5567)\n",
      "Epoch [43/50], Batch [220/250] | D Loss: 0.5330 | G Loss: 12.1199 (GAN: 0.8926, L1: 3.6705, Perceptual: 7.5567)\n",
      "Epoch [43/50], Batch [230/250] | D Loss: 0.6113 | G Loss: 11.5454 (GAN: 0.7239, L1: 3.3189, Perceptual: 7.5026)\n",
      "Epoch [43/50], Batch [230/250] | D Loss: 0.6113 | G Loss: 11.5454 (GAN: 0.7239, L1: 3.3189, Perceptual: 7.5026)\n",
      "Epoch [43/50], Batch [240/250] | D Loss: 0.6836 | G Loss: 12.5985 (GAN: 0.8678, L1: 4.0763, Perceptual: 7.6544)\n",
      "Epoch [43/50], Batch [240/250] | D Loss: 0.6836 | G Loss: 12.5985 (GAN: 0.8678, L1: 4.0763, Perceptual: 7.6544)\n",
      "Epoch [43/50], Batch [250/250] | D Loss: 0.5210 | G Loss: 11.3542 (GAN: 0.8827, L1: 3.1749, Perceptual: 7.2966)\n",
      "Epoch 43 completed. Current LR G: 0.000070, LR D: 0.000035\n",
      "Epoch [43/50], Batch [250/250] | D Loss: 0.5210 | G Loss: 11.3542 (GAN: 0.8827, L1: 3.1749, Perceptual: 7.2966)\n",
      "Epoch 43 completed. Current LR G: 0.000070, LR D: 0.000035\n",
      "Epoch [44/50], Batch [10/250] | D Loss: 0.6717 | G Loss: 10.6916 (GAN: 0.7427, L1: 3.0655, Perceptual: 6.8834)\n",
      "Epoch [44/50], Batch [10/250] | D Loss: 0.6717 | G Loss: 10.6916 (GAN: 0.7427, L1: 3.0655, Perceptual: 6.8834)\n",
      "Epoch [44/50], Batch [20/250] | D Loss: 0.4814 | G Loss: 12.5572 (GAN: 0.9898, L1: 3.6589, Perceptual: 7.9085)\n",
      "Epoch [44/50], Batch [20/250] | D Loss: 0.4814 | G Loss: 12.5572 (GAN: 0.9898, L1: 3.6589, Perceptual: 7.9085)\n",
      "Epoch [44/50], Batch [30/250] | D Loss: 0.5997 | G Loss: 11.9391 (GAN: 0.8317, L1: 3.4990, Perceptual: 7.6084)\n",
      "Epoch [44/50], Batch [30/250] | D Loss: 0.5997 | G Loss: 11.9391 (GAN: 0.8317, L1: 3.4990, Perceptual: 7.6084)\n",
      "Epoch [44/50], Batch [40/250] | D Loss: 0.7188 | G Loss: 11.0027 (GAN: 0.8759, L1: 3.3112, Perceptual: 6.8156)\n",
      "Epoch [44/50], Batch [40/250] | D Loss: 0.7188 | G Loss: 11.0027 (GAN: 0.8759, L1: 3.3112, Perceptual: 6.8156)\n",
      "Epoch [44/50], Batch [50/250] | D Loss: 0.4444 | G Loss: 12.1002 (GAN: 0.8946, L1: 3.6684, Perceptual: 7.5372)\n",
      "Epoch [44/50], Batch [50/250] | D Loss: 0.4444 | G Loss: 12.1002 (GAN: 0.8946, L1: 3.6684, Perceptual: 7.5372)\n",
      "Epoch [44/50], Batch [60/250] | D Loss: 0.5726 | G Loss: 12.0250 (GAN: 0.7837, L1: 3.8081, Perceptual: 7.4332)\n",
      "Epoch [44/50], Batch [60/250] | D Loss: 0.5726 | G Loss: 12.0250 (GAN: 0.7837, L1: 3.8081, Perceptual: 7.4332)\n",
      "Epoch [44/50], Batch [70/250] | D Loss: 0.6596 | G Loss: 11.6339 (GAN: 0.7770, L1: 3.2013, Perceptual: 7.6556)\n",
      "Epoch [44/50], Batch [70/250] | D Loss: 0.6596 | G Loss: 11.6339 (GAN: 0.7770, L1: 3.2013, Perceptual: 7.6556)\n",
      "Epoch [44/50], Batch [80/250] | D Loss: 0.5582 | G Loss: 11.5814 (GAN: 0.7901, L1: 3.1336, Perceptual: 7.6577)\n",
      "Epoch [44/50], Batch [80/250] | D Loss: 0.5582 | G Loss: 11.5814 (GAN: 0.7901, L1: 3.1336, Perceptual: 7.6577)\n",
      "Epoch [44/50], Batch [90/250] | D Loss: 0.6090 | G Loss: 12.1987 (GAN: 0.8285, L1: 3.5746, Perceptual: 7.7956)\n",
      "Epoch [44/50], Batch [90/250] | D Loss: 0.6090 | G Loss: 12.1987 (GAN: 0.8285, L1: 3.5746, Perceptual: 7.7956)\n",
      "Epoch [44/50], Batch [100/250] | D Loss: 0.5926 | G Loss: 11.8817 (GAN: 0.8900, L1: 3.5573, Perceptual: 7.4344)\n",
      "Epoch [44/50], Batch [100/250] | D Loss: 0.5926 | G Loss: 11.8817 (GAN: 0.8900, L1: 3.5573, Perceptual: 7.4344)\n",
      "Epoch [44/50], Batch [110/250] | D Loss: 0.6120 | G Loss: 10.8545 (GAN: 0.9344, L1: 2.9005, Perceptual: 7.0197)\n",
      "Epoch [44/50], Batch [110/250] | D Loss: 0.6120 | G Loss: 10.8545 (GAN: 0.9344, L1: 2.9005, Perceptual: 7.0197)\n",
      "Epoch [44/50], Batch [120/250] | D Loss: 0.5340 | G Loss: 11.6024 (GAN: 0.6976, L1: 3.1972, Perceptual: 7.7076)\n",
      "Epoch [44/50], Batch [120/250] | D Loss: 0.5340 | G Loss: 11.6024 (GAN: 0.6976, L1: 3.1972, Perceptual: 7.7076)\n",
      "Epoch [44/50], Batch [130/250] | D Loss: 0.6510 | G Loss: 10.5657 (GAN: 0.7688, L1: 3.1548, Perceptual: 6.6421)\n",
      "Epoch [44/50], Batch [130/250] | D Loss: 0.6510 | G Loss: 10.5657 (GAN: 0.7688, L1: 3.1548, Perceptual: 6.6421)\n",
      "Epoch [44/50], Batch [140/250] | D Loss: 0.6104 | G Loss: 11.5644 (GAN: 0.8412, L1: 3.4183, Perceptual: 7.3049)\n",
      "Epoch [44/50], Batch [140/250] | D Loss: 0.6104 | G Loss: 11.5644 (GAN: 0.8412, L1: 3.4183, Perceptual: 7.3049)\n",
      "Epoch [44/50], Batch [150/250] | D Loss: 0.7987 | G Loss: 10.7402 (GAN: 0.8985, L1: 2.8860, Perceptual: 6.9557)\n",
      "Epoch [44/50], Batch [150/250] | D Loss: 0.7987 | G Loss: 10.7402 (GAN: 0.8985, L1: 2.8860, Perceptual: 6.9557)\n",
      "Epoch [44/50], Batch [160/250] | D Loss: 0.5313 | G Loss: 11.6914 (GAN: 0.9230, L1: 3.3297, Perceptual: 7.4387)\n",
      "Epoch [44/50], Batch [160/250] | D Loss: 0.5313 | G Loss: 11.6914 (GAN: 0.9230, L1: 3.3297, Perceptual: 7.4387)\n",
      "Epoch [44/50], Batch [170/250] | D Loss: 0.4866 | G Loss: 11.9015 (GAN: 0.7901, L1: 3.5840, Perceptual: 7.5274)\n",
      "Epoch [44/50], Batch [170/250] | D Loss: 0.4866 | G Loss: 11.9015 (GAN: 0.7901, L1: 3.5840, Perceptual: 7.5274)\n",
      "Epoch [44/50], Batch [180/250] | D Loss: 0.5622 | G Loss: 11.8627 (GAN: 0.7817, L1: 3.4772, Perceptual: 7.6037)\n",
      "Epoch [44/50], Batch [180/250] | D Loss: 0.5622 | G Loss: 11.8627 (GAN: 0.7817, L1: 3.4772, Perceptual: 7.6037)\n",
      "Epoch [44/50], Batch [190/250] | D Loss: 0.5718 | G Loss: 12.0663 (GAN: 0.6782, L1: 3.7572, Perceptual: 7.6309)\n",
      "Epoch [44/50], Batch [190/250] | D Loss: 0.5718 | G Loss: 12.0663 (GAN: 0.6782, L1: 3.7572, Perceptual: 7.6309)\n",
      "Epoch [44/50], Batch [200/250] | D Loss: 0.6035 | G Loss: 12.3925 (GAN: 0.8871, L1: 3.6621, Perceptual: 7.8433)\n",
      "Epoch [44/50], Batch [200/250] | D Loss: 0.6035 | G Loss: 12.3925 (GAN: 0.8871, L1: 3.6621, Perceptual: 7.8433)\n",
      "Epoch [44/50], Batch [210/250] | D Loss: 0.4437 | G Loss: 12.0023 (GAN: 0.9731, L1: 3.5118, Perceptual: 7.5175)\n",
      "Epoch [44/50], Batch [210/250] | D Loss: 0.4437 | G Loss: 12.0023 (GAN: 0.9731, L1: 3.5118, Perceptual: 7.5175)\n",
      "Epoch [44/50], Batch [220/250] | D Loss: 0.7234 | G Loss: 11.0007 (GAN: 1.0155, L1: 3.0675, Perceptual: 6.9176)\n",
      "Epoch [44/50], Batch [220/250] | D Loss: 0.7234 | G Loss: 11.0007 (GAN: 1.0155, L1: 3.0675, Perceptual: 6.9176)\n",
      "Epoch [44/50], Batch [230/250] | D Loss: 0.8008 | G Loss: 10.6016 (GAN: 1.0254, L1: 2.8796, Perceptual: 6.6967)\n",
      "Epoch [44/50], Batch [230/250] | D Loss: 0.8008 | G Loss: 10.6016 (GAN: 1.0254, L1: 2.8796, Perceptual: 6.6967)\n",
      "Epoch [44/50], Batch [240/250] | D Loss: 0.5435 | G Loss: 12.0310 (GAN: 0.9091, L1: 3.4321, Perceptual: 7.6897)\n",
      "Epoch [44/50], Batch [240/250] | D Loss: 0.5435 | G Loss: 12.0310 (GAN: 0.9091, L1: 3.4321, Perceptual: 7.6897)\n",
      "Epoch [44/50], Batch [250/250] | D Loss: 0.5217 | G Loss: 11.7168 (GAN: 0.8013, L1: 3.4721, Perceptual: 7.4434)\n",
      "Epoch 44 completed. Current LR G: 0.000060, LR D: 0.000030\n",
      "Epoch [44/50], Batch [250/250] | D Loss: 0.5217 | G Loss: 11.7168 (GAN: 0.8013, L1: 3.4721, Perceptual: 7.4434)\n",
      "Epoch 44 completed. Current LR G: 0.000060, LR D: 0.000030\n",
      "Epoch [45/50], Batch [10/250] | D Loss: 0.6261 | G Loss: 11.4733 (GAN: 0.9429, L1: 3.2922, Perceptual: 7.2382)\n",
      "Epoch [45/50], Batch [10/250] | D Loss: 0.6261 | G Loss: 11.4733 (GAN: 0.9429, L1: 3.2922, Perceptual: 7.2382)\n",
      "Epoch [45/50], Batch [20/250] | D Loss: 0.6939 | G Loss: 11.7221 (GAN: 0.9392, L1: 3.1815, Perceptual: 7.6014)\n",
      "Epoch [45/50], Batch [20/250] | D Loss: 0.6939 | G Loss: 11.7221 (GAN: 0.9392, L1: 3.1815, Perceptual: 7.6014)\n",
      "Epoch [45/50], Batch [30/250] | D Loss: 0.5664 | G Loss: 12.6449 (GAN: 1.1154, L1: 3.6401, Perceptual: 7.8894)\n",
      "Epoch [45/50], Batch [30/250] | D Loss: 0.5664 | G Loss: 12.6449 (GAN: 1.1154, L1: 3.6401, Perceptual: 7.8894)\n",
      "Epoch [45/50], Batch [40/250] | D Loss: 0.5972 | G Loss: 11.6830 (GAN: 0.7460, L1: 3.4434, Perceptual: 7.4936)\n",
      "Epoch [45/50], Batch [40/250] | D Loss: 0.5972 | G Loss: 11.6830 (GAN: 0.7460, L1: 3.4434, Perceptual: 7.4936)\n",
      "Epoch [45/50], Batch [50/250] | D Loss: 0.6652 | G Loss: 10.7345 (GAN: 0.8862, L1: 3.0626, Perceptual: 6.7857)\n",
      "Epoch [45/50], Batch [50/250] | D Loss: 0.6652 | G Loss: 10.7345 (GAN: 0.8862, L1: 3.0626, Perceptual: 6.7857)\n",
      "Epoch [45/50], Batch [60/250] | D Loss: 0.5411 | G Loss: 11.4909 (GAN: 0.9087, L1: 3.4624, Perceptual: 7.1198)\n",
      "Epoch [45/50], Batch [60/250] | D Loss: 0.5411 | G Loss: 11.4909 (GAN: 0.9087, L1: 3.4624, Perceptual: 7.1198)\n",
      "Epoch [45/50], Batch [70/250] | D Loss: 0.5392 | G Loss: 11.5722 (GAN: 0.7682, L1: 3.4208, Perceptual: 7.3831)\n",
      "Epoch [45/50], Batch [70/250] | D Loss: 0.5392 | G Loss: 11.5722 (GAN: 0.7682, L1: 3.4208, Perceptual: 7.3831)\n",
      "Epoch [45/50], Batch [80/250] | D Loss: 0.5251 | G Loss: 12.0097 (GAN: 0.8116, L1: 3.5137, Perceptual: 7.6844)\n",
      "Epoch [45/50], Batch [80/250] | D Loss: 0.5251 | G Loss: 12.0097 (GAN: 0.8116, L1: 3.5137, Perceptual: 7.6844)\n",
      "Epoch [45/50], Batch [90/250] | D Loss: 0.6382 | G Loss: 11.2511 (GAN: 0.8591, L1: 3.2011, Perceptual: 7.1909)\n",
      "Epoch [45/50], Batch [90/250] | D Loss: 0.6382 | G Loss: 11.2511 (GAN: 0.8591, L1: 3.2011, Perceptual: 7.1909)\n",
      "Epoch [45/50], Batch [100/250] | D Loss: 0.6299 | G Loss: 11.1545 (GAN: 0.9029, L1: 3.3296, Perceptual: 6.9219)\n",
      "Epoch [45/50], Batch [100/250] | D Loss: 0.6299 | G Loss: 11.1545 (GAN: 0.9029, L1: 3.3296, Perceptual: 6.9219)\n",
      "Epoch [45/50], Batch [110/250] | D Loss: 0.4608 | G Loss: 12.1553 (GAN: 0.8954, L1: 3.4914, Perceptual: 7.7685)\n",
      "Epoch [45/50], Batch [110/250] | D Loss: 0.4608 | G Loss: 12.1553 (GAN: 0.8954, L1: 3.4914, Perceptual: 7.7685)\n",
      "Epoch [45/50], Batch [120/250] | D Loss: 0.6209 | G Loss: 11.5849 (GAN: 0.7903, L1: 3.4462, Perceptual: 7.3484)\n",
      "Epoch [45/50], Batch [120/250] | D Loss: 0.6209 | G Loss: 11.5849 (GAN: 0.7903, L1: 3.4462, Perceptual: 7.3484)\n",
      "Epoch [45/50], Batch [130/250] | D Loss: 0.6141 | G Loss: 10.8473 (GAN: 1.1772, L1: 3.0191, Perceptual: 6.6510)\n",
      "Epoch [45/50], Batch [130/250] | D Loss: 0.6141 | G Loss: 10.8473 (GAN: 1.1772, L1: 3.0191, Perceptual: 6.6510)\n",
      "Epoch [45/50], Batch [140/250] | D Loss: 0.6494 | G Loss: 11.2703 (GAN: 0.8137, L1: 3.3222, Perceptual: 7.1344)\n",
      "Epoch [45/50], Batch [140/250] | D Loss: 0.6494 | G Loss: 11.2703 (GAN: 0.8137, L1: 3.3222, Perceptual: 7.1344)\n",
      "Epoch [45/50], Batch [150/250] | D Loss: 0.4260 | G Loss: 12.4765 (GAN: 1.0261, L1: 3.7334, Perceptual: 7.7170)\n",
      "Epoch [45/50], Batch [150/250] | D Loss: 0.4260 | G Loss: 12.4765 (GAN: 1.0261, L1: 3.7334, Perceptual: 7.7170)\n",
      "Epoch [45/50], Batch [160/250] | D Loss: 0.5941 | G Loss: 10.5574 (GAN: 0.7957, L1: 3.0041, Perceptual: 6.7575)\n",
      "Epoch [45/50], Batch [160/250] | D Loss: 0.5941 | G Loss: 10.5574 (GAN: 0.7957, L1: 3.0041, Perceptual: 6.7575)\n",
      "Epoch [45/50], Batch [170/250] | D Loss: 0.5561 | G Loss: 12.3922 (GAN: 0.9521, L1: 3.4650, Perceptual: 7.9751)\n",
      "Epoch [45/50], Batch [170/250] | D Loss: 0.5561 | G Loss: 12.3922 (GAN: 0.9521, L1: 3.4650, Perceptual: 7.9751)\n",
      "Epoch [45/50], Batch [180/250] | D Loss: 0.5202 | G Loss: 11.1390 (GAN: 0.7483, L1: 3.2679, Perceptual: 7.1227)\n",
      "Epoch [45/50], Batch [180/250] | D Loss: 0.5202 | G Loss: 11.1390 (GAN: 0.7483, L1: 3.2679, Perceptual: 7.1227)\n",
      "Epoch [45/50], Batch [190/250] | D Loss: 0.7012 | G Loss: 10.9378 (GAN: 0.8360, L1: 3.2140, Perceptual: 6.8879)\n",
      "Epoch [45/50], Batch [190/250] | D Loss: 0.7012 | G Loss: 10.9378 (GAN: 0.8360, L1: 3.2140, Perceptual: 6.8879)\n",
      "Epoch [45/50], Batch [200/250] | D Loss: 0.6763 | G Loss: 10.8765 (GAN: 0.8643, L1: 3.2123, Perceptual: 6.7999)\n",
      "Epoch [45/50], Batch [200/250] | D Loss: 0.6763 | G Loss: 10.8765 (GAN: 0.8643, L1: 3.2123, Perceptual: 6.7999)\n",
      "Epoch [45/50], Batch [210/250] | D Loss: 0.6035 | G Loss: 11.6469 (GAN: 0.9959, L1: 3.3432, Perceptual: 7.3078)\n",
      "Epoch [45/50], Batch [210/250] | D Loss: 0.6035 | G Loss: 11.6469 (GAN: 0.9959, L1: 3.3432, Perceptual: 7.3078)\n",
      "Epoch [45/50], Batch [220/250] | D Loss: 0.6026 | G Loss: 11.7512 (GAN: 0.6916, L1: 3.5230, Perceptual: 7.5366)\n",
      "Epoch [45/50], Batch [220/250] | D Loss: 0.6026 | G Loss: 11.7512 (GAN: 0.6916, L1: 3.5230, Perceptual: 7.5366)\n",
      "Epoch [45/50], Batch [230/250] | D Loss: 0.6106 | G Loss: 10.7780 (GAN: 0.8006, L1: 3.1382, Perceptual: 6.8392)\n",
      "Epoch [45/50], Batch [230/250] | D Loss: 0.6106 | G Loss: 10.7780 (GAN: 0.8006, L1: 3.1382, Perceptual: 6.8392)\n",
      "Epoch [45/50], Batch [240/250] | D Loss: 0.6309 | G Loss: 11.1804 (GAN: 0.8004, L1: 3.1396, Perceptual: 7.2404)\n",
      "Epoch [45/50], Batch [240/250] | D Loss: 0.6309 | G Loss: 11.1804 (GAN: 0.8004, L1: 3.1396, Perceptual: 7.2404)\n",
      "Epoch [45/50], Batch [250/250] | D Loss: 0.6313 | G Loss: 11.1788 (GAN: 0.9344, L1: 3.1539, Perceptual: 7.0905)\n",
      "Epoch 45 completed. Current LR G: 0.000050, LR D: 0.000025\n",
      "--- Entering save/checkpoint block for epoch 45 ---\n",
      "Saving samples and checkpoint for epoch 45...\n",
      "Epoch [45/50], Batch [250/250] | D Loss: 0.6313 | G Loss: 11.1788 (GAN: 0.9344, L1: 3.1539, Perceptual: 7.0905)\n",
      "Epoch 45 completed. Current LR G: 0.000050, LR D: 0.000025\n",
      "--- Entering save/checkpoint block for epoch 45 ---\n",
      "Saving samples and checkpoint for epoch 45...\n",
      "Successfully saved image to: output/generated_images\\epoch_45_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_45_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Epoch [46/50], Batch [10/250] | D Loss: 0.6082 | G Loss: 10.9999 (GAN: 0.7978, L1: 3.0961, Perceptual: 7.1059)\n",
      "Epoch [46/50], Batch [10/250] | D Loss: 0.6082 | G Loss: 10.9999 (GAN: 0.7978, L1: 3.0961, Perceptual: 7.1059)\n",
      "Epoch [46/50], Batch [20/250] | D Loss: 0.5912 | G Loss: 10.9220 (GAN: 0.7866, L1: 3.2664, Perceptual: 6.8690)\n",
      "Epoch [46/50], Batch [20/250] | D Loss: 0.5912 | G Loss: 10.9220 (GAN: 0.7866, L1: 3.2664, Perceptual: 6.8690)\n",
      "Epoch [46/50], Batch [30/250] | D Loss: 0.6002 | G Loss: 11.9447 (GAN: 0.8435, L1: 3.4613, Perceptual: 7.6398)\n",
      "Epoch [46/50], Batch [30/250] | D Loss: 0.6002 | G Loss: 11.9447 (GAN: 0.8435, L1: 3.4613, Perceptual: 7.6398)\n",
      "Epoch [46/50], Batch [40/250] | D Loss: 0.6809 | G Loss: 11.5202 (GAN: 0.8982, L1: 3.4839, Perceptual: 7.1381)\n",
      "Epoch [46/50], Batch [40/250] | D Loss: 0.6809 | G Loss: 11.5202 (GAN: 0.8982, L1: 3.4839, Perceptual: 7.1381)\n",
      "Epoch [46/50], Batch [50/250] | D Loss: 0.5197 | G Loss: 10.6185 (GAN: 0.7721, L1: 3.2592, Perceptual: 6.5872)\n",
      "Epoch [46/50], Batch [50/250] | D Loss: 0.5197 | G Loss: 10.6185 (GAN: 0.7721, L1: 3.2592, Perceptual: 6.5872)\n",
      "Epoch [46/50], Batch [60/250] | D Loss: 0.4948 | G Loss: 11.7702 (GAN: 0.7781, L1: 3.4467, Perceptual: 7.5454)\n",
      "Epoch [46/50], Batch [60/250] | D Loss: 0.4948 | G Loss: 11.7702 (GAN: 0.7781, L1: 3.4467, Perceptual: 7.5454)\n",
      "Epoch [46/50], Batch [70/250] | D Loss: 0.5707 | G Loss: 11.8390 (GAN: 0.9763, L1: 3.6345, Perceptual: 7.2282)\n",
      "Epoch [46/50], Batch [70/250] | D Loss: 0.5707 | G Loss: 11.8390 (GAN: 0.9763, L1: 3.6345, Perceptual: 7.2282)\n",
      "Epoch [46/50], Batch [80/250] | D Loss: 0.5728 | G Loss: 11.9480 (GAN: 0.8769, L1: 3.4528, Perceptual: 7.6182)\n",
      "Epoch [46/50], Batch [80/250] | D Loss: 0.5728 | G Loss: 11.9480 (GAN: 0.8769, L1: 3.4528, Perceptual: 7.6182)\n",
      "Epoch [46/50], Batch [90/250] | D Loss: 0.7382 | G Loss: 12.8485 (GAN: 0.6281, L1: 3.8506, Perceptual: 8.3698)\n",
      "Epoch [46/50], Batch [90/250] | D Loss: 0.7382 | G Loss: 12.8485 (GAN: 0.6281, L1: 3.8506, Perceptual: 8.3698)\n",
      "Epoch [46/50], Batch [100/250] | D Loss: 0.6184 | G Loss: 11.5114 (GAN: 0.9091, L1: 3.2158, Perceptual: 7.3865)\n",
      "Epoch [46/50], Batch [100/250] | D Loss: 0.6184 | G Loss: 11.5114 (GAN: 0.9091, L1: 3.2158, Perceptual: 7.3865)\n",
      "Epoch [46/50], Batch [110/250] | D Loss: 0.6292 | G Loss: 11.2589 (GAN: 0.9148, L1: 3.1264, Perceptual: 7.2177)\n",
      "Epoch [46/50], Batch [110/250] | D Loss: 0.6292 | G Loss: 11.2589 (GAN: 0.9148, L1: 3.1264, Perceptual: 7.2177)\n",
      "Epoch [46/50], Batch [120/250] | D Loss: 0.5811 | G Loss: 12.2183 (GAN: 0.9240, L1: 3.6637, Perceptual: 7.6306)\n",
      "Epoch [46/50], Batch [120/250] | D Loss: 0.5811 | G Loss: 12.2183 (GAN: 0.9240, L1: 3.6637, Perceptual: 7.6306)\n",
      "Epoch [46/50], Batch [130/250] | D Loss: 0.4310 | G Loss: 12.5247 (GAN: 0.9359, L1: 3.9275, Perceptual: 7.6613)\n",
      "Epoch [46/50], Batch [130/250] | D Loss: 0.4310 | G Loss: 12.5247 (GAN: 0.9359, L1: 3.9275, Perceptual: 7.6613)\n",
      "Epoch [46/50], Batch [140/250] | D Loss: 0.6310 | G Loss: 11.8975 (GAN: 1.0163, L1: 3.6519, Perceptual: 7.2292)\n",
      "Epoch [46/50], Batch [140/250] | D Loss: 0.6310 | G Loss: 11.8975 (GAN: 1.0163, L1: 3.6519, Perceptual: 7.2292)\n",
      "Epoch [46/50], Batch [150/250] | D Loss: 0.7392 | G Loss: 12.3650 (GAN: 0.8265, L1: 3.9205, Perceptual: 7.6180)\n",
      "Epoch [46/50], Batch [150/250] | D Loss: 0.7392 | G Loss: 12.3650 (GAN: 0.8265, L1: 3.9205, Perceptual: 7.6180)\n",
      "Epoch [46/50], Batch [160/250] | D Loss: 0.7026 | G Loss: 10.7584 (GAN: 0.9566, L1: 2.9905, Perceptual: 6.8113)\n",
      "Epoch [46/50], Batch [160/250] | D Loss: 0.7026 | G Loss: 10.7584 (GAN: 0.9566, L1: 2.9905, Perceptual: 6.8113)\n",
      "Epoch [46/50], Batch [170/250] | D Loss: 0.5081 | G Loss: 12.1279 (GAN: 0.7628, L1: 3.7241, Perceptual: 7.6411)\n",
      "Epoch [46/50], Batch [170/250] | D Loss: 0.5081 | G Loss: 12.1279 (GAN: 0.7628, L1: 3.7241, Perceptual: 7.6411)\n",
      "Epoch [46/50], Batch [180/250] | D Loss: 0.7094 | G Loss: 12.0855 (GAN: 0.8302, L1: 3.5816, Perceptual: 7.6737)\n",
      "Epoch [46/50], Batch [180/250] | D Loss: 0.7094 | G Loss: 12.0855 (GAN: 0.8302, L1: 3.5816, Perceptual: 7.6737)\n",
      "Epoch [46/50], Batch [190/250] | D Loss: 0.6220 | G Loss: 10.7400 (GAN: 0.6917, L1: 3.1071, Perceptual: 6.9411)\n",
      "Epoch [46/50], Batch [190/250] | D Loss: 0.6220 | G Loss: 10.7400 (GAN: 0.6917, L1: 3.1071, Perceptual: 6.9411)\n",
      "Epoch [46/50], Batch [200/250] | D Loss: 0.4447 | G Loss: 12.2885 (GAN: 0.8951, L1: 3.8895, Perceptual: 7.5039)\n",
      "Epoch [46/50], Batch [200/250] | D Loss: 0.4447 | G Loss: 12.2885 (GAN: 0.8951, L1: 3.8895, Perceptual: 7.5039)\n",
      "Epoch [46/50], Batch [210/250] | D Loss: 0.6698 | G Loss: 10.7365 (GAN: 0.7127, L1: 3.1593, Perceptual: 6.8644)\n",
      "Epoch [46/50], Batch [210/250] | D Loss: 0.6698 | G Loss: 10.7365 (GAN: 0.7127, L1: 3.1593, Perceptual: 6.8644)\n",
      "Epoch [46/50], Batch [220/250] | D Loss: 0.6060 | G Loss: 10.8311 (GAN: 0.7218, L1: 3.1100, Perceptual: 6.9993)\n",
      "Epoch [46/50], Batch [220/250] | D Loss: 0.6060 | G Loss: 10.8311 (GAN: 0.7218, L1: 3.1100, Perceptual: 6.9993)\n",
      "Epoch [46/50], Batch [230/250] | D Loss: 0.5290 | G Loss: 11.2289 (GAN: 0.7682, L1: 3.3406, Perceptual: 7.1201)\n",
      "Epoch [46/50], Batch [230/250] | D Loss: 0.5290 | G Loss: 11.2289 (GAN: 0.7682, L1: 3.3406, Perceptual: 7.1201)\n",
      "Epoch [46/50], Batch [240/250] | D Loss: 0.5597 | G Loss: 13.0116 (GAN: 0.7457, L1: 4.0223, Perceptual: 8.2436)\n",
      "Epoch [46/50], Batch [240/250] | D Loss: 0.5597 | G Loss: 13.0116 (GAN: 0.7457, L1: 4.0223, Perceptual: 8.2436)\n",
      "Epoch [46/50], Batch [250/250] | D Loss: 0.6629 | G Loss: 11.7695 (GAN: 0.7570, L1: 3.9099, Perceptual: 7.1026)\n",
      "Epoch 46 completed. Current LR G: 0.000040, LR D: 0.000020\n",
      "Epoch [46/50], Batch [250/250] | D Loss: 0.6629 | G Loss: 11.7695 (GAN: 0.7570, L1: 3.9099, Perceptual: 7.1026)\n",
      "Epoch 46 completed. Current LR G: 0.000040, LR D: 0.000020\n",
      "Epoch [47/50], Batch [10/250] | D Loss: 0.5798 | G Loss: 10.7107 (GAN: 0.8021, L1: 2.9860, Perceptual: 6.9225)\n",
      "Epoch [47/50], Batch [10/250] | D Loss: 0.5798 | G Loss: 10.7107 (GAN: 0.8021, L1: 2.9860, Perceptual: 6.9225)\n",
      "Epoch [47/50], Batch [20/250] | D Loss: 0.5645 | G Loss: 12.1329 (GAN: 0.8398, L1: 3.4924, Perceptual: 7.8008)\n",
      "Epoch [47/50], Batch [20/250] | D Loss: 0.5645 | G Loss: 12.1329 (GAN: 0.8398, L1: 3.4924, Perceptual: 7.8008)\n",
      "Epoch [47/50], Batch [30/250] | D Loss: 0.6553 | G Loss: 11.1802 (GAN: 0.7163, L1: 3.3670, Perceptual: 7.0969)\n",
      "Epoch [47/50], Batch [30/250] | D Loss: 0.6553 | G Loss: 11.1802 (GAN: 0.7163, L1: 3.3670, Perceptual: 7.0969)\n",
      "Epoch [47/50], Batch [40/250] | D Loss: 0.5017 | G Loss: 12.2800 (GAN: 0.8723, L1: 3.5687, Perceptual: 7.8390)\n",
      "Epoch [47/50], Batch [40/250] | D Loss: 0.5017 | G Loss: 12.2800 (GAN: 0.8723, L1: 3.5687, Perceptual: 7.8390)\n",
      "Epoch [47/50], Batch [50/250] | D Loss: 0.5574 | G Loss: 11.8347 (GAN: 0.8823, L1: 3.1614, Perceptual: 7.7910)\n",
      "Epoch [47/50], Batch [50/250] | D Loss: 0.5574 | G Loss: 11.8347 (GAN: 0.8823, L1: 3.1614, Perceptual: 7.7910)\n",
      "Epoch [47/50], Batch [60/250] | D Loss: 0.5250 | G Loss: 12.0093 (GAN: 0.9456, L1: 3.7037, Perceptual: 7.3599)\n",
      "Epoch [47/50], Batch [60/250] | D Loss: 0.5250 | G Loss: 12.0093 (GAN: 0.9456, L1: 3.7037, Perceptual: 7.3599)\n",
      "Epoch [47/50], Batch [70/250] | D Loss: 0.5833 | G Loss: 10.4754 (GAN: 0.8367, L1: 2.8809, Perceptual: 6.7578)\n",
      "Epoch [47/50], Batch [70/250] | D Loss: 0.5833 | G Loss: 10.4754 (GAN: 0.8367, L1: 2.8809, Perceptual: 6.7578)\n",
      "Epoch [47/50], Batch [80/250] | D Loss: 0.5188 | G Loss: 11.7484 (GAN: 0.8810, L1: 3.5444, Perceptual: 7.3230)\n",
      "Epoch [47/50], Batch [80/250] | D Loss: 0.5188 | G Loss: 11.7484 (GAN: 0.8810, L1: 3.5444, Perceptual: 7.3230)\n",
      "Epoch [47/50], Batch [90/250] | D Loss: 0.5837 | G Loss: 11.8253 (GAN: 0.8449, L1: 3.5830, Perceptual: 7.3975)\n",
      "Epoch [47/50], Batch [90/250] | D Loss: 0.5837 | G Loss: 11.8253 (GAN: 0.8449, L1: 3.5830, Perceptual: 7.3975)\n",
      "Epoch [47/50], Batch [100/250] | D Loss: 0.6633 | G Loss: 10.6247 (GAN: 1.0670, L1: 2.8508, Perceptual: 6.7069)\n",
      "Epoch [47/50], Batch [100/250] | D Loss: 0.6633 | G Loss: 10.6247 (GAN: 1.0670, L1: 2.8508, Perceptual: 6.7069)\n",
      "Epoch [47/50], Batch [110/250] | D Loss: 0.6258 | G Loss: 11.0178 (GAN: 0.8621, L1: 3.3477, Perceptual: 6.8080)\n",
      "Epoch [47/50], Batch [110/250] | D Loss: 0.6258 | G Loss: 11.0178 (GAN: 0.8621, L1: 3.3477, Perceptual: 6.8080)\n",
      "Epoch [47/50], Batch [120/250] | D Loss: 0.6291 | G Loss: 11.5329 (GAN: 0.9346, L1: 3.3929, Perceptual: 7.2054)\n",
      "Epoch [47/50], Batch [120/250] | D Loss: 0.6291 | G Loss: 11.5329 (GAN: 0.9346, L1: 3.3929, Perceptual: 7.2054)\n",
      "Epoch [47/50], Batch [130/250] | D Loss: 0.6034 | G Loss: 11.4054 (GAN: 0.7886, L1: 3.3440, Perceptual: 7.2727)\n",
      "Epoch [47/50], Batch [130/250] | D Loss: 0.6034 | G Loss: 11.4054 (GAN: 0.7886, L1: 3.3440, Perceptual: 7.2727)\n",
      "Epoch [47/50], Batch [140/250] | D Loss: 0.6008 | G Loss: 12.4074 (GAN: 0.8683, L1: 3.9210, Perceptual: 7.6181)\n",
      "Epoch [47/50], Batch [140/250] | D Loss: 0.6008 | G Loss: 12.4074 (GAN: 0.8683, L1: 3.9210, Perceptual: 7.6181)\n",
      "Epoch [47/50], Batch [150/250] | D Loss: 0.5688 | G Loss: 11.6995 (GAN: 0.9390, L1: 3.4196, Perceptual: 7.3409)\n",
      "Epoch [47/50], Batch [150/250] | D Loss: 0.5688 | G Loss: 11.6995 (GAN: 0.9390, L1: 3.4196, Perceptual: 7.3409)\n",
      "Epoch [47/50], Batch [160/250] | D Loss: 0.5610 | G Loss: 11.6972 (GAN: 0.7566, L1: 3.7713, Perceptual: 7.1693)\n",
      "Epoch [47/50], Batch [160/250] | D Loss: 0.5610 | G Loss: 11.6972 (GAN: 0.7566, L1: 3.7713, Perceptual: 7.1693)\n",
      "Epoch [47/50], Batch [170/250] | D Loss: 0.4997 | G Loss: 11.3708 (GAN: 0.8911, L1: 3.3706, Perceptual: 7.1091)\n",
      "Epoch [47/50], Batch [170/250] | D Loss: 0.4997 | G Loss: 11.3708 (GAN: 0.8911, L1: 3.3706, Perceptual: 7.1091)\n",
      "Epoch [47/50], Batch [180/250] | D Loss: 0.7347 | G Loss: 10.7428 (GAN: 0.7498, L1: 3.0391, Perceptual: 6.9539)\n",
      "Epoch [47/50], Batch [180/250] | D Loss: 0.7347 | G Loss: 10.7428 (GAN: 0.7498, L1: 3.0391, Perceptual: 6.9539)\n",
      "Epoch [47/50], Batch [190/250] | D Loss: 0.5333 | G Loss: 12.9812 (GAN: 0.8185, L1: 4.1072, Perceptual: 8.0555)\n",
      "Epoch [47/50], Batch [190/250] | D Loss: 0.5333 | G Loss: 12.9812 (GAN: 0.8185, L1: 4.1072, Perceptual: 8.0555)\n",
      "Epoch [47/50], Batch [200/250] | D Loss: 0.5445 | G Loss: 11.8395 (GAN: 0.7228, L1: 3.6244, Perceptual: 7.4924)\n",
      "Epoch [47/50], Batch [200/250] | D Loss: 0.5445 | G Loss: 11.8395 (GAN: 0.7228, L1: 3.6244, Perceptual: 7.4924)\n",
      "Epoch [47/50], Batch [210/250] | D Loss: 0.5239 | G Loss: 11.0073 (GAN: 0.8340, L1: 3.2655, Perceptual: 6.9078)\n",
      "Epoch [47/50], Batch [210/250] | D Loss: 0.5239 | G Loss: 11.0073 (GAN: 0.8340, L1: 3.2655, Perceptual: 6.9078)\n",
      "Epoch [47/50], Batch [220/250] | D Loss: 0.5404 | G Loss: 12.9603 (GAN: 0.9709, L1: 3.7894, Perceptual: 8.2000)\n",
      "Epoch [47/50], Batch [220/250] | D Loss: 0.5404 | G Loss: 12.9603 (GAN: 0.9709, L1: 3.7894, Perceptual: 8.2000)\n",
      "Epoch [47/50], Batch [230/250] | D Loss: 0.5767 | G Loss: 11.3837 (GAN: 0.9095, L1: 3.3063, Perceptual: 7.1680)\n",
      "Epoch [47/50], Batch [230/250] | D Loss: 0.5767 | G Loss: 11.3837 (GAN: 0.9095, L1: 3.3063, Perceptual: 7.1680)\n",
      "Epoch [47/50], Batch [240/250] | D Loss: 0.5807 | G Loss: 12.2212 (GAN: 0.8325, L1: 3.5922, Perceptual: 7.7964)\n",
      "Epoch [47/50], Batch [240/250] | D Loss: 0.5807 | G Loss: 12.2212 (GAN: 0.8325, L1: 3.5922, Perceptual: 7.7964)\n",
      "Epoch [47/50], Batch [250/250] | D Loss: 0.8025 | G Loss: 10.7239 (GAN: 0.9657, L1: 2.9511, Perceptual: 6.8071)\n",
      "Epoch 47 completed. Current LR G: 0.000030, LR D: 0.000015\n",
      "Epoch [47/50], Batch [250/250] | D Loss: 0.8025 | G Loss: 10.7239 (GAN: 0.9657, L1: 2.9511, Perceptual: 6.8071)\n",
      "Epoch 47 completed. Current LR G: 0.000030, LR D: 0.000015\n",
      "Epoch [48/50], Batch [10/250] | D Loss: 0.4900 | G Loss: 12.1237 (GAN: 0.7465, L1: 3.7432, Perceptual: 7.6341)\n",
      "Epoch [48/50], Batch [10/250] | D Loss: 0.4900 | G Loss: 12.1237 (GAN: 0.7465, L1: 3.7432, Perceptual: 7.6341)\n",
      "Epoch [48/50], Batch [20/250] | D Loss: 0.6628 | G Loss: 12.0187 (GAN: 0.7836, L1: 3.6584, Perceptual: 7.5767)\n",
      "Epoch [48/50], Batch [20/250] | D Loss: 0.6628 | G Loss: 12.0187 (GAN: 0.7836, L1: 3.6584, Perceptual: 7.5767)\n",
      "Epoch [48/50], Batch [30/250] | D Loss: 0.5552 | G Loss: 11.4329 (GAN: 0.8033, L1: 3.6028, Perceptual: 7.0268)\n",
      "Epoch [48/50], Batch [30/250] | D Loss: 0.5552 | G Loss: 11.4329 (GAN: 0.8033, L1: 3.6028, Perceptual: 7.0268)\n",
      "Epoch [48/50], Batch [40/250] | D Loss: 0.5414 | G Loss: 11.3239 (GAN: 0.7716, L1: 3.4215, Perceptual: 7.1308)\n",
      "Epoch [48/50], Batch [40/250] | D Loss: 0.5414 | G Loss: 11.3239 (GAN: 0.7716, L1: 3.4215, Perceptual: 7.1308)\n",
      "Epoch [48/50], Batch [50/250] | D Loss: 0.7115 | G Loss: 10.2997 (GAN: 0.8531, L1: 2.8682, Perceptual: 6.5784)\n",
      "Epoch [48/50], Batch [50/250] | D Loss: 0.7115 | G Loss: 10.2997 (GAN: 0.8531, L1: 2.8682, Perceptual: 6.5784)\n",
      "Epoch [48/50], Batch [60/250] | D Loss: 0.4985 | G Loss: 12.7176 (GAN: 0.9192, L1: 3.9370, Perceptual: 7.8615)\n",
      "Epoch [48/50], Batch [60/250] | D Loss: 0.4985 | G Loss: 12.7176 (GAN: 0.9192, L1: 3.9370, Perceptual: 7.8615)\n",
      "Epoch [48/50], Batch [70/250] | D Loss: 0.7448 | G Loss: 10.8524 (GAN: 0.9583, L1: 2.9797, Perceptual: 6.9144)\n",
      "Epoch [48/50], Batch [70/250] | D Loss: 0.7448 | G Loss: 10.8524 (GAN: 0.9583, L1: 2.9797, Perceptual: 6.9144)\n",
      "Epoch [48/50], Batch [80/250] | D Loss: 0.5357 | G Loss: 11.0205 (GAN: 0.8993, L1: 3.3196, Perceptual: 6.8017)\n",
      "Epoch [48/50], Batch [80/250] | D Loss: 0.5357 | G Loss: 11.0205 (GAN: 0.8993, L1: 3.3196, Perceptual: 6.8017)\n",
      "Epoch [48/50], Batch [90/250] | D Loss: 0.5889 | G Loss: 11.5689 (GAN: 0.7699, L1: 3.5368, Perceptual: 7.2621)\n",
      "Epoch [48/50], Batch [90/250] | D Loss: 0.5889 | G Loss: 11.5689 (GAN: 0.7699, L1: 3.5368, Perceptual: 7.2621)\n",
      "Epoch [48/50], Batch [100/250] | D Loss: 0.6429 | G Loss: 11.2234 (GAN: 1.0319, L1: 3.1576, Perceptual: 7.0339)\n",
      "Epoch [48/50], Batch [100/250] | D Loss: 0.6429 | G Loss: 11.2234 (GAN: 1.0319, L1: 3.1576, Perceptual: 7.0339)\n",
      "Epoch [48/50], Batch [110/250] | D Loss: 0.7902 | G Loss: 10.7658 (GAN: 1.0312, L1: 3.0033, Perceptual: 6.7313)\n",
      "Epoch [48/50], Batch [110/250] | D Loss: 0.7902 | G Loss: 10.7658 (GAN: 1.0312, L1: 3.0033, Perceptual: 6.7313)\n",
      "Epoch [48/50], Batch [120/250] | D Loss: 0.6510 | G Loss: 10.8968 (GAN: 0.7237, L1: 3.3084, Perceptual: 6.8647)\n",
      "Epoch [48/50], Batch [120/250] | D Loss: 0.6510 | G Loss: 10.8968 (GAN: 0.7237, L1: 3.3084, Perceptual: 6.8647)\n",
      "Epoch [48/50], Batch [130/250] | D Loss: 0.6202 | G Loss: 12.3624 (GAN: 1.0264, L1: 3.8191, Perceptual: 7.5170)\n",
      "Epoch [48/50], Batch [130/250] | D Loss: 0.6202 | G Loss: 12.3624 (GAN: 1.0264, L1: 3.8191, Perceptual: 7.5170)\n",
      "Epoch [48/50], Batch [140/250] | D Loss: 0.6846 | G Loss: 11.8705 (GAN: 0.7389, L1: 3.4854, Perceptual: 7.6461)\n",
      "Epoch [48/50], Batch [140/250] | D Loss: 0.6846 | G Loss: 11.8705 (GAN: 0.7389, L1: 3.4854, Perceptual: 7.6461)\n",
      "Epoch [48/50], Batch [150/250] | D Loss: 0.5108 | G Loss: 12.5149 (GAN: 0.7888, L1: 4.0263, Perceptual: 7.6998)\n",
      "Epoch [48/50], Batch [150/250] | D Loss: 0.5108 | G Loss: 12.5149 (GAN: 0.7888, L1: 4.0263, Perceptual: 7.6998)\n",
      "Epoch [48/50], Batch [160/250] | D Loss: 0.5964 | G Loss: 11.8743 (GAN: 0.8838, L1: 3.6316, Perceptual: 7.3589)\n",
      "Epoch [48/50], Batch [160/250] | D Loss: 0.5964 | G Loss: 11.8743 (GAN: 0.8838, L1: 3.6316, Perceptual: 7.3589)\n",
      "Epoch [48/50], Batch [170/250] | D Loss: 0.5744 | G Loss: 11.6027 (GAN: 0.8022, L1: 3.4751, Perceptual: 7.3254)\n",
      "Epoch [48/50], Batch [170/250] | D Loss: 0.5744 | G Loss: 11.6027 (GAN: 0.8022, L1: 3.4751, Perceptual: 7.3254)\n",
      "Epoch [48/50], Batch [180/250] | D Loss: 0.6404 | G Loss: 11.6289 (GAN: 0.7840, L1: 3.4253, Perceptual: 7.4196)\n",
      "Epoch [48/50], Batch [180/250] | D Loss: 0.6404 | G Loss: 11.6289 (GAN: 0.7840, L1: 3.4253, Perceptual: 7.4196)\n",
      "Epoch [48/50], Batch [190/250] | D Loss: 0.5467 | G Loss: 11.7769 (GAN: 0.8669, L1: 3.3382, Perceptual: 7.5717)\n",
      "Epoch [48/50], Batch [190/250] | D Loss: 0.5467 | G Loss: 11.7769 (GAN: 0.8669, L1: 3.3382, Perceptual: 7.5717)\n",
      "Epoch [48/50], Batch [200/250] | D Loss: 0.5469 | G Loss: 12.6398 (GAN: 0.8753, L1: 4.1680, Perceptual: 7.5965)\n",
      "Epoch [48/50], Batch [200/250] | D Loss: 0.5469 | G Loss: 12.6398 (GAN: 0.8753, L1: 4.1680, Perceptual: 7.5965)\n",
      "Epoch [48/50], Batch [210/250] | D Loss: 0.5767 | G Loss: 11.7570 (GAN: 0.7384, L1: 3.7230, Perceptual: 7.2956)\n",
      "Epoch [48/50], Batch [210/250] | D Loss: 0.5767 | G Loss: 11.7570 (GAN: 0.7384, L1: 3.7230, Perceptual: 7.2956)\n",
      "Epoch [48/50], Batch [220/250] | D Loss: 0.5943 | G Loss: 11.2708 (GAN: 0.8753, L1: 3.1378, Perceptual: 7.2577)\n",
      "Epoch [48/50], Batch [220/250] | D Loss: 0.5943 | G Loss: 11.2708 (GAN: 0.8753, L1: 3.1378, Perceptual: 7.2577)\n",
      "Epoch [48/50], Batch [230/250] | D Loss: 0.5633 | G Loss: 11.6770 (GAN: 0.8408, L1: 3.4802, Perceptual: 7.3560)\n",
      "Epoch [48/50], Batch [230/250] | D Loss: 0.5633 | G Loss: 11.6770 (GAN: 0.8408, L1: 3.4802, Perceptual: 7.3560)\n",
      "Epoch [48/50], Batch [240/250] | D Loss: 0.5358 | G Loss: 11.7953 (GAN: 0.9226, L1: 3.3830, Perceptual: 7.4897)\n",
      "Epoch [48/50], Batch [240/250] | D Loss: 0.5358 | G Loss: 11.7953 (GAN: 0.9226, L1: 3.3830, Perceptual: 7.4897)\n",
      "Epoch [48/50], Batch [250/250] | D Loss: 0.6135 | G Loss: 12.7404 (GAN: 0.7170, L1: 3.9392, Perceptual: 8.0842)\n",
      "Epoch 48 completed. Current LR G: 0.000020, LR D: 0.000010\n",
      "Epoch [48/50], Batch [250/250] | D Loss: 0.6135 | G Loss: 12.7404 (GAN: 0.7170, L1: 3.9392, Perceptual: 8.0842)\n",
      "Epoch 48 completed. Current LR G: 0.000020, LR D: 0.000010\n",
      "Epoch [49/50], Batch [10/250] | D Loss: 0.6618 | G Loss: 11.1599 (GAN: 0.6435, L1: 3.3972, Perceptual: 7.1193)\n",
      "Epoch [49/50], Batch [10/250] | D Loss: 0.6618 | G Loss: 11.1599 (GAN: 0.6435, L1: 3.3972, Perceptual: 7.1193)\n",
      "Epoch [49/50], Batch [20/250] | D Loss: 0.5556 | G Loss: 11.0768 (GAN: 0.9871, L1: 3.2250, Perceptual: 6.8647)\n",
      "Epoch [49/50], Batch [20/250] | D Loss: 0.5556 | G Loss: 11.0768 (GAN: 0.9871, L1: 3.2250, Perceptual: 6.8647)\n",
      "Epoch [49/50], Batch [30/250] | D Loss: 0.6727 | G Loss: 11.9657 (GAN: 0.9859, L1: 3.5747, Perceptual: 7.4051)\n",
      "Epoch [49/50], Batch [30/250] | D Loss: 0.6727 | G Loss: 11.9657 (GAN: 0.9859, L1: 3.5747, Perceptual: 7.4051)\n",
      "Epoch [49/50], Batch [40/250] | D Loss: 0.5535 | G Loss: 12.4171 (GAN: 0.9979, L1: 3.8385, Perceptual: 7.5806)\n",
      "Epoch [49/50], Batch [40/250] | D Loss: 0.5535 | G Loss: 12.4171 (GAN: 0.9979, L1: 3.8385, Perceptual: 7.5806)\n",
      "Epoch [49/50], Batch [50/250] | D Loss: 0.6190 | G Loss: 11.6060 (GAN: 0.7791, L1: 3.5146, Perceptual: 7.3124)\n",
      "Epoch [49/50], Batch [50/250] | D Loss: 0.6190 | G Loss: 11.6060 (GAN: 0.7791, L1: 3.5146, Perceptual: 7.3124)\n",
      "Epoch [49/50], Batch [60/250] | D Loss: 0.5072 | G Loss: 11.8915 (GAN: 0.8789, L1: 3.6854, Perceptual: 7.3272)\n",
      "Epoch [49/50], Batch [60/250] | D Loss: 0.5072 | G Loss: 11.8915 (GAN: 0.8789, L1: 3.6854, Perceptual: 7.3272)\n",
      "Epoch [49/50], Batch [70/250] | D Loss: 0.5691 | G Loss: 10.9354 (GAN: 0.9985, L1: 3.0398, Perceptual: 6.8971)\n",
      "Epoch [49/50], Batch [70/250] | D Loss: 0.5691 | G Loss: 10.9354 (GAN: 0.9985, L1: 3.0398, Perceptual: 6.8971)\n",
      "Epoch [49/50], Batch [80/250] | D Loss: 0.7008 | G Loss: 11.9264 (GAN: 0.7423, L1: 3.8027, Perceptual: 7.3814)\n",
      "Epoch [49/50], Batch [80/250] | D Loss: 0.7008 | G Loss: 11.9264 (GAN: 0.7423, L1: 3.8027, Perceptual: 7.3814)\n",
      "Epoch [49/50], Batch [90/250] | D Loss: 0.6210 | G Loss: 11.4715 (GAN: 0.5950, L1: 3.3048, Perceptual: 7.5717)\n",
      "Epoch [49/50], Batch [90/250] | D Loss: 0.6210 | G Loss: 11.4715 (GAN: 0.5950, L1: 3.3048, Perceptual: 7.5717)\n",
      "Epoch [49/50], Batch [100/250] | D Loss: 0.5857 | G Loss: 11.9426 (GAN: 1.1080, L1: 3.2498, Perceptual: 7.5848)\n",
      "Epoch [49/50], Batch [100/250] | D Loss: 0.5857 | G Loss: 11.9426 (GAN: 1.1080, L1: 3.2498, Perceptual: 7.5848)\n",
      "Epoch [49/50], Batch [110/250] | D Loss: 0.5610 | G Loss: 12.4377 (GAN: 0.8469, L1: 3.7524, Perceptual: 7.8384)\n",
      "Epoch [49/50], Batch [110/250] | D Loss: 0.5610 | G Loss: 12.4377 (GAN: 0.8469, L1: 3.7524, Perceptual: 7.8384)\n",
      "Epoch [49/50], Batch [120/250] | D Loss: 0.5674 | G Loss: 11.9003 (GAN: 0.8521, L1: 3.5274, Perceptual: 7.5208)\n",
      "Epoch [49/50], Batch [120/250] | D Loss: 0.5674 | G Loss: 11.9003 (GAN: 0.8521, L1: 3.5274, Perceptual: 7.5208)\n",
      "Epoch [49/50], Batch [130/250] | D Loss: 0.5440 | G Loss: 11.9674 (GAN: 0.9671, L1: 3.3178, Perceptual: 7.6824)\n",
      "Epoch [49/50], Batch [130/250] | D Loss: 0.5440 | G Loss: 11.9674 (GAN: 0.9671, L1: 3.3178, Perceptual: 7.6824)\n",
      "Epoch [49/50], Batch [140/250] | D Loss: 0.4690 | G Loss: 11.8883 (GAN: 0.9604, L1: 3.2636, Perceptual: 7.6643)\n",
      "Epoch [49/50], Batch [140/250] | D Loss: 0.4690 | G Loss: 11.8883 (GAN: 0.9604, L1: 3.2636, Perceptual: 7.6643)\n",
      "Epoch [49/50], Batch [150/250] | D Loss: 0.5787 | G Loss: 11.2221 (GAN: 0.8884, L1: 3.1774, Perceptual: 7.1562)\n",
      "Epoch [49/50], Batch [150/250] | D Loss: 0.5787 | G Loss: 11.2221 (GAN: 0.8884, L1: 3.1774, Perceptual: 7.1562)\n",
      "Epoch [49/50], Batch [160/250] | D Loss: 0.5712 | G Loss: 11.3256 (GAN: 0.8195, L1: 3.4176, Perceptual: 7.0885)\n",
      "Epoch [49/50], Batch [160/250] | D Loss: 0.5712 | G Loss: 11.3256 (GAN: 0.8195, L1: 3.4176, Perceptual: 7.0885)\n",
      "Epoch [49/50], Batch [170/250] | D Loss: 0.5715 | G Loss: 12.4106 (GAN: 0.7421, L1: 3.9643, Perceptual: 7.7041)\n",
      "Epoch [49/50], Batch [170/250] | D Loss: 0.5715 | G Loss: 12.4106 (GAN: 0.7421, L1: 3.9643, Perceptual: 7.7041)\n",
      "Epoch [49/50], Batch [180/250] | D Loss: 0.5299 | G Loss: 11.4129 (GAN: 0.9555, L1: 3.2304, Perceptual: 7.2271)\n",
      "Epoch [49/50], Batch [180/250] | D Loss: 0.5299 | G Loss: 11.4129 (GAN: 0.9555, L1: 3.2304, Perceptual: 7.2271)\n",
      "Epoch [49/50], Batch [190/250] | D Loss: 0.5074 | G Loss: 11.0741 (GAN: 0.7771, L1: 3.0816, Perceptual: 7.2154)\n",
      "Epoch [49/50], Batch [190/250] | D Loss: 0.5074 | G Loss: 11.0741 (GAN: 0.7771, L1: 3.0816, Perceptual: 7.2154)\n",
      "Epoch [49/50], Batch [200/250] | D Loss: 0.5998 | G Loss: 13.0947 (GAN: 0.9224, L1: 3.6942, Perceptual: 8.4782)\n",
      "Epoch [49/50], Batch [200/250] | D Loss: 0.5998 | G Loss: 13.0947 (GAN: 0.9224, L1: 3.6942, Perceptual: 8.4782)\n",
      "Epoch [49/50], Batch [210/250] | D Loss: 0.5409 | G Loss: 11.7954 (GAN: 0.9140, L1: 3.4330, Perceptual: 7.4485)\n",
      "Epoch [49/50], Batch [210/250] | D Loss: 0.5409 | G Loss: 11.7954 (GAN: 0.9140, L1: 3.4330, Perceptual: 7.4485)\n",
      "Epoch [49/50], Batch [220/250] | D Loss: 0.5171 | G Loss: 12.0582 (GAN: 0.8510, L1: 3.4392, Perceptual: 7.7680)\n",
      "Epoch [49/50], Batch [220/250] | D Loss: 0.5171 | G Loss: 12.0582 (GAN: 0.8510, L1: 3.4392, Perceptual: 7.7680)\n",
      "Epoch [49/50], Batch [230/250] | D Loss: 0.5751 | G Loss: 12.0978 (GAN: 0.8368, L1: 3.8884, Perceptual: 7.3727)\n",
      "Epoch [49/50], Batch [230/250] | D Loss: 0.5751 | G Loss: 12.0978 (GAN: 0.8368, L1: 3.8884, Perceptual: 7.3727)\n",
      "Epoch [49/50], Batch [240/250] | D Loss: 0.6081 | G Loss: 12.8760 (GAN: 0.6749, L1: 4.1077, Perceptual: 8.0934)\n",
      "Epoch [49/50], Batch [240/250] | D Loss: 0.6081 | G Loss: 12.8760 (GAN: 0.6749, L1: 4.1077, Perceptual: 8.0934)\n",
      "Epoch [49/50], Batch [250/250] | D Loss: 0.4502 | G Loss: 11.8589 (GAN: 0.8487, L1: 3.5170, Perceptual: 7.4933)\n",
      "Epoch 49 completed. Current LR G: 0.000010, LR D: 0.000005\n",
      "Epoch [49/50], Batch [250/250] | D Loss: 0.4502 | G Loss: 11.8589 (GAN: 0.8487, L1: 3.5170, Perceptual: 7.4933)\n",
      "Epoch 49 completed. Current LR G: 0.000010, LR D: 0.000005\n",
      "Epoch [50/50], Batch [10/250] | D Loss: 0.6229 | G Loss: 11.1598 (GAN: 0.6759, L1: 3.5687, Perceptual: 6.9152)\n",
      "Epoch [50/50], Batch [10/250] | D Loss: 0.6229 | G Loss: 11.1598 (GAN: 0.6759, L1: 3.5687, Perceptual: 6.9152)\n",
      "Epoch [50/50], Batch [20/250] | D Loss: 0.5992 | G Loss: 11.6960 (GAN: 0.7519, L1: 3.5404, Perceptual: 7.4037)\n",
      "Epoch [50/50], Batch [20/250] | D Loss: 0.5992 | G Loss: 11.6960 (GAN: 0.7519, L1: 3.5404, Perceptual: 7.4037)\n",
      "Epoch [50/50], Batch [30/250] | D Loss: 0.6611 | G Loss: 10.6909 (GAN: 1.0034, L1: 2.7170, Perceptual: 6.9705)\n",
      "Epoch [50/50], Batch [30/250] | D Loss: 0.6611 | G Loss: 10.6909 (GAN: 1.0034, L1: 2.7170, Perceptual: 6.9705)\n",
      "Epoch [50/50], Batch [40/250] | D Loss: 0.5678 | G Loss: 10.7027 (GAN: 0.9014, L1: 2.7325, Perceptual: 7.0689)\n",
      "Epoch [50/50], Batch [40/250] | D Loss: 0.5678 | G Loss: 10.7027 (GAN: 0.9014, L1: 2.7325, Perceptual: 7.0689)\n",
      "Epoch [50/50], Batch [50/250] | D Loss: 0.5716 | G Loss: 11.6295 (GAN: 0.8790, L1: 3.4582, Perceptual: 7.2923)\n",
      "Epoch [50/50], Batch [50/250] | D Loss: 0.5716 | G Loss: 11.6295 (GAN: 0.8790, L1: 3.4582, Perceptual: 7.2923)\n",
      "Epoch [50/50], Batch [60/250] | D Loss: 0.5920 | G Loss: 11.3441 (GAN: 0.8836, L1: 3.3830, Perceptual: 7.0775)\n",
      "Epoch [50/50], Batch [60/250] | D Loss: 0.5920 | G Loss: 11.3441 (GAN: 0.8836, L1: 3.3830, Perceptual: 7.0775)\n",
      "Epoch [50/50], Batch [70/250] | D Loss: 0.6462 | G Loss: 10.7627 (GAN: 0.8982, L1: 3.0729, Perceptual: 6.7916)\n",
      "Epoch [50/50], Batch [70/250] | D Loss: 0.6462 | G Loss: 10.7627 (GAN: 0.8982, L1: 3.0729, Perceptual: 6.7916)\n",
      "Epoch [50/50], Batch [80/250] | D Loss: 0.6638 | G Loss: 11.5689 (GAN: 0.8556, L1: 3.5256, Perceptual: 7.1877)\n",
      "Epoch [50/50], Batch [80/250] | D Loss: 0.6638 | G Loss: 11.5689 (GAN: 0.8556, L1: 3.5256, Perceptual: 7.1877)\n",
      "Epoch [50/50], Batch [90/250] | D Loss: 0.5634 | G Loss: 11.6071 (GAN: 0.8609, L1: 3.4830, Perceptual: 7.2632)\n",
      "Epoch [50/50], Batch [90/250] | D Loss: 0.5634 | G Loss: 11.6071 (GAN: 0.8609, L1: 3.4830, Perceptual: 7.2632)\n",
      "Epoch [50/50], Batch [100/250] | D Loss: 0.5170 | G Loss: 12.9410 (GAN: 0.8270, L1: 4.0353, Perceptual: 8.0787)\n",
      "Epoch [50/50], Batch [100/250] | D Loss: 0.5170 | G Loss: 12.9410 (GAN: 0.8270, L1: 4.0353, Perceptual: 8.0787)\n",
      "Epoch [50/50], Batch [110/250] | D Loss: 0.5493 | G Loss: 12.1865 (GAN: 0.8671, L1: 3.5073, Perceptual: 7.8121)\n",
      "Epoch [50/50], Batch [110/250] | D Loss: 0.5493 | G Loss: 12.1865 (GAN: 0.8671, L1: 3.5073, Perceptual: 7.8121)\n",
      "Epoch [50/50], Batch [120/250] | D Loss: 0.6269 | G Loss: 12.3075 (GAN: 0.7533, L1: 3.9881, Perceptual: 7.5661)\n",
      "Epoch [50/50], Batch [120/250] | D Loss: 0.6269 | G Loss: 12.3075 (GAN: 0.7533, L1: 3.9881, Perceptual: 7.5661)\n",
      "Epoch [50/50], Batch [130/250] | D Loss: 0.7655 | G Loss: 10.5768 (GAN: 0.9027, L1: 3.0178, Perceptual: 6.6563)\n",
      "Epoch [50/50], Batch [130/250] | D Loss: 0.7655 | G Loss: 10.5768 (GAN: 0.9027, L1: 3.0178, Perceptual: 6.6563)\n",
      "Epoch [50/50], Batch [140/250] | D Loss: 0.5077 | G Loss: 11.3413 (GAN: 0.9109, L1: 3.3099, Perceptual: 7.1206)\n",
      "Epoch [50/50], Batch [140/250] | D Loss: 0.5077 | G Loss: 11.3413 (GAN: 0.9109, L1: 3.3099, Perceptual: 7.1206)\n",
      "Epoch [50/50], Batch [150/250] | D Loss: 0.6470 | G Loss: 11.7930 (GAN: 0.8538, L1: 3.7606, Perceptual: 7.1786)\n",
      "Epoch [50/50], Batch [150/250] | D Loss: 0.6470 | G Loss: 11.7930 (GAN: 0.8538, L1: 3.7606, Perceptual: 7.1786)\n",
      "Epoch [50/50], Batch [160/250] | D Loss: 0.5841 | G Loss: 11.9666 (GAN: 0.9197, L1: 3.4560, Perceptual: 7.5908)\n",
      "Epoch [50/50], Batch [160/250] | D Loss: 0.5841 | G Loss: 11.9666 (GAN: 0.9197, L1: 3.4560, Perceptual: 7.5908)\n",
      "Epoch [50/50], Batch [170/250] | D Loss: 0.5828 | G Loss: 11.6499 (GAN: 0.7809, L1: 3.5407, Perceptual: 7.3283)\n",
      "Epoch [50/50], Batch [170/250] | D Loss: 0.5828 | G Loss: 11.6499 (GAN: 0.7809, L1: 3.5407, Perceptual: 7.3283)\n",
      "Epoch [50/50], Batch [180/250] | D Loss: 0.6043 | G Loss: 11.7732 (GAN: 0.9336, L1: 3.2792, Perceptual: 7.5604)\n",
      "Epoch [50/50], Batch [180/250] | D Loss: 0.6043 | G Loss: 11.7732 (GAN: 0.9336, L1: 3.2792, Perceptual: 7.5604)\n",
      "Epoch [50/50], Batch [190/250] | D Loss: 0.6032 | G Loss: 11.5723 (GAN: 0.8358, L1: 3.4348, Perceptual: 7.3017)\n",
      "Epoch [50/50], Batch [190/250] | D Loss: 0.6032 | G Loss: 11.5723 (GAN: 0.8358, L1: 3.4348, Perceptual: 7.3017)\n",
      "Epoch [50/50], Batch [200/250] | D Loss: 0.6041 | G Loss: 12.3776 (GAN: 0.8710, L1: 3.7172, Perceptual: 7.7893)\n",
      "Epoch [50/50], Batch [200/250] | D Loss: 0.6041 | G Loss: 12.3776 (GAN: 0.8710, L1: 3.7172, Perceptual: 7.7893)\n",
      "Epoch [50/50], Batch [210/250] | D Loss: 0.6689 | G Loss: 12.1459 (GAN: 0.9075, L1: 3.4263, Perceptual: 7.8121)\n",
      "Epoch [50/50], Batch [210/250] | D Loss: 0.6689 | G Loss: 12.1459 (GAN: 0.9075, L1: 3.4263, Perceptual: 7.8121)\n",
      "Epoch [50/50], Batch [220/250] | D Loss: 0.6465 | G Loss: 12.4090 (GAN: 0.6884, L1: 3.6731, Perceptual: 8.0475)\n",
      "Epoch [50/50], Batch [220/250] | D Loss: 0.6465 | G Loss: 12.4090 (GAN: 0.6884, L1: 3.6731, Perceptual: 8.0475)\n",
      "Epoch [50/50], Batch [230/250] | D Loss: 0.6450 | G Loss: 11.0556 (GAN: 0.7544, L1: 3.3154, Perceptual: 6.9858)\n",
      "Epoch [50/50], Batch [230/250] | D Loss: 0.6450 | G Loss: 11.0556 (GAN: 0.7544, L1: 3.3154, Perceptual: 6.9858)\n",
      "Epoch [50/50], Batch [240/250] | D Loss: 0.7319 | G Loss: 11.6429 (GAN: 0.7932, L1: 3.6450, Perceptual: 7.2047)\n",
      "Epoch [50/50], Batch [240/250] | D Loss: 0.7319 | G Loss: 11.6429 (GAN: 0.7932, L1: 3.6450, Perceptual: 7.2047)\n",
      "Epoch [50/50], Batch [250/250] | D Loss: 0.5924 | G Loss: 13.0886 (GAN: 0.7999, L1: 4.0336, Perceptual: 8.2551)\n",
      "Epoch 50 completed. Current LR G: 0.000000, LR D: 0.000000\n",
      "--- Entering save/checkpoint block for epoch 50 ---\n",
      "Saving samples and checkpoint for epoch 50...\n",
      "Epoch [50/50], Batch [250/250] | D Loss: 0.5924 | G Loss: 13.0886 (GAN: 0.7999, L1: 4.0336, Perceptual: 8.2551)\n",
      "Epoch 50 completed. Current LR G: 0.000000, LR D: 0.000000\n",
      "--- Entering save/checkpoint block for epoch 50 ---\n",
      "Saving samples and checkpoint for epoch 50...\n",
      "Successfully saved image to: output/generated_images\\epoch_50_batch_250.png\n",
      "Successfully saved image to: output/generated_images\\epoch_50_batch_250.png\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Training complete.\n",
      "\n",
      "Starting Evaluation...\n",
      "Models saved to output/generated_images\\checkpoints\n",
      "Training complete.\n",
      "\n",
      "Starting Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATHARVA KHOLLAM\\AppData\\Local\\Temp\\ipykernel_21376\\1254336353.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(generator_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with 1000 images from kvasir-seg\\Kvasir-SEG\\images\n",
      "Eval Batch [10/250] | Avg PSNR: 28.70 | Avg SSIM: 0.7827\n",
      "Eval Batch [10/250] | Avg PSNR: 28.70 | Avg SSIM: 0.7827\n",
      "Eval Batch [20/250] | Avg PSNR: 28.77 | Avg SSIM: 0.7851\n",
      "Eval Batch [20/250] | Avg PSNR: 28.77 | Avg SSIM: 0.7851\n",
      "Eval Batch [30/250] | Avg PSNR: 28.66 | Avg SSIM: 0.7854\n",
      "Eval Batch [30/250] | Avg PSNR: 28.66 | Avg SSIM: 0.7854\n",
      "Eval Batch [40/250] | Avg PSNR: 28.64 | Avg SSIM: 0.7848\n",
      "Eval Batch [40/250] | Avg PSNR: 28.64 | Avg SSIM: 0.7848\n",
      "Eval Batch [50/250] | Avg PSNR: 28.73 | Avg SSIM: 0.7854\n",
      "Eval Batch [50/250] | Avg PSNR: 28.73 | Avg SSIM: 0.7854\n",
      "Eval Batch [60/250] | Avg PSNR: 28.78 | Avg SSIM: 0.7859\n",
      "Eval Batch [60/250] | Avg PSNR: 28.78 | Avg SSIM: 0.7859\n",
      "Eval Batch [70/250] | Avg PSNR: 28.73 | Avg SSIM: 0.7825\n",
      "Eval Batch [70/250] | Avg PSNR: 28.73 | Avg SSIM: 0.7825\n",
      "Eval Batch [80/250] | Avg PSNR: 28.76 | Avg SSIM: 0.7840\n",
      "Eval Batch [80/250] | Avg PSNR: 28.76 | Avg SSIM: 0.7840\n",
      "Eval Batch [90/250] | Avg PSNR: 28.77 | Avg SSIM: 0.7842\n",
      "Eval Batch [90/250] | Avg PSNR: 28.77 | Avg SSIM: 0.7842\n",
      "Eval Batch [100/250] | Avg PSNR: 28.83 | Avg SSIM: 0.7856\n",
      "Eval Batch [100/250] | Avg PSNR: 28.83 | Avg SSIM: 0.7856\n",
      "Eval Batch [110/250] | Avg PSNR: 28.88 | Avg SSIM: 0.7869\n",
      "Eval Batch [110/250] | Avg PSNR: 28.88 | Avg SSIM: 0.7869\n",
      "Eval Batch [120/250] | Avg PSNR: 28.88 | Avg SSIM: 0.7862\n",
      "Eval Batch [120/250] | Avg PSNR: 28.88 | Avg SSIM: 0.7862\n",
      "Eval Batch [130/250] | Avg PSNR: 28.91 | Avg SSIM: 0.7877\n",
      "Eval Batch [130/250] | Avg PSNR: 28.91 | Avg SSIM: 0.7877\n",
      "Eval Batch [140/250] | Avg PSNR: 28.90 | Avg SSIM: 0.7879\n",
      "Eval Batch [140/250] | Avg PSNR: 28.90 | Avg SSIM: 0.7879\n",
      "Eval Batch [150/250] | Avg PSNR: 28.90 | Avg SSIM: 0.7881\n",
      "Eval Batch [150/250] | Avg PSNR: 28.90 | Avg SSIM: 0.7881\n",
      "Eval Batch [160/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7880\n",
      "Eval Batch [160/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7880\n",
      "Eval Batch [170/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7883\n",
      "Eval Batch [170/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7883\n",
      "Eval Batch [180/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7886\n",
      "Eval Batch [180/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7886\n",
      "Eval Batch [190/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7889\n",
      "Eval Batch [190/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7889\n",
      "Eval Batch [200/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7883\n",
      "Eval Batch [200/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7883\n",
      "Eval Batch [210/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7877\n",
      "Eval Batch [210/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7877\n",
      "Eval Batch [220/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7878\n",
      "Eval Batch [220/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7878\n",
      "Eval Batch [230/250] | Avg PSNR: 28.94 | Avg SSIM: 0.7881\n",
      "Eval Batch [230/250] | Avg PSNR: 28.94 | Avg SSIM: 0.7881\n",
      "Eval Batch [240/250] | Avg PSNR: 28.94 | Avg SSIM: 0.7882\n",
      "Eval Batch [240/250] | Avg PSNR: 28.94 | Avg SSIM: 0.7882\n",
      "Eval Batch [250/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7875\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Average PSNR: 28.92\n",
      "Average SSIM: 0.7875\n",
      "Evaluation complete.\n",
      "Eval Batch [250/250] | Avg PSNR: 28.92 | Avg SSIM: 0.7875\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Average PSNR: 28.92\n",
      "Average SSIM: 0.7875\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(Config.DATA_ROOT):\n",
    "        print(f\"Error: Data root directory not found at {Config.DATA_ROOT}\")\n",
    "        print(\"Please download the Kvasir-SEG dataset and extract it such that\")\n",
    "        print(f\"the image files are located under '{Config.DATA_ROOT}'\")\n",
    "        print(\"For example, if you download 'Kvasir-SEG.zip', extract it,\")\n",
    "        print(\"you should have a structure like: kvasirseg/Kvasir-SEG/images/image_00001.jpg\")\n",
    "        exit()\n",
    "\n",
    "    if os.path.exists(Config.OUTPUT_DIR):\n",
    "        print(f\"Clearing existing output directory: {Config.OUTPUT_DIR}\")\n",
    "        shutil.rmtree(Config.OUTPUT_DIR)\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Output directory '{Config.OUTPUT_DIR}' created and ready.\")\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    train_model()\n",
    "\n",
    "    last_generator_checkpoint = os.path.join(Config.CHECKPOINT_DIR, f'generator_epoch_{Config.NUM_EPOCHS}.pth')\n",
    "    if os.path.exists(last_generator_checkpoint):\n",
    "        evaluate_model(last_generator_checkpoint)\n",
    "    else:\n",
    "        print(f\"Could not find the last generator checkpoint at {last_generator_checkpoint} for evaluation.\")\n",
    "        print(\"Ensure training completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T11:06:32.354202Z",
     "iopub.status.busy": "2025-06-05T11:06:32.353963Z",
     "iopub.status.idle": "2025-06-05T11:06:32.361172Z",
     "shell.execute_reply": "2025-06-05T11:06:32.360475Z",
     "shell.execute_reply.started": "2025-06-05T11:06:32.354178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking images in: data/Kvasir-SEG/images\n",
      "Error: The specified data root directory does NOT exist: data/Kvasir-SEG/images\n",
      "Please ensure your Kvasir-SEG dataset is correctly extracted and placed.\n",
      "Expected structure: data/Kvasir-SEG/images/\n",
      "\n",
      "To set up the dataset:\n",
      "1. Download the Kvasir-SEG dataset\n",
      "2. Extract it to create a 'data' folder in your project directory\n",
      "3. Ensure the structure is: data/Kvasir-SEG/images/image_*.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    DATA_ROOT = 'data/Kvasir-SEG/images'  # Local path to the dataset\n",
    "\n",
    "print(f\"Checking images in: {Config.DATA_ROOT}\")\n",
    "\n",
    "if not os.path.exists(Config.DATA_ROOT):\n",
    "    print(f\"Error: The specified data root directory does NOT exist: {Config.DATA_ROOT}\")\n",
    "    print(\"Please ensure your Kvasir-SEG dataset is correctly extracted and placed.\")\n",
    "    print(\"Expected structure: data/Kvasir-SEG/images/\")\n",
    "    print(\"\\nTo set up the dataset:\")\n",
    "    print(\"1. Download the Kvasir-SEG dataset\")\n",
    "    print(\"2. Extract it to create a 'data' folder in your project directory\")\n",
    "    print(\"3. Ensure the structure is: data/Kvasir-SEG/images/image_*.jpg\")\n",
    "else:\n",
    "    image_files = [f for f in os.listdir(Config.DATA_ROOT) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if len(image_files) == 0:\n",
    "        print(f\"Warning: No image files (jpg, jpeg, png) found in {Config.DATA_ROOT}.\")\n",
    "        print(\"Please verify the contents of this directory.\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} image files. Here are the first 5:\")\n",
    "        for i, filename in enumerate(image_files[:5]):\n",
    "            print(os.path.join(Config.DATA_ROOT, filename))\n",
    "        print(\"Dataset path appears correct and images are found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1316959,
     "sourceId": 7681479,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
