{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7681479,"sourceType":"datasetVersion","datasetId":1316959}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import vgg19\nimport os\nfrom PIL import Image\nimport numpy as np\nimport random\nfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\nimport matplotlib.pyplot as plt\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:47.871221Z","iopub.execute_input":"2025-06-05T08:30:47.871735Z","iopub.status.idle":"2025-06-05T08:30:53.495744Z","shell.execute_reply.started":"2025-06-05T08:30:47.871710Z","shell.execute_reply":"2025-06-05T08:30:53.495155Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.496916Z","iopub.execute_input":"2025-06-05T08:30:53.497312Z","iopub.status.idle":"2025-06-05T08:30:53.584816Z","shell.execute_reply.started":"2025-06-05T08:30:53.497291Z","shell.execute_reply":"2025-06-05T08:30:53.584028Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Config:\n    DATA_ROOT = '/kaggle/input/kvasirseg/Kvasir-SEG/Kvasir-SEG/images'\n    OUTPUT_DIR = '/kaggle/working/generated_images'\n    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n\n    IMAGE_SIZE = 256\n    NUM_CHANNELS = 3\n\n    BATCH_SIZE = 4\n    NUM_EPOCHS = 50\n    LEARNING_RATE_G = 2e-4\n    LEARNING_RATE_D = 1e-4\n    BETA1 = 0.5\n    LAMBDA_L1 = 70.0\n    LAMBDA_PERCEPTUAL = 10.0\n\n   \n    G_TRAIN_MULTIPLIER = 3 \n\n    NOISE_STD_DEV = 0.15\n    NOISE_STD_DEV_VARIATION = 0.05\n    \n    SALT_VS_PEPPER_RATIO = 0.5\n    SP_NOISE_PROB = 0.01\n    \n    BLUR_KERNEL_SIZE = 5\n    BRIGHTNESS_FACTOR = 0.2\n    CONTRAST_FACTOR = 0.2\n    SATURATION_FACTOR = 0.2\n    HUE_FACTOR = 0.05\n\n    \n    LR_DECAY_START_EPOCH = 30 \n    SAVE_EVERY_N_EPOCHS = 5\n    LOG_EVERY_N_BATCHES = 10\n\n    DEVICE = DEVICE\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.585771Z","iopub.execute_input":"2025-06-05T08:30:53.586056Z","iopub.status.idle":"2025-06-05T08:30:53.597077Z","shell.execute_reply.started":"2025-06-05T08:30:53.586031Z","shell.execute_reply":"2025-06-05T08:30:53.596396Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def apply_degradation(image_tensor, config):\n    image_tensor_cpu = image_tensor.cpu()\n    degraded_tensor = image_tensor_cpu.clone() \n\n    current_noise_std_dev = config.NOISE_STD_DEV + random.uniform(-config.NOISE_STD_DEV_VARIATION, config.NOISE_STD_DEV_VARIATION)\n    current_noise_std_dev = max(0, current_noise_std_dev)\n\n    gaussian_noise = torch.randn_like(degraded_tensor) * current_noise_std_dev\n    degraded_tensor = degraded_tensor + gaussian_noise\n    degraded_tensor = torch.clamp(degraded_tensor, 0, 1)\n\n\n    num_pixels = degraded_tensor.numel()\n    num_sp_noise_pixels = int(num_pixels * config.SP_NOISE_PROB)\n\n    noise_indices = torch.randperm(num_pixels)[:num_sp_noise_pixels]\n\n    salt_mask = torch.rand(num_sp_noise_pixels) < config.SALT_VS_PEPPER_RATIO\n    pepper_mask = ~salt_mask\n\n    degraded_tensor_flat = degraded_tensor.view(-1)\n\n    # Apply pepper noise (set to 0)\n    degraded_tensor_flat[noise_indices[pepper_mask]] = 0.0\n\n    # Apply salt noise (set to 1)\n    degraded_tensor_flat[noise_indices[salt_mask]] = 1.0\n    \n    # Reshape back to original dimensions\n    degraded_tensor = degraded_tensor_flat.view(degraded_tensor.shape)\n\n\n    if config.BLUR_KERNEL_SIZE > 0:\n        blur_transform = transforms.GaussianBlur(\n            kernel_size=(config.BLUR_KERNEL_SIZE, config.BLUR_KERNEL_SIZE),\n            sigma=(0.1, 2.0)\n        )\n        degraded_tensor = blur_transform(degraded_tensor)\n\n    jitter_transform = transforms.ColorJitter(\n        brightness=config.BRIGHTNESS_FACTOR,\n        contrast=config.CONTRAST_FACTOR,\n        saturation=config.SATURATION_FACTOR,\n        hue=config.HUE_FACTOR\n    )\n    degraded_tensor = jitter_transform(degraded_tensor)\n\n    return degraded_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.597940Z","iopub.execute_input":"2025-06-05T08:30:53.598211Z","iopub.status.idle":"2025-06-05T08:30:53.609662Z","shell.execute_reply.started":"2025-06-05T08:30:53.598187Z","shell.execute_reply":"2025-06-05T08:30:53.609022Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def save_images(low_res, real_high_res, fake_high_res, epoch, batch_idx, output_dir):\n    low_res_np = (low_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n    real_high_res_np = (real_high_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n    fake_high_res_np = (fake_high_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n\n    fig, axes = plt.subplots(Config.BATCH_SIZE, 3, figsize=(9, 3 * Config.BATCH_SIZE))\n    fig.suptitle(f'Epoch {epoch}, Batch {batch_idx}', fontsize=16)\n\n    for i in range(Config.BATCH_SIZE):\n        axes[i, 0].imshow(low_res_np[i])\n        axes[i, 0].set_title('Low-Quality Input')\n        axes[i, 0].axis('off')\n\n        axes[i, 1].imshow(real_high_res_np[i])\n        axes[i, 1].set_title('Real High-Quality')\n        axes[i, 1].axis('off')\n\n        axes[i, 2].imshow(fake_high_res_np[i])\n        axes[i, 2].set_title('Generated High-Quality')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    \n    filename = os.path.join(output_dir, f'epoch_{epoch}_batch_{batch_idx}.png')\n    \n    try:\n        plt.savefig(filename)\n        print(f\"Successfully saved image to: {filename}\")\n    except Exception as e:\n        print(f\"Error saving image {filename}: {e}\")\n    finally:\n        plt.close(fig)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.611397Z","iopub.execute_input":"2025-06-05T08:30:53.611669Z","iopub.status.idle":"2025-06-05T08:30:53.624904Z","shell.execute_reply.started":"2025-06-05T08:30:53.611651Z","shell.execute_reply":"2025-06-05T08:30:53.624298Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def calculate_metrics(real_images, fake_images):\n    real_images_np = (real_images * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n    fake_images_np = (fake_images * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n\n    psnr_scores = []\n    ssim_scores = []\n\n    for i in range(real_images_np.shape[0]):\n        psnr = peak_signal_noise_ratio(real_images_np[i], fake_images_np[i], data_range=1.0)\n        ssim = structural_similarity(real_images_np[i], fake_images_np[i], data_range=1.0, channel_axis=-1)\n        psnr_scores.append(psnr)\n        ssim_scores.append(ssim)\n    \n    return np.mean(psnr_scores), np.mean(ssim_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.625639Z","iopub.execute_input":"2025-06-05T08:30:53.625880Z","iopub.status.idle":"2025-06-05T08:30:53.636995Z","shell.execute_reply.started":"2025-06-05T08:30:53.625863Z","shell.execute_reply":"2025-06-05T08:30:53.636402Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class KvasirSEGDataset(Dataset):\n    def __init__(self, root_dir, image_size, transform=None, degradation_transform=None):\n        self.root_dir = root_dir\n        self.image_size = image_size\n        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n        self.transform = transform\n        self.degradation_transform = degradation_transform\n\n        if self.transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((self.image_size, self.image_size)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n            ])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.root_dir, img_name)\n        \n        image = Image.open(img_path).convert('RGB')\n        high_quality_image = self.transform(image)\n\n        high_quality_image_0_1 = (high_quality_image * 0.5 + 0.5)\n        \n        if self.degradation_transform:\n            low_quality_image_0_1 = self.degradation_transform(high_quality_image_0_1)\n        else:\n            low_quality_image_0_1 = high_quality_image_0_1\n\n        low_quality_image = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])(low_quality_image_0_1)\n\n        return low_quality_image, high_quality_image\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.637646Z","iopub.execute_input":"2025-06-05T08:30:53.637827Z","iopub.status.idle":"2025-06-05T08:30:53.646914Z","shell.execute_reply.started":"2025-06-05T08:30:53.637812Z","shell.execute_reply":"2025-06-05T08:30:53.646268Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class UNetDown(nn.Module):\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n        super().__init__()\n        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0):\n        super().__init__()\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.647820Z","iopub.execute_input":"2025-06-05T08:30:53.648267Z","iopub.status.idle":"2025-06-05T08:30:53.663750Z","shell.execute_reply.started":"2025-06-05T08:30:53.648238Z","shell.execute_reply":"2025-06-05T08:30:53.663066Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels * 2, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(256, 512, 4, 1, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(512, 1, 4, 1, 1, bias=False)\n        )\n\n    def forward(self, img_A, img_B):\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.664496Z","iopub.execute_input":"2025-06-05T08:30:53.664800Z","iopub.status.idle":"2025-06-05T08:30:53.678162Z","shell.execute_reply.started":"2025-06-05T08:30:53.664778Z","shell.execute_reply":"2025-06-05T08:30:53.677497Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class PerceptualLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg = vgg19(pretrained=True).features\n        for param in vgg.parameters():\n            param.requires_grad = False\n        \n        self.features = nn.Sequential(\n            vgg[:2],\n            vgg[2:7],\n            vgg[7:12],\n            vgg[12:21]\n        ).to(DEVICE).eval()\n\n        self.criterion = nn.L1Loss()\n\n    def forward(self, fake_img, real_img):\n        normalize_vgg = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n        fake_img_norm = normalize_vgg((fake_img * 0.5 + 0.5))\n        real_img_norm = normalize_vgg((real_img * 0.5 + 0.5))\n\n        fake_features = self.features(fake_img_norm)\n        real_features = self.features(real_img_norm)\n        \n        return self.criterion(fake_features, real_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.678814Z","iopub.execute_input":"2025-06-05T08:30:53.679039Z","iopub.status.idle":"2025-06-05T08:30:53.686805Z","shell.execute_reply.started":"2025-06-05T08:30:53.679023Z","shell.execute_reply":"2025-06-05T08:30:53.686170Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_model():\n    print(f\"Using device: {DEVICE}\")\n\n    dataset = KvasirSEGDataset(\n        root_dir=Config.DATA_ROOT,\n        image_size=Config.IMAGE_SIZE,\n        degradation_transform=lambda img_tensor: apply_degradation(img_tensor, Config)\n    )\n    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n\n    if len(dataloader) == 0:\n        print(f\"Error: Dataloader is empty. No images found in {Config.DATA_ROOT} or batch size is too large for the dataset.\")\n        print(\"Please ensure the Kvasir-SEG dataset is correctly placed and contains images.\")\n        return # Exit if no data is found\n\n    generator = UnetGenerator(Config.NUM_CHANNELS, Config.NUM_CHANNELS).to(DEVICE)\n    discriminator = Discriminator(Config.NUM_CHANNELS).to(DEVICE)\n\n    generator.apply(weights_init)\n    discriminator.apply(weights_init)\n\n    criterion_GAN = nn.BCEWithLogitsLoss()\n    criterion_L1 = nn.L1Loss()\n    perceptual_loss_fn = PerceptualLoss()\n\n    optimizer_G = optim.Adam(generator.parameters(), lr=Config.LEARNING_RATE_G, betas=(Config.BETA1, 0.999))\n    optimizer_D = optim.Adam(discriminator.parameters(), lr=Config.LEARNING_RATE_D, betas=(Config.BETA1, 0.999))\n\n    def lambda_rule(epoch):\n        if epoch < Config.LR_DECAY_START_EPOCH:\n            return 1.0\n        else:\n            return 1.0 - (epoch - Config.LR_DECAY_START_EPOCH) / (Config.NUM_EPOCHS - Config.LR_DECAY_START_EPOCH)\n\n    scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n    scheduler_D = optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=lambda_rule)\n\n    print(\"Starting Training Loop...\")\n    for epoch in range(Config.NUM_EPOCHS):\n        for i, (low_res_img, high_res_img) in enumerate(dataloader):\n            low_res_img = low_res_img.to(DEVICE)\n            high_res_img = high_res_img.to(DEVICE)\n\n\n            optimizer_D.zero_grad()\n\n            real_labels = torch.ones(low_res_img.size(0), 1, 30, 30, device=DEVICE)\n            fake_high_res_for_D = generator(low_res_img)\n            \n            output_real = discriminator(low_res_img, high_res_img)\n            loss_D_real = criterion_GAN(output_real, real_labels)\n\n            fake_labels = torch.zeros(low_res_img.size(0), 1, 30, 30, device=DEVICE)\n            output_fake = discriminator(low_res_img, fake_high_res_for_D.detach())\n            loss_D_fake = criterion_GAN(output_fake, fake_labels)\n\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n            loss_D.backward()\n            optimizer_D.step()\n\n\n            for _ in range(Config.G_TRAIN_MULTIPLIER):\n                optimizer_G.zero_grad()\n                \n                fake_high_res_img = generator(low_res_img)\n\n                output_fake_for_G = discriminator(low_res_img, fake_high_res_img)\n                loss_G_GAN = criterion_GAN(output_fake_for_G, real_labels)\n\n                loss_G_L1 = criterion_L1(fake_high_res_img, high_res_img) * Config.LAMBDA_L1\n\n                loss_G_perceptual = perceptual_loss_fn(fake_high_res_img, high_res_img) * Config.LAMBDA_PERCEPTUAL\n\n                loss_G = loss_G_GAN + loss_G_L1 + loss_G_perceptual\n                loss_G.backward() \n                optimizer_G.step()\n\n            if (i + 1) % Config.LOG_EVERY_N_BATCHES == 0:\n                print(f\"Epoch [{epoch+1}/{Config.NUM_EPOCHS}], Batch [{i+1}/{len(dataloader)}] | \"\n                      f\"D Loss: {loss_D.item():.4f} | G Loss: {loss_G.item():.4f} \"\n                      f\"(GAN: {loss_G_GAN.item():.4f}, L1: {loss_G_L1.item():.4f}, Perceptual: {loss_G_perceptual.item():.4f})\")\n        \n        scheduler_G.step()\n        scheduler_D.step()\n        print(f\"Epoch {epoch+1} completed. Current LR G: {optimizer_G.param_groups[0]['lr']:.6f}, LR D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n\n\n        if (epoch + 1) % Config.SAVE_EVERY_N_EPOCHS == 0 or (epoch + 1) == Config.NUM_EPOCHS:\n            print(f\"--- Entering save/checkpoint block for epoch {epoch+1} ---\")\n            print(f\"Saving samples and checkpoint for epoch {epoch+1}...\")\n            with torch.no_grad():\n                try:\n                    sample_low_res, sample_high_res = next(iter(dataloader))\n                    sample_low_res = sample_low_res.to(DEVICE)\n                    sample_high_res = sample_high_res.to(DEVICE)\n                    \n                    generated_high_res = generator(sample_low_res) \n                    save_images(sample_low_res, sample_high_res, generated_high_res, epoch + 1, i + 1, Config.OUTPUT_DIR)\n                except StopIteration:\n                    print(f\"Warning: Dataloader exhausted at epoch {epoch+1} for sample saving. Skipping image save for this epoch.\")\n\n\n            try:\n                torch.save(generator.state_dict(), os.path.join(Config.CHECKPOINT_DIR, f'generator_epoch_{epoch+1}.pth'))\n                torch.save(discriminator.state_dict(), os.path.join(Config.CHECKPOINT_DIR, f'discriminator_epoch_{epoch+1}.pth'))\n                print(f\"Models saved to {Config.CHECKPOINT_DIR}\")\n            except Exception as e:\n                print(f\"Error saving model checkpoints for epoch {epoch+1}: {e}\")\n\n    print(\"Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.687691Z","iopub.execute_input":"2025-06-05T08:30:53.688104Z","iopub.status.idle":"2025-06-05T08:30:53.704177Z","shell.execute_reply.started":"2025-06-05T08:30:53.688087Z","shell.execute_reply":"2025-06-05T08:30:53.703438Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class UnetGenerator(nn.Module):\n    \"\"\"\n    U-Net Generator architecture for image-to-image translation.\n    \"\"\"\n    def __init__(self, in_channels=3, out_channels=3):\n        super().__init__()\n        # Encoder\n        self.down1 = UNetDown(in_channels, 64, normalize=False) # 256 -> 128\n        self.down2 = UNetDown(64, 128) # 128 -> 64\n        self.down3 = UNetDown(128, 256) # 64 -> 32\n        self.down4 = UNetDown(256, 512, dropout=0.5) # 32 -> 16\n        self.down5 = UNetDown(512, 512, dropout=0.5) # 16 -> 8\n        self.down6 = UNetDown(512, 512, dropout=0.5) # 8 -> 4\n        self.down7 = UNetDown(512, 512, dropout=0.5) # 4 -> 2\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5) # 2 -> 1\n\n        # Decoder\n        self.up1 = UNetUp(512, 512, dropout=0.5) # 1 -> 2 (concat with down7)\n        self.up2 = UNetUp(1024, 512, dropout=0.5) # 2 -> 4 (concat with down6)\n        self.up3 = UNetUp(1024, 512, dropout=0.5) # 4 -> 8 (concat with down5)\n        self.up4 = UNetUp(1024, 512) # 8 -> 16 (concat with down4)\n        self.up5 = UNetUp(1024, 256) # 16 -> 32 (concat with down3)\n        self.up6 = UNetUp(512, 128) # 32 -> 64 (concat with down2)\n        self.up7 = UNetUp(256, 64) # 64 -> 128 (concat with down1)\n\n        self.final_conv = nn.Sequential(\n            nn.ConvTranspose2d(128, out_channels, 4, 2, 1), # 128 -> 256\n            nn.Tanh() # Output pixels in [-1, 1]\n        )\n\n    def forward(self, x):\n        # Encoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        # Decoder with skip connections\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        return self.final_conv(u7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.704905Z","iopub.execute_input":"2025-06-05T08:30:53.705118Z","iopub.status.idle":"2025-06-05T08:30:53.715757Z","shell.execute_reply.started":"2025-06-05T08:30:53.705102Z","shell.execute_reply":"2025-06-05T08:30:53.715029Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def evaluate_model(generator_path):\n    print(\"\\nStarting Evaluation...\")\n    generator = UnetGenerator(Config.NUM_CHANNELS, Config.NUM_CHANNELS).to(DEVICE)\n    generator.load_state_dict(torch.load(generator_path, map_location=DEVICE))\n    generator.eval()\n\n    dataset = KvasirSEGDataset(\n        root_dir=Config.DATA_ROOT,\n        image_size=Config.IMAGE_SIZE,\n        degradation_transform=lambda img_tensor: apply_degradation(img_tensor, Config)\n    )\n    eval_dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n    if len(eval_dataloader) == 0:\n        print(f\"Error: Evaluation Dataloader is empty. Cannot perform evaluation.\")\n        return\n\n    all_psnr = []\n    all_ssim = []\n\n    with torch.no_grad():\n        for i, (low_res_img, high_res_img) in enumerate(eval_dataloader):\n            low_res_img = low_res_img.to(DEVICE)\n            high_res_img = high_res_img.to(DEVICE)\n\n            fake_high_res_img = generator(low_res_img)\n            \n            psnr, ssim = calculate_metrics(high_res_img, fake_high_res_img)\n            all_psnr.append(psnr)\n            all_ssim.append(ssim)\n\n            if (i + 1) % Config.LOG_EVERY_N_BATCHES == 0:\n                print(f\"Eval Batch [{i+1}/{len(eval_dataloader)}] | Avg PSNR: {np.mean(all_psnr):.2f} | Avg SSIM: {np.mean(all_ssim):.4f}\")\n\n    avg_psnr = np.mean(all_psnr)\n    avg_ssim = np.mean(all_ssim)\n    print(f\"\\n--- Evaluation Results ---\")\n    print(f\"Average PSNR: {avg_psnr:.2f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    print(\"Evaluation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.716434Z","iopub.execute_input":"2025-06-05T08:30:53.716683Z","iopub.status.idle":"2025-06-05T08:30:53.727347Z","shell.execute_reply.started":"2025-06-05T08:30:53.716663Z","shell.execute_reply":"2025-06-05T08:30:53.726795Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    if not os.path.exists(Config.DATA_ROOT):\n        print(f\"Error: Data root directory not found at {Config.DATA_ROOT}\")\n        print(\"Please download the Kvasir-SEG dataset and extract it such that\")\n        print(f\"the image files are located under '{Config.DATA_ROOT}'\")\n        print(\"For example, if you download 'Kvasir-SEG.zip', extract it,\")\n        print(\"you should have a structure like: kvasirseg/Kvasir-SEG/images/image_00001.jpg\")\n        exit()\n\n    if os.path.exists(Config.OUTPUT_DIR):\n        print(f\"Clearing existing output directory: {Config.OUTPUT_DIR}\")\n        shutil.rmtree(Config.OUTPUT_DIR)\n    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n    print(f\"Output directory '{Config.OUTPUT_DIR}' created and ready.\")\n\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    train_model()\n\n    last_generator_checkpoint = os.path.join(Config.CHECKPOINT_DIR, f'generator_epoch_{Config.NUM_EPOCHS}.pth')\n    if os.path.exists(last_generator_checkpoint):\n        evaluate_model(last_generator_checkpoint)\n    else:\n        print(f\"Could not find the last generator checkpoint at {last_generator_checkpoint} for evaluation.\")\n        print(\"Ensure training completed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:30:53.729680Z","iopub.execute_input":"2025-06-05T08:30:53.730066Z","iopub.status.idle":"2025-06-05T11:06:32.352399Z","shell.execute_reply.started":"2025-06-05T08:30:53.730049Z","shell.execute_reply":"2025-06-05T11:06:32.351355Z"}},"outputs":[{"name":"stdout","text":"Output directory '/kaggle/working/generated_images' created and ready.\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 203MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Starting Training Loop...\nEpoch [1/50], Batch [10/250] | D Loss: 0.7123 | G Loss: 22.0349 (GAN: 0.7769, L1: 10.2461, Perceptual: 11.0120)\nEpoch [1/50], Batch [20/250] | D Loss: 0.6977 | G Loss: 19.8129 (GAN: 0.8883, L1: 7.8719, Perceptual: 11.0527)\nEpoch [1/50], Batch [30/250] | D Loss: 0.6994 | G Loss: 19.1088 (GAN: 0.7691, L1: 7.9440, Perceptual: 10.3957)\nEpoch [1/50], Batch [40/250] | D Loss: 0.6880 | G Loss: 17.8787 (GAN: 0.8234, L1: 7.0230, Perceptual: 10.0323)\nEpoch [1/50], Batch [50/250] | D Loss: 0.7077 | G Loss: 15.6969 (GAN: 0.7364, L1: 5.0688, Perceptual: 9.8917)\nEpoch [1/50], Batch [60/250] | D Loss: 0.6892 | G Loss: 15.0869 (GAN: 0.7043, L1: 4.9144, Perceptual: 9.4682)\nEpoch [1/50], Batch [70/250] | D Loss: 0.6875 | G Loss: 15.3069 (GAN: 0.7836, L1: 5.4285, Perceptual: 9.0948)\nEpoch [1/50], Batch [80/250] | D Loss: 0.6666 | G Loss: 14.4515 (GAN: 0.6726, L1: 4.8817, Perceptual: 8.8973)\nEpoch [1/50], Batch [90/250] | D Loss: 0.6751 | G Loss: 14.9006 (GAN: 0.7777, L1: 5.0084, Perceptual: 9.1145)\nEpoch [1/50], Batch [100/250] | D Loss: 0.7330 | G Loss: 13.8907 (GAN: 0.7252, L1: 4.4113, Perceptual: 8.7542)\nEpoch [1/50], Batch [110/250] | D Loss: 0.6856 | G Loss: 16.1290 (GAN: 0.7386, L1: 5.4506, Perceptual: 9.9399)\nEpoch [1/50], Batch [120/250] | D Loss: 0.7472 | G Loss: 14.2874 (GAN: 0.7779, L1: 4.7504, Perceptual: 8.7591)\nEpoch [1/50], Batch [130/250] | D Loss: 0.6425 | G Loss: 13.5595 (GAN: 0.7719, L1: 4.4736, Perceptual: 8.3140)\nEpoch [1/50], Batch [140/250] | D Loss: 0.6826 | G Loss: 14.5472 (GAN: 0.7979, L1: 5.1539, Perceptual: 8.5953)\nEpoch [1/50], Batch [150/250] | D Loss: 0.7347 | G Loss: 14.7298 (GAN: 0.7276, L1: 4.8036, Perceptual: 9.1986)\nEpoch [1/50], Batch [160/250] | D Loss: 0.6592 | G Loss: 14.0701 (GAN: 0.6935, L1: 4.6101, Perceptual: 8.7665)\nEpoch [1/50], Batch [170/250] | D Loss: 0.5872 | G Loss: 16.9198 (GAN: 0.7790, L1: 7.2622, Perceptual: 8.8786)\nEpoch [1/50], Batch [180/250] | D Loss: 0.6629 | G Loss: 15.7168 (GAN: 0.9069, L1: 5.5083, Perceptual: 9.3016)\nEpoch [1/50], Batch [190/250] | D Loss: 0.6313 | G Loss: 13.8559 (GAN: 0.8274, L1: 4.4039, Perceptual: 8.6246)\nEpoch [1/50], Batch [200/250] | D Loss: 0.6559 | G Loss: 14.3060 (GAN: 0.7274, L1: 5.0442, Perceptual: 8.5343)\nEpoch [1/50], Batch [210/250] | D Loss: 0.6964 | G Loss: 12.2866 (GAN: 0.6462, L1: 3.9913, Perceptual: 7.6492)\nEpoch [1/50], Batch [220/250] | D Loss: 0.6012 | G Loss: 13.9292 (GAN: 0.7602, L1: 4.6695, Perceptual: 8.4995)\nEpoch [1/50], Batch [230/250] | D Loss: 0.7339 | G Loss: 13.7485 (GAN: 0.7928, L1: 4.7259, Perceptual: 8.2298)\nEpoch [1/50], Batch [240/250] | D Loss: 0.6743 | G Loss: 12.9078 (GAN: 0.8515, L1: 4.1582, Perceptual: 7.8981)\nEpoch [1/50], Batch [250/250] | D Loss: 0.6704 | G Loss: 13.6796 (GAN: 0.8188, L1: 4.8953, Perceptual: 7.9655)\nEpoch 1 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [2/50], Batch [10/250] | D Loss: 0.4955 | G Loss: 16.2208 (GAN: 0.9331, L1: 5.9000, Perceptual: 9.3876)\nEpoch [2/50], Batch [20/250] | D Loss: 0.5856 | G Loss: 14.2070 (GAN: 0.8879, L1: 4.7930, Perceptual: 8.5261)\nEpoch [2/50], Batch [30/250] | D Loss: 0.8101 | G Loss: 12.5340 (GAN: 1.0076, L1: 4.2114, Perceptual: 7.3149)\nEpoch [2/50], Batch [40/250] | D Loss: 0.5952 | G Loss: 14.4310 (GAN: 0.8756, L1: 5.3686, Perceptual: 8.1869)\nEpoch [2/50], Batch [50/250] | D Loss: 0.8684 | G Loss: 13.5356 (GAN: 0.8242, L1: 4.6760, Perceptual: 8.0354)\nEpoch [2/50], Batch [60/250] | D Loss: 0.6789 | G Loss: 11.6884 (GAN: 0.7530, L1: 3.5235, Perceptual: 7.4118)\nEpoch [2/50], Batch [70/250] | D Loss: 0.7227 | G Loss: 13.2836 (GAN: 0.8671, L1: 4.0369, Perceptual: 8.3796)\nEpoch [2/50], Batch [80/250] | D Loss: 0.5361 | G Loss: 15.0389 (GAN: 0.8515, L1: 5.5256, Perceptual: 8.6618)\nEpoch [2/50], Batch [90/250] | D Loss: 0.6326 | G Loss: 13.5693 (GAN: 0.8784, L1: 4.3568, Perceptual: 8.3341)\nEpoch [2/50], Batch [100/250] | D Loss: 0.6199 | G Loss: 13.9180 (GAN: 0.9235, L1: 4.6753, Perceptual: 8.3191)\nEpoch [2/50], Batch [110/250] | D Loss: 0.5503 | G Loss: 13.4143 (GAN: 0.8102, L1: 4.3932, Perceptual: 8.2110)\nEpoch [2/50], Batch [120/250] | D Loss: 0.6267 | G Loss: 12.5090 (GAN: 0.8054, L1: 3.6730, Perceptual: 8.0306)\nEpoch [2/50], Batch [130/250] | D Loss: 0.6275 | G Loss: 13.0587 (GAN: 0.9430, L1: 4.2137, Perceptual: 7.9020)\nEpoch [2/50], Batch [140/250] | D Loss: 0.6316 | G Loss: 13.7649 (GAN: 0.7657, L1: 4.6171, Perceptual: 8.3821)\nEpoch [2/50], Batch [150/250] | D Loss: 0.6433 | G Loss: 12.1961 (GAN: 0.7282, L1: 3.6340, Perceptual: 7.8339)\nEpoch [2/50], Batch [160/250] | D Loss: 0.6326 | G Loss: 14.5889 (GAN: 0.9345, L1: 4.9207, Perceptual: 8.7337)\nEpoch [2/50], Batch [170/250] | D Loss: 0.6022 | G Loss: 14.2126 (GAN: 0.5709, L1: 4.9636, Perceptual: 8.6782)\nEpoch [2/50], Batch [180/250] | D Loss: 0.6247 | G Loss: 12.5770 (GAN: 0.6752, L1: 4.1348, Perceptual: 7.7671)\nEpoch [2/50], Batch [190/250] | D Loss: 0.5673 | G Loss: 14.2850 (GAN: 0.8415, L1: 4.8382, Perceptual: 8.6053)\nEpoch [2/50], Batch [200/250] | D Loss: 0.5230 | G Loss: 13.2572 (GAN: 0.5282, L1: 4.4930, Perceptual: 8.2361)\nEpoch [2/50], Batch [210/250] | D Loss: 0.7037 | G Loss: 14.9144 (GAN: 0.9542, L1: 4.7912, Perceptual: 9.1690)\nEpoch [2/50], Batch [220/250] | D Loss: 0.7002 | G Loss: 12.8306 (GAN: 0.8010, L1: 4.0449, Perceptual: 7.9848)\nEpoch [2/50], Batch [230/250] | D Loss: 0.7398 | G Loss: 13.7364 (GAN: 0.8021, L1: 4.4591, Perceptual: 8.4752)\nEpoch [2/50], Batch [240/250] | D Loss: 0.5517 | G Loss: 14.5105 (GAN: 0.9076, L1: 4.7149, Perceptual: 8.8880)\nEpoch [2/50], Batch [250/250] | D Loss: 0.6883 | G Loss: 11.6725 (GAN: 0.7966, L1: 3.6350, Perceptual: 7.2408)\nEpoch 2 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [3/50], Batch [10/250] | D Loss: 0.5242 | G Loss: 14.1164 (GAN: 1.0598, L1: 4.8055, Perceptual: 8.2511)\nEpoch [3/50], Batch [20/250] | D Loss: 0.5477 | G Loss: 14.3222 (GAN: 0.8872, L1: 5.2941, Perceptual: 8.1409)\nEpoch [3/50], Batch [30/250] | D Loss: 0.7159 | G Loss: 13.3913 (GAN: 0.6397, L1: 4.7618, Perceptual: 7.9898)\nEpoch [3/50], Batch [40/250] | D Loss: 0.6906 | G Loss: 13.0342 (GAN: 0.7019, L1: 4.0964, Perceptual: 8.2358)\nEpoch [3/50], Batch [50/250] | D Loss: 0.6285 | G Loss: 12.5621 (GAN: 1.0382, L1: 4.0010, Perceptual: 7.5228)\nEpoch [3/50], Batch [60/250] | D Loss: 0.6812 | G Loss: 12.5719 (GAN: 0.7900, L1: 3.9741, Perceptual: 7.8079)\nEpoch [3/50], Batch [70/250] | D Loss: 0.4876 | G Loss: 14.0921 (GAN: 0.9859, L1: 5.3070, Perceptual: 7.7992)\nEpoch [3/50], Batch [80/250] | D Loss: 0.6049 | G Loss: 14.7162 (GAN: 0.7082, L1: 5.1405, Perceptual: 8.8675)\nEpoch [3/50], Batch [90/250] | D Loss: 0.7098 | G Loss: 13.4736 (GAN: 0.7553, L1: 4.7797, Perceptual: 7.9386)\nEpoch [3/50], Batch [100/250] | D Loss: 0.5793 | G Loss: 12.2360 (GAN: 0.5816, L1: 3.9327, Perceptual: 7.7217)\nEpoch [3/50], Batch [110/250] | D Loss: 0.5751 | G Loss: 13.0932 (GAN: 0.8386, L1: 3.9437, Perceptual: 8.3109)\nEpoch [3/50], Batch [120/250] | D Loss: 0.5274 | G Loss: 13.4347 (GAN: 0.7883, L1: 4.6456, Perceptual: 8.0008)\nEpoch [3/50], Batch [130/250] | D Loss: 0.7537 | G Loss: 13.0936 (GAN: 0.9866, L1: 3.9769, Perceptual: 8.1302)\nEpoch [3/50], Batch [140/250] | D Loss: 0.5605 | G Loss: 14.9629 (GAN: 0.8952, L1: 5.0670, Perceptual: 9.0006)\nEpoch [3/50], Batch [150/250] | D Loss: 0.6604 | G Loss: 11.3047 (GAN: 0.7561, L1: 3.4273, Perceptual: 7.1214)\nEpoch [3/50], Batch [160/250] | D Loss: 0.8037 | G Loss: 12.6531 (GAN: 0.8082, L1: 3.7677, Perceptual: 8.0773)\nEpoch [3/50], Batch [170/250] | D Loss: 0.7001 | G Loss: 13.6513 (GAN: 0.8416, L1: 4.5687, Perceptual: 8.2411)\nEpoch [3/50], Batch [180/250] | D Loss: 0.5729 | G Loss: 12.9586 (GAN: 0.8268, L1: 4.4764, Perceptual: 7.6554)\nEpoch [3/50], Batch [190/250] | D Loss: 0.5452 | G Loss: 13.5707 (GAN: 0.6642, L1: 5.2863, Perceptual: 7.6203)\nEpoch [3/50], Batch [200/250] | D Loss: 0.7595 | G Loss: 13.5527 (GAN: 0.9494, L1: 4.4164, Perceptual: 8.1869)\nEpoch [3/50], Batch [210/250] | D Loss: 0.7006 | G Loss: 13.1531 (GAN: 0.6400, L1: 4.4886, Perceptual: 8.0245)\nEpoch [3/50], Batch [220/250] | D Loss: 0.6326 | G Loss: 13.5069 (GAN: 0.6582, L1: 4.1616, Perceptual: 8.6871)\nEpoch [3/50], Batch [230/250] | D Loss: 0.5314 | G Loss: 12.9751 (GAN: 0.6126, L1: 4.7597, Perceptual: 7.6028)\nEpoch [3/50], Batch [240/250] | D Loss: 0.6933 | G Loss: 13.1901 (GAN: 0.6089, L1: 4.3961, Perceptual: 8.1850)\nEpoch [3/50], Batch [250/250] | D Loss: 0.6377 | G Loss: 12.2982 (GAN: 0.7562, L1: 3.7984, Perceptual: 7.7437)\nEpoch 3 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [4/50], Batch [10/250] | D Loss: 0.5777 | G Loss: 12.4261 (GAN: 0.7766, L1: 4.0170, Perceptual: 7.6325)\nEpoch [4/50], Batch [20/250] | D Loss: 0.7345 | G Loss: 13.7124 (GAN: 0.9177, L1: 4.5700, Perceptual: 8.2247)\nEpoch [4/50], Batch [30/250] | D Loss: 0.5904 | G Loss: 13.6246 (GAN: 0.8228, L1: 4.4771, Perceptual: 8.3247)\nEpoch [4/50], Batch [40/250] | D Loss: 0.6371 | G Loss: 13.3685 (GAN: 0.9473, L1: 4.7667, Perceptual: 7.6544)\nEpoch [4/50], Batch [50/250] | D Loss: 0.5771 | G Loss: 12.8887 (GAN: 0.8916, L1: 4.0516, Perceptual: 7.9455)\nEpoch [4/50], Batch [60/250] | D Loss: 0.5729 | G Loss: 14.1003 (GAN: 0.7234, L1: 4.7884, Perceptual: 8.5885)\nEpoch [4/50], Batch [70/250] | D Loss: 0.6646 | G Loss: 12.8668 (GAN: 0.6876, L1: 4.4643, Perceptual: 7.7149)\nEpoch [4/50], Batch [80/250] | D Loss: 0.6460 | G Loss: 12.3633 (GAN: 0.7816, L1: 3.7196, Perceptual: 7.8620)\nEpoch [4/50], Batch [90/250] | D Loss: 0.6468 | G Loss: 13.4847 (GAN: 0.7727, L1: 4.3250, Perceptual: 8.3870)\nEpoch [4/50], Batch [100/250] | D Loss: 0.6439 | G Loss: 13.7874 (GAN: 0.6877, L1: 4.5774, Perceptual: 8.5223)\nEpoch [4/50], Batch [110/250] | D Loss: 0.7711 | G Loss: 13.0055 (GAN: 0.9214, L1: 3.7518, Perceptual: 8.3322)\nEpoch [4/50], Batch [120/250] | D Loss: 0.6380 | G Loss: 13.7475 (GAN: 0.9244, L1: 4.4024, Perceptual: 8.4207)\nEpoch [4/50], Batch [130/250] | D Loss: 0.6199 | G Loss: 13.8969 (GAN: 1.1532, L1: 4.0669, Perceptual: 8.6767)\nEpoch [4/50], Batch [140/250] | D Loss: 0.4980 | G Loss: 13.5250 (GAN: 0.8014, L1: 4.2339, Perceptual: 8.4897)\nEpoch [4/50], Batch [150/250] | D Loss: 0.6879 | G Loss: 13.0515 (GAN: 0.8615, L1: 4.1692, Perceptual: 8.0208)\nEpoch [4/50], Batch [160/250] | D Loss: 0.5107 | G Loss: 13.5793 (GAN: 1.0326, L1: 4.1536, Perceptual: 8.3932)\nEpoch [4/50], Batch [170/250] | D Loss: 0.6484 | G Loss: 14.8056 (GAN: 1.2911, L1: 4.6985, Perceptual: 8.8160)\nEpoch [4/50], Batch [180/250] | D Loss: 0.5041 | G Loss: 13.6611 (GAN: 0.7587, L1: 4.8554, Perceptual: 8.0470)\nEpoch [4/50], Batch [190/250] | D Loss: 0.6714 | G Loss: 12.2910 (GAN: 0.8105, L1: 3.5829, Perceptual: 7.8976)\nEpoch [4/50], Batch [200/250] | D Loss: 0.5320 | G Loss: 12.1041 (GAN: 0.7213, L1: 3.8142, Perceptual: 7.5687)\nEpoch [4/50], Batch [210/250] | D Loss: 0.5608 | G Loss: 12.2120 (GAN: 0.6813, L1: 3.7835, Perceptual: 7.7473)\nEpoch [4/50], Batch [220/250] | D Loss: 0.6198 | G Loss: 13.4602 (GAN: 1.0862, L1: 4.2006, Perceptual: 8.1735)\nEpoch [4/50], Batch [230/250] | D Loss: 0.5831 | G Loss: 13.1143 (GAN: 0.9574, L1: 3.6841, Perceptual: 8.4729)\nEpoch [4/50], Batch [240/250] | D Loss: 0.6220 | G Loss: 14.2730 (GAN: 1.0433, L1: 4.7227, Perceptual: 8.5070)\nEpoch [4/50], Batch [250/250] | D Loss: 0.5829 | G Loss: 12.8470 (GAN: 0.6326, L1: 4.2426, Perceptual: 7.9718)\nEpoch 4 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [5/50], Batch [10/250] | D Loss: 0.5159 | G Loss: 12.8124 (GAN: 0.6401, L1: 4.0619, Perceptual: 8.1104)\nEpoch [5/50], Batch [20/250] | D Loss: 0.7621 | G Loss: 13.8804 (GAN: 1.0787, L1: 4.4430, Perceptual: 8.3587)\nEpoch [5/50], Batch [30/250] | D Loss: 0.6757 | G Loss: 14.0887 (GAN: 1.1020, L1: 4.0610, Perceptual: 8.9258)\nEpoch [5/50], Batch [40/250] | D Loss: 0.4308 | G Loss: 13.1658 (GAN: 0.4949, L1: 4.4302, Perceptual: 8.2406)\nEpoch [5/50], Batch [50/250] | D Loss: 0.6944 | G Loss: 13.4061 (GAN: 1.1729, L1: 4.1795, Perceptual: 8.0536)\nEpoch [5/50], Batch [60/250] | D Loss: 0.5572 | G Loss: 13.6509 (GAN: 1.0966, L1: 4.3828, Perceptual: 8.1716)\nEpoch [5/50], Batch [70/250] | D Loss: 0.6519 | G Loss: 12.6991 (GAN: 0.7830, L1: 4.1061, Perceptual: 7.8101)\nEpoch [5/50], Batch [80/250] | D Loss: 0.4673 | G Loss: 13.5530 (GAN: 0.9216, L1: 4.2268, Perceptual: 8.4046)\nEpoch [5/50], Batch [90/250] | D Loss: 0.7032 | G Loss: 11.0772 (GAN: 0.7024, L1: 3.0209, Perceptual: 7.3539)\nEpoch [5/50], Batch [100/250] | D Loss: 0.5997 | G Loss: 13.0883 (GAN: 1.0460, L1: 4.0501, Perceptual: 7.9922)\nEpoch [5/50], Batch [110/250] | D Loss: 0.6760 | G Loss: 13.2438 (GAN: 0.8992, L1: 4.5861, Perceptual: 7.7584)\nEpoch [5/50], Batch [120/250] | D Loss: 0.6155 | G Loss: 12.6103 (GAN: 0.8915, L1: 4.0319, Perceptual: 7.6869)\nEpoch [5/50], Batch [130/250] | D Loss: 0.5969 | G Loss: 11.9855 (GAN: 0.7389, L1: 3.8419, Perceptual: 7.4047)\nEpoch [5/50], Batch [140/250] | D Loss: 0.5539 | G Loss: 15.1555 (GAN: 0.9854, L1: 5.9018, Perceptual: 8.2683)\nEpoch [5/50], Batch [150/250] | D Loss: 0.5267 | G Loss: 12.2994 (GAN: 0.9976, L1: 3.7377, Perceptual: 7.5640)\nEpoch [5/50], Batch [160/250] | D Loss: 0.5985 | G Loss: 12.2855 (GAN: 0.8667, L1: 3.4081, Perceptual: 8.0108)\nEpoch [5/50], Batch [170/250] | D Loss: 0.5932 | G Loss: 14.4514 (GAN: 0.7910, L1: 4.6374, Perceptual: 9.0230)\nEpoch [5/50], Batch [180/250] | D Loss: 0.4972 | G Loss: 13.4412 (GAN: 0.9016, L1: 4.2379, Perceptual: 8.3017)\nEpoch [5/50], Batch [190/250] | D Loss: 0.7103 | G Loss: 13.5402 (GAN: 0.9708, L1: 4.4618, Perceptual: 8.1076)\nEpoch [5/50], Batch [200/250] | D Loss: 0.4268 | G Loss: 12.6991 (GAN: 0.8302, L1: 4.4546, Perceptual: 7.4143)\nEpoch [5/50], Batch [210/250] | D Loss: 0.8218 | G Loss: 10.8471 (GAN: 0.7841, L1: 3.1440, Perceptual: 6.9189)\nEpoch [5/50], Batch [220/250] | D Loss: 0.7375 | G Loss: 12.2971 (GAN: 0.8591, L1: 4.1419, Perceptual: 7.2960)\nEpoch [5/50], Batch [230/250] | D Loss: 0.6090 | G Loss: 12.5738 (GAN: 0.8420, L1: 3.7981, Perceptual: 7.9337)\nEpoch [5/50], Batch [240/250] | D Loss: 0.7594 | G Loss: 12.5951 (GAN: 0.7330, L1: 3.9701, Perceptual: 7.8920)\nEpoch [5/50], Batch [250/250] | D Loss: 0.5329 | G Loss: 15.1959 (GAN: 1.2189, L1: 5.7525, Perceptual: 8.2245)\nEpoch 5 completed. Current LR G: 0.000200, LR D: 0.000100\n--- Entering save/checkpoint block for epoch 5 ---\nSaving samples and checkpoint for epoch 5...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_5_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [6/50], Batch [10/250] | D Loss: 0.5551 | G Loss: 12.6526 (GAN: 0.8345, L1: 3.9154, Perceptual: 7.9027)\nEpoch [6/50], Batch [20/250] | D Loss: 0.8007 | G Loss: 11.7739 (GAN: 1.0861, L1: 3.4320, Perceptual: 7.2558)\nEpoch [6/50], Batch [30/250] | D Loss: 0.7354 | G Loss: 12.7197 (GAN: 0.8697, L1: 3.8417, Perceptual: 8.0083)\nEpoch [6/50], Batch [40/250] | D Loss: 0.6706 | G Loss: 12.3624 (GAN: 0.8428, L1: 4.3010, Perceptual: 7.2186)\nEpoch [6/50], Batch [50/250] | D Loss: 0.6240 | G Loss: 12.0526 (GAN: 0.6966, L1: 3.8764, Perceptual: 7.4796)\nEpoch [6/50], Batch [60/250] | D Loss: 0.7204 | G Loss: 13.2349 (GAN: 1.0549, L1: 4.0756, Perceptual: 8.1044)\nEpoch [6/50], Batch [70/250] | D Loss: 0.6064 | G Loss: 12.9485 (GAN: 0.8580, L1: 3.8255, Perceptual: 8.2649)\nEpoch [6/50], Batch [80/250] | D Loss: 0.7795 | G Loss: 11.2963 (GAN: 0.6709, L1: 3.6684, Perceptual: 6.9570)\nEpoch [6/50], Batch [90/250] | D Loss: 0.5450 | G Loss: 12.6374 (GAN: 0.7526, L1: 3.8003, Perceptual: 8.0846)\nEpoch [6/50], Batch [100/250] | D Loss: 0.6017 | G Loss: 12.2938 (GAN: 0.9915, L1: 4.1823, Perceptual: 7.1199)\nEpoch [6/50], Batch [110/250] | D Loss: 0.6769 | G Loss: 13.1815 (GAN: 0.7499, L1: 4.3001, Perceptual: 8.1315)\nEpoch [6/50], Batch [120/250] | D Loss: 0.5658 | G Loss: 11.8455 (GAN: 1.0777, L1: 3.5893, Perceptual: 7.1785)\nEpoch [6/50], Batch [130/250] | D Loss: 0.5866 | G Loss: 12.1295 (GAN: 0.7024, L1: 3.7824, Perceptual: 7.6447)\nEpoch [6/50], Batch [140/250] | D Loss: 0.5411 | G Loss: 12.5344 (GAN: 1.0493, L1: 4.0938, Perceptual: 7.3914)\nEpoch [6/50], Batch [150/250] | D Loss: 0.5979 | G Loss: 11.9048 (GAN: 0.7253, L1: 3.7632, Perceptual: 7.4163)\nEpoch [6/50], Batch [160/250] | D Loss: 0.6336 | G Loss: 14.1817 (GAN: 0.9318, L1: 4.5550, Perceptual: 8.6949)\nEpoch [6/50], Batch [170/250] | D Loss: 0.5628 | G Loss: 11.9646 (GAN: 0.8463, L1: 3.6961, Perceptual: 7.4223)\nEpoch [6/50], Batch [180/250] | D Loss: 0.6860 | G Loss: 12.3306 (GAN: 0.9597, L1: 3.7761, Perceptual: 7.5948)\nEpoch [6/50], Batch [190/250] | D Loss: 0.5479 | G Loss: 12.5918 (GAN: 0.8443, L1: 4.2940, Perceptual: 7.4534)\nEpoch [6/50], Batch [200/250] | D Loss: 0.6049 | G Loss: 10.8612 (GAN: 0.6061, L1: 3.4044, Perceptual: 6.8506)\nEpoch [6/50], Batch [210/250] | D Loss: 0.6275 | G Loss: 12.8851 (GAN: 0.7940, L1: 4.0280, Perceptual: 8.0631)\nEpoch [6/50], Batch [220/250] | D Loss: 0.6811 | G Loss: 12.2222 (GAN: 0.8125, L1: 3.9076, Perceptual: 7.5021)\nEpoch [6/50], Batch [230/250] | D Loss: 0.5946 | G Loss: 13.0386 (GAN: 0.9730, L1: 4.1019, Perceptual: 7.9636)\nEpoch [6/50], Batch [240/250] | D Loss: 0.5075 | G Loss: 12.5917 (GAN: 0.8794, L1: 3.9333, Perceptual: 7.7791)\nEpoch [6/50], Batch [250/250] | D Loss: 0.5743 | G Loss: 12.2381 (GAN: 0.7854, L1: 3.7701, Perceptual: 7.6826)\nEpoch 6 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [7/50], Batch [10/250] | D Loss: 0.5162 | G Loss: 11.8733 (GAN: 0.5214, L1: 3.9847, Perceptual: 7.3671)\nEpoch [7/50], Batch [20/250] | D Loss: 0.5426 | G Loss: 12.9251 (GAN: 0.9667, L1: 3.8408, Perceptual: 8.1176)\nEpoch [7/50], Batch [30/250] | D Loss: 0.6499 | G Loss: 12.2536 (GAN: 1.0994, L1: 3.5551, Perceptual: 7.5991)\nEpoch [7/50], Batch [40/250] | D Loss: 0.6146 | G Loss: 13.4817 (GAN: 0.7927, L1: 4.1951, Perceptual: 8.4939)\nEpoch [7/50], Batch [50/250] | D Loss: 0.6300 | G Loss: 12.0601 (GAN: 0.7265, L1: 3.8963, Perceptual: 7.4373)\nEpoch [7/50], Batch [60/250] | D Loss: 0.6974 | G Loss: 11.5241 (GAN: 0.7737, L1: 3.4402, Perceptual: 7.3102)\nEpoch [7/50], Batch [70/250] | D Loss: 0.6507 | G Loss: 12.4249 (GAN: 0.8709, L1: 3.8416, Perceptual: 7.7123)\nEpoch [7/50], Batch [80/250] | D Loss: 0.5462 | G Loss: 13.3556 (GAN: 1.0788, L1: 4.2277, Perceptual: 8.0491)\nEpoch [7/50], Batch [90/250] | D Loss: 0.6852 | G Loss: 11.5431 (GAN: 0.5942, L1: 3.4752, Perceptual: 7.4737)\nEpoch [7/50], Batch [100/250] | D Loss: 0.7056 | G Loss: 11.2720 (GAN: 0.7987, L1: 3.3434, Perceptual: 7.1299)\nEpoch [7/50], Batch [110/250] | D Loss: 0.5710 | G Loss: 12.9107 (GAN: 0.6590, L1: 4.3303, Perceptual: 7.9214)\nEpoch [7/50], Batch [120/250] | D Loss: 0.4386 | G Loss: 13.9798 (GAN: 0.6105, L1: 4.9725, Perceptual: 8.3968)\nEpoch [7/50], Batch [130/250] | D Loss: 0.6644 | G Loss: 12.0936 (GAN: 0.6690, L1: 3.7198, Perceptual: 7.7048)\nEpoch [7/50], Batch [140/250] | D Loss: 0.8816 | G Loss: 11.5156 (GAN: 0.8779, L1: 3.4886, Perceptual: 7.1490)\nEpoch [7/50], Batch [150/250] | D Loss: 0.6230 | G Loss: 13.9249 (GAN: 0.8102, L1: 4.5702, Perceptual: 8.5444)\nEpoch [7/50], Batch [160/250] | D Loss: 0.5098 | G Loss: 12.9388 (GAN: 0.8018, L1: 4.4604, Perceptual: 7.6766)\nEpoch [7/50], Batch [170/250] | D Loss: 0.5560 | G Loss: 12.5565 (GAN: 0.9681, L1: 3.9689, Perceptual: 7.6196)\nEpoch [7/50], Batch [180/250] | D Loss: 0.5820 | G Loss: 12.6718 (GAN: 0.8729, L1: 3.8745, Perceptual: 7.9245)\nEpoch [7/50], Batch [190/250] | D Loss: 0.5030 | G Loss: 13.2047 (GAN: 0.9889, L1: 4.3676, Perceptual: 7.8481)\nEpoch [7/50], Batch [200/250] | D Loss: 0.5829 | G Loss: 12.3389 (GAN: 0.7990, L1: 3.8452, Perceptual: 7.6947)\nEpoch [7/50], Batch [210/250] | D Loss: 0.7559 | G Loss: 10.4676 (GAN: 0.7678, L1: 3.2499, Perceptual: 6.4499)\nEpoch [7/50], Batch [220/250] | D Loss: 0.5912 | G Loss: 12.5291 (GAN: 1.0660, L1: 3.9729, Perceptual: 7.4903)\nEpoch [7/50], Batch [230/250] | D Loss: 0.7249 | G Loss: 11.6671 (GAN: 0.8057, L1: 3.4605, Perceptual: 7.4009)\nEpoch [7/50], Batch [240/250] | D Loss: 0.5503 | G Loss: 12.4353 (GAN: 0.5294, L1: 4.2582, Perceptual: 7.6476)\nEpoch [7/50], Batch [250/250] | D Loss: 0.7072 | G Loss: 10.8811 (GAN: 0.6926, L1: 3.3113, Perceptual: 6.8772)\nEpoch 7 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [8/50], Batch [10/250] | D Loss: 0.5749 | G Loss: 13.2317 (GAN: 0.7768, L1: 5.0770, Perceptual: 7.3779)\nEpoch [8/50], Batch [20/250] | D Loss: 0.5757 | G Loss: 13.5341 (GAN: 0.8151, L1: 4.0937, Perceptual: 8.6253)\nEpoch [8/50], Batch [30/250] | D Loss: 0.6485 | G Loss: 11.4051 (GAN: 0.7662, L1: 3.2833, Perceptual: 7.3556)\nEpoch [8/50], Batch [40/250] | D Loss: 0.6566 | G Loss: 13.5544 (GAN: 0.6136, L1: 4.9635, Perceptual: 7.9773)\nEpoch [8/50], Batch [50/250] | D Loss: 0.6826 | G Loss: 12.2401 (GAN: 0.8819, L1: 3.6224, Perceptual: 7.7359)\nEpoch [8/50], Batch [60/250] | D Loss: 0.5608 | G Loss: 11.8143 (GAN: 0.6897, L1: 3.8414, Perceptual: 7.2832)\nEpoch [8/50], Batch [70/250] | D Loss: 0.5542 | G Loss: 11.9663 (GAN: 1.0774, L1: 3.6624, Perceptual: 7.2266)\nEpoch [8/50], Batch [80/250] | D Loss: 0.7293 | G Loss: 11.0946 (GAN: 0.7971, L1: 3.2723, Perceptual: 7.0253)\nEpoch [8/50], Batch [90/250] | D Loss: 0.6653 | G Loss: 11.5988 (GAN: 0.7647, L1: 3.6730, Perceptual: 7.1611)\nEpoch [8/50], Batch [100/250] | D Loss: 0.8227 | G Loss: 13.9698 (GAN: 1.1198, L1: 4.4473, Perceptual: 8.4027)\nEpoch [8/50], Batch [110/250] | D Loss: 0.5382 | G Loss: 12.6961 (GAN: 0.5517, L1: 4.3654, Perceptual: 7.7790)\nEpoch [8/50], Batch [120/250] | D Loss: 0.6715 | G Loss: 11.2011 (GAN: 0.9215, L1: 3.4756, Perceptual: 6.8040)\nEpoch [8/50], Batch [130/250] | D Loss: 0.5280 | G Loss: 11.6957 (GAN: 0.6555, L1: 3.8873, Perceptual: 7.1530)\nEpoch [8/50], Batch [140/250] | D Loss: 0.6727 | G Loss: 11.4962 (GAN: 0.7554, L1: 3.6703, Perceptual: 7.0706)\nEpoch [8/50], Batch [150/250] | D Loss: 0.5208 | G Loss: 12.6836 (GAN: 0.8663, L1: 3.8613, Perceptual: 7.9560)\nEpoch [8/50], Batch [160/250] | D Loss: 0.5323 | G Loss: 12.1178 (GAN: 0.8012, L1: 3.7224, Perceptual: 7.5943)\nEpoch [8/50], Batch [170/250] | D Loss: 0.6708 | G Loss: 11.7974 (GAN: 0.6487, L1: 3.7044, Perceptual: 7.4443)\nEpoch [8/50], Batch [180/250] | D Loss: 0.6791 | G Loss: 11.4573 (GAN: 0.9907, L1: 3.5639, Perceptual: 6.9027)\nEpoch [8/50], Batch [190/250] | D Loss: 0.6691 | G Loss: 11.2923 (GAN: 0.7467, L1: 3.6436, Perceptual: 6.9020)\nEpoch [8/50], Batch [200/250] | D Loss: 0.6611 | G Loss: 10.6335 (GAN: 0.7142, L1: 3.1691, Perceptual: 6.7502)\nEpoch [8/50], Batch [210/250] | D Loss: 0.6656 | G Loss: 12.5736 (GAN: 0.9783, L1: 4.2517, Perceptual: 7.3436)\nEpoch [8/50], Batch [220/250] | D Loss: 0.5644 | G Loss: 12.2048 (GAN: 0.6741, L1: 3.9314, Perceptual: 7.5993)\nEpoch [8/50], Batch [230/250] | D Loss: 0.6297 | G Loss: 12.1245 (GAN: 0.7457, L1: 3.7235, Perceptual: 7.6553)\nEpoch [8/50], Batch [240/250] | D Loss: 0.5358 | G Loss: 13.2411 (GAN: 0.7316, L1: 4.1621, Perceptual: 8.3473)\nEpoch [8/50], Batch [250/250] | D Loss: 0.6067 | G Loss: 13.1444 (GAN: 0.7891, L1: 3.9998, Perceptual: 8.3556)\nEpoch 8 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [9/50], Batch [10/250] | D Loss: 0.6952 | G Loss: 12.0094 (GAN: 0.9521, L1: 3.5837, Perceptual: 7.4735)\nEpoch [9/50], Batch [20/250] | D Loss: 0.5607 | G Loss: 12.2302 (GAN: 0.9712, L1: 3.6073, Perceptual: 7.6518)\nEpoch [9/50], Batch [30/250] | D Loss: 0.6714 | G Loss: 11.1943 (GAN: 0.8740, L1: 3.6770, Perceptual: 6.6433)\nEpoch [9/50], Batch [40/250] | D Loss: 0.5274 | G Loss: 13.0657 (GAN: 0.9694, L1: 4.1908, Perceptual: 7.9055)\nEpoch [9/50], Batch [50/250] | D Loss: 0.8274 | G Loss: 13.0948 (GAN: 1.0136, L1: 4.2289, Perceptual: 7.8523)\nEpoch [9/50], Batch [100/250] | D Loss: 0.6969 | G Loss: 12.1839 (GAN: 0.9032, L1: 3.7584, Perceptual: 7.5223)\nEpoch [9/50], Batch [110/250] | D Loss: 0.6770 | G Loss: 11.9700 (GAN: 1.0091, L1: 3.7795, Perceptual: 7.1814)\nEpoch [9/50], Batch [120/250] | D Loss: 0.8346 | G Loss: 12.1111 (GAN: 0.9398, L1: 3.7003, Perceptual: 7.4710)\nEpoch [9/50], Batch [130/250] | D Loss: 0.6147 | G Loss: 12.0870 (GAN: 0.7422, L1: 3.5826, Perceptual: 7.7622)\nEpoch [9/50], Batch [140/250] | D Loss: 0.4865 | G Loss: 13.0962 (GAN: 0.6769, L1: 4.1533, Perceptual: 8.2660)\nEpoch [9/50], Batch [150/250] | D Loss: 0.6625 | G Loss: 10.8506 (GAN: 0.7021, L1: 3.2188, Perceptual: 6.9297)\nEpoch [9/50], Batch [160/250] | D Loss: 0.5772 | G Loss: 13.2214 (GAN: 0.7473, L1: 4.3077, Perceptual: 8.1664)\nEpoch [9/50], Batch [170/250] | D Loss: 0.8724 | G Loss: 12.0152 (GAN: 1.0911, L1: 3.4889, Perceptual: 7.4352)\nEpoch [9/50], Batch [180/250] | D Loss: 0.5871 | G Loss: 12.6365 (GAN: 0.7056, L1: 3.9857, Perceptual: 7.9452)\nEpoch [9/50], Batch [190/250] | D Loss: 0.6334 | G Loss: 12.2678 (GAN: 0.8420, L1: 3.5873, Perceptual: 7.8385)\nEpoch [9/50], Batch [200/250] | D Loss: 0.7282 | G Loss: 10.9627 (GAN: 0.6696, L1: 3.3010, Perceptual: 6.9921)\nEpoch [9/50], Batch [210/250] | D Loss: 0.7296 | G Loss: 11.5600 (GAN: 0.9286, L1: 3.3439, Perceptual: 7.2874)\nEpoch [9/50], Batch [220/250] | D Loss: 0.7665 | G Loss: 12.9745 (GAN: 0.9246, L1: 4.4194, Perceptual: 7.6306)\nEpoch [9/50], Batch [230/250] | D Loss: 0.6320 | G Loss: 13.1393 (GAN: 0.5809, L1: 4.2911, Perceptual: 8.2673)\nEpoch [9/50], Batch [240/250] | D Loss: 0.5841 | G Loss: 11.9874 (GAN: 0.7965, L1: 3.7073, Perceptual: 7.4836)\nEpoch [9/50], Batch [250/250] | D Loss: 0.7207 | G Loss: 11.9998 (GAN: 0.8460, L1: 3.5846, Perceptual: 7.5691)\nEpoch 9 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [10/50], Batch [10/250] | D Loss: 0.5746 | G Loss: 14.2948 (GAN: 0.7035, L1: 4.9538, Perceptual: 8.6376)\nEpoch [10/50], Batch [20/250] | D Loss: 0.6328 | G Loss: 13.3551 (GAN: 1.0765, L1: 4.2430, Perceptual: 8.0356)\nEpoch [10/50], Batch [30/250] | D Loss: 0.5733 | G Loss: 12.1572 (GAN: 0.9680, L1: 3.7664, Perceptual: 7.4228)\nEpoch [10/50], Batch [40/250] | D Loss: 0.6054 | G Loss: 13.2357 (GAN: 0.6440, L1: 4.4661, Perceptual: 8.1255)\nEpoch [10/50], Batch [50/250] | D Loss: 0.6436 | G Loss: 12.2175 (GAN: 0.8785, L1: 3.7250, Perceptual: 7.6139)\nEpoch [10/50], Batch [60/250] | D Loss: 0.7542 | G Loss: 11.7036 (GAN: 0.5171, L1: 3.7681, Perceptual: 7.4184)\nEpoch [10/50], Batch [70/250] | D Loss: 0.5472 | G Loss: 11.7273 (GAN: 0.7331, L1: 3.6364, Perceptual: 7.3578)\nEpoch [10/50], Batch [80/250] | D Loss: 0.9323 | G Loss: 13.2421 (GAN: 0.8385, L1: 4.6195, Perceptual: 7.7841)\nEpoch [10/50], Batch [90/250] | D Loss: 0.7051 | G Loss: 11.8996 (GAN: 0.8023, L1: 3.7503, Perceptual: 7.3470)\nEpoch [10/50], Batch [100/250] | D Loss: 0.5143 | G Loss: 12.5144 (GAN: 0.6287, L1: 4.1125, Perceptual: 7.7733)\nEpoch [10/50], Batch [110/250] | D Loss: 0.6311 | G Loss: 11.9080 (GAN: 0.8317, L1: 3.4641, Perceptual: 7.6122)\nEpoch [10/50], Batch [120/250] | D Loss: 0.5063 | G Loss: 12.5694 (GAN: 0.7851, L1: 3.9890, Perceptual: 7.7954)\nEpoch [10/50], Batch [130/250] | D Loss: 0.6315 | G Loss: 12.9332 (GAN: 0.8184, L1: 4.3414, Perceptual: 7.7734)\nEpoch [10/50], Batch [140/250] | D Loss: 0.4632 | G Loss: 12.8615 (GAN: 0.7940, L1: 4.6514, Perceptual: 7.4161)\nEpoch [10/50], Batch [150/250] | D Loss: 0.6168 | G Loss: 11.0848 (GAN: 0.8656, L1: 3.1352, Perceptual: 7.0840)\nEpoch [10/50], Batch [160/250] | D Loss: 0.5823 | G Loss: 13.4405 (GAN: 1.0061, L1: 4.1276, Perceptual: 8.3068)\nEpoch [10/50], Batch [170/250] | D Loss: 0.5455 | G Loss: 12.2722 (GAN: 0.7874, L1: 3.5698, Perceptual: 7.9149)\nEpoch [10/50], Batch [180/250] | D Loss: 0.6502 | G Loss: 11.8201 (GAN: 0.4846, L1: 3.5958, Perceptual: 7.7398)\nEpoch [10/50], Batch [190/250] | D Loss: 0.6600 | G Loss: 12.6741 (GAN: 1.0089, L1: 3.9309, Perceptual: 7.7343)\nEpoch [10/50], Batch [200/250] | D Loss: 0.6052 | G Loss: 11.5423 (GAN: 0.8506, L1: 3.3648, Perceptual: 7.3269)\nEpoch [10/50], Batch [210/250] | D Loss: 0.6145 | G Loss: 10.9851 (GAN: 0.7097, L1: 3.2100, Perceptual: 7.0655)\nEpoch [10/50], Batch [220/250] | D Loss: 0.4865 | G Loss: 13.2637 (GAN: 0.8431, L1: 4.3659, Perceptual: 8.0546)\nEpoch [10/50], Batch [230/250] | D Loss: 0.5099 | G Loss: 13.3439 (GAN: 1.0630, L1: 4.2036, Perceptual: 8.0773)\nEpoch [10/50], Batch [240/250] | D Loss: 0.6556 | G Loss: 12.8074 (GAN: 0.7054, L1: 4.1758, Perceptual: 7.9263)\nEpoch [10/50], Batch [250/250] | D Loss: 0.5932 | G Loss: 12.2928 (GAN: 0.8511, L1: 3.9803, Perceptual: 7.4614)\nEpoch 10 completed. Current LR G: 0.000200, LR D: 0.000100\n--- Entering save/checkpoint block for epoch 10 ---\nSaving samples and checkpoint for epoch 10...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_10_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [11/50], Batch [10/250] | D Loss: 0.7449 | G Loss: 11.4495 (GAN: 0.9416, L1: 3.2547, Perceptual: 7.2532)\nEpoch [11/50], Batch [20/250] | D Loss: 0.5611 | G Loss: 11.5326 (GAN: 0.6426, L1: 3.5477, Perceptual: 7.3423)\nEpoch [11/50], Batch [30/250] | D Loss: 0.5503 | G Loss: 12.2383 (GAN: 0.7878, L1: 3.7682, Perceptual: 7.6823)\nEpoch [11/50], Batch [40/250] | D Loss: 0.6551 | G Loss: 14.2566 (GAN: 0.8617, L1: 5.0833, Perceptual: 8.3117)\nEpoch [11/50], Batch [50/250] | D Loss: 0.7765 | G Loss: 11.1526 (GAN: 0.7049, L1: 3.4775, Perceptual: 6.9703)\nEpoch [11/50], Batch [60/250] | D Loss: 0.6645 | G Loss: 11.2421 (GAN: 0.6787, L1: 3.5852, Perceptual: 6.9782)\nEpoch [11/50], Batch [70/250] | D Loss: 0.6265 | G Loss: 11.8864 (GAN: 0.5330, L1: 4.2828, Perceptual: 7.0706)\nEpoch [11/50], Batch [80/250] | D Loss: 0.5103 | G Loss: 11.9496 (GAN: 0.8335, L1: 3.8168, Perceptual: 7.2993)\nEpoch [11/50], Batch [90/250] | D Loss: 0.4456 | G Loss: 12.5755 (GAN: 0.9545, L1: 4.1182, Perceptual: 7.5029)\nEpoch [11/50], Batch [100/250] | D Loss: 0.5930 | G Loss: 12.1599 (GAN: 0.8121, L1: 3.7812, Perceptual: 7.5666)\nEpoch [11/50], Batch [110/250] | D Loss: 0.7773 | G Loss: 12.4836 (GAN: 0.8200, L1: 3.9140, Perceptual: 7.7496)\nEpoch [11/50], Batch [120/250] | D Loss: 0.6931 | G Loss: 12.1278 (GAN: 0.4918, L1: 4.0256, Perceptual: 7.6103)\nEpoch [11/50], Batch [130/250] | D Loss: 0.5137 | G Loss: 12.8213 (GAN: 0.9602, L1: 4.1046, Perceptual: 7.7565)\nEpoch [11/50], Batch [140/250] | D Loss: 0.4306 | G Loss: 12.6278 (GAN: 0.7733, L1: 4.0962, Perceptual: 7.7583)\nEpoch [11/50], Batch [150/250] | D Loss: 0.7027 | G Loss: 11.7512 (GAN: 0.6956, L1: 3.9463, Perceptual: 7.1093)\nEpoch [11/50], Batch [160/250] | D Loss: 0.6878 | G Loss: 12.4296 (GAN: 1.0099, L1: 3.5922, Perceptual: 7.8275)\nEpoch [11/50], Batch [170/250] | D Loss: 0.5836 | G Loss: 12.3446 (GAN: 0.7526, L1: 3.5694, Perceptual: 8.0227)\nEpoch [11/50], Batch [180/250] | D Loss: 0.5967 | G Loss: 12.4271 (GAN: 0.8687, L1: 3.9225, Perceptual: 7.6359)\nEpoch [11/50], Batch [190/250] | D Loss: 0.5969 | G Loss: 13.2840 (GAN: 0.7821, L1: 4.3635, Perceptual: 8.1383)\nEpoch [11/50], Batch [200/250] | D Loss: 0.5303 | G Loss: 12.6070 (GAN: 0.6105, L1: 4.1325, Perceptual: 7.8640)\nEpoch [11/50], Batch [210/250] | D Loss: 0.6694 | G Loss: 11.5712 (GAN: 0.8437, L1: 3.3010, Perceptual: 7.4265)\nEpoch [11/50], Batch [220/250] | D Loss: 0.6340 | G Loss: 11.6034 (GAN: 0.9944, L1: 3.7333, Perceptual: 6.8757)\nEpoch [11/50], Batch [230/250] | D Loss: 0.5617 | G Loss: 12.3939 (GAN: 0.7979, L1: 3.8802, Perceptual: 7.7158)\nEpoch [11/50], Batch [240/250] | D Loss: 0.6775 | G Loss: 11.3372 (GAN: 0.6658, L1: 3.3483, Perceptual: 7.3231)\nEpoch [11/50], Batch [250/250] | D Loss: 0.7291 | G Loss: 12.0151 (GAN: 0.6439, L1: 3.7664, Perceptual: 7.6048)\nEpoch 11 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [12/50], Batch [10/250] | D Loss: 0.6608 | G Loss: 12.0281 (GAN: 0.8504, L1: 3.6488, Perceptual: 7.5290)\nEpoch [12/50], Batch [20/250] | D Loss: 0.6282 | G Loss: 12.4842 (GAN: 1.0174, L1: 3.6545, Perceptual: 7.8123)\nEpoch [12/50], Batch [30/250] | D Loss: 0.6862 | G Loss: 11.6322 (GAN: 0.9735, L1: 3.4579, Perceptual: 7.2008)\nEpoch [12/50], Batch [40/250] | D Loss: 0.5428 | G Loss: 12.4123 (GAN: 0.8818, L1: 3.8358, Perceptual: 7.6947)\nEpoch [12/50], Batch [50/250] | D Loss: 0.8526 | G Loss: 12.2422 (GAN: 0.9585, L1: 3.7270, Perceptual: 7.5567)\nEpoch [12/50], Batch [60/250] | D Loss: 0.6937 | G Loss: 11.6947 (GAN: 0.6278, L1: 3.6783, Perceptual: 7.3886)\nEpoch [12/50], Batch [70/250] | D Loss: 0.5439 | G Loss: 11.7576 (GAN: 0.6753, L1: 3.6467, Perceptual: 7.4357)\nEpoch [12/50], Batch [80/250] | D Loss: 0.5854 | G Loss: 12.2736 (GAN: 0.8905, L1: 3.8020, Perceptual: 7.5812)\nEpoch [12/50], Batch [90/250] | D Loss: 0.9123 | G Loss: 11.6859 (GAN: 0.7243, L1: 3.8114, Perceptual: 7.1502)\nEpoch [12/50], Batch [100/250] | D Loss: 0.5917 | G Loss: 11.1601 (GAN: 0.7638, L1: 3.2895, Perceptual: 7.1068)\nEpoch [12/50], Batch [110/250] | D Loss: 0.5290 | G Loss: 12.7317 (GAN: 0.9024, L1: 3.8203, Perceptual: 8.0091)\nEpoch [12/50], Batch [120/250] | D Loss: 0.5722 | G Loss: 11.6364 (GAN: 0.7658, L1: 3.5849, Perceptual: 7.2856)\nEpoch [12/50], Batch [130/250] | D Loss: 0.7889 | G Loss: 11.3113 (GAN: 0.9439, L1: 3.5616, Perceptual: 6.8058)\nEpoch [12/50], Batch [140/250] | D Loss: 0.7608 | G Loss: 11.8184 (GAN: 0.6743, L1: 3.4682, Perceptual: 7.6759)\nEpoch [12/50], Batch [150/250] | D Loss: 0.6522 | G Loss: 12.5854 (GAN: 0.8681, L1: 4.1567, Perceptual: 7.5606)\nEpoch [12/50], Batch [160/250] | D Loss: 0.7149 | G Loss: 11.8943 (GAN: 0.8597, L1: 3.4156, Perceptual: 7.6190)\nEpoch [12/50], Batch [170/250] | D Loss: 0.6887 | G Loss: 11.6729 (GAN: 0.8137, L1: 4.0955, Perceptual: 6.7637)\nEpoch [12/50], Batch [180/250] | D Loss: 0.5947 | G Loss: 12.5092 (GAN: 0.8304, L1: 4.0652, Perceptual: 7.6136)\nEpoch [12/50], Batch [190/250] | D Loss: 0.6308 | G Loss: 11.1496 (GAN: 0.7051, L1: 3.4047, Perceptual: 7.0398)\nEpoch [12/50], Batch [200/250] | D Loss: 0.4849 | G Loss: 12.2073 (GAN: 0.7345, L1: 4.1178, Perceptual: 7.3550)\nEpoch [12/50], Batch [210/250] | D Loss: 0.6178 | G Loss: 12.0310 (GAN: 0.7137, L1: 3.6917, Perceptual: 7.6256)\nEpoch [12/50], Batch [220/250] | D Loss: 0.5246 | G Loss: 12.7339 (GAN: 0.6935, L1: 3.9856, Perceptual: 8.0548)\nEpoch [12/50], Batch [230/250] | D Loss: 0.5481 | G Loss: 13.2305 (GAN: 0.7680, L1: 4.0518, Perceptual: 8.4107)\nEpoch [12/50], Batch [240/250] | D Loss: 0.5161 | G Loss: 11.3699 (GAN: 0.4415, L1: 3.6529, Perceptual: 7.2754)\nEpoch [12/50], Batch [250/250] | D Loss: 0.8095 | G Loss: 11.7962 (GAN: 1.0897, L1: 3.4479, Perceptual: 7.2587)\nEpoch 12 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [13/50], Batch [10/250] | D Loss: 0.5450 | G Loss: 12.6279 (GAN: 0.8033, L1: 4.0507, Perceptual: 7.7739)\nEpoch [13/50], Batch [20/250] | D Loss: 0.8158 | G Loss: 11.8865 (GAN: 0.9294, L1: 3.9555, Perceptual: 7.0017)\nEpoch [13/50], Batch [30/250] | D Loss: 0.6442 | G Loss: 12.3437 (GAN: 0.9890, L1: 3.8016, Perceptual: 7.5530)\nEpoch [13/50], Batch [40/250] | D Loss: 0.5669 | G Loss: 12.4496 (GAN: 0.9748, L1: 3.7850, Perceptual: 7.6898)\nEpoch [13/50], Batch [50/250] | D Loss: 0.5973 | G Loss: 12.9612 (GAN: 0.7718, L1: 4.2977, Perceptual: 7.8917)\nEpoch [13/50], Batch [60/250] | D Loss: 0.6311 | G Loss: 11.5713 (GAN: 0.9707, L1: 3.2549, Perceptual: 7.3457)\nEpoch [13/50], Batch [70/250] | D Loss: 0.6200 | G Loss: 13.4376 (GAN: 1.0079, L1: 4.0250, Perceptual: 8.4047)\nEpoch [13/50], Batch [80/250] | D Loss: 0.6296 | G Loss: 10.8414 (GAN: 0.6684, L1: 3.5310, Perceptual: 6.6420)\nEpoch [13/50], Batch [90/250] | D Loss: 0.4301 | G Loss: 12.0198 (GAN: 0.8052, L1: 3.6751, Perceptual: 7.5395)\nEpoch [13/50], Batch [100/250] | D Loss: 0.4581 | G Loss: 13.3772 (GAN: 1.0744, L1: 4.5880, Perceptual: 7.7148)\nEpoch [13/50], Batch [110/250] | D Loss: 0.7164 | G Loss: 12.0950 (GAN: 0.8253, L1: 3.5726, Perceptual: 7.6971)\nEpoch [13/50], Batch [120/250] | D Loss: 0.8124 | G Loss: 11.9421 (GAN: 0.8294, L1: 3.7030, Perceptual: 7.4098)\nEpoch [13/50], Batch [130/250] | D Loss: 0.5287 | G Loss: 12.3152 (GAN: 0.8570, L1: 3.7055, Perceptual: 7.7526)\nEpoch [13/50], Batch [140/250] | D Loss: 0.7769 | G Loss: 11.5150 (GAN: 0.8830, L1: 3.5131, Perceptual: 7.1190)\nEpoch [13/50], Batch [150/250] | D Loss: 0.7410 | G Loss: 11.6989 (GAN: 0.7589, L1: 3.4062, Perceptual: 7.5338)\nEpoch [13/50], Batch [160/250] | D Loss: 0.5463 | G Loss: 12.6602 (GAN: 0.7674, L1: 4.3306, Perceptual: 7.5622)\nEpoch [13/50], Batch [170/250] | D Loss: 0.6307 | G Loss: 12.4037 (GAN: 0.9478, L1: 3.7109, Perceptual: 7.7451)\nEpoch [13/50], Batch [180/250] | D Loss: 0.5989 | G Loss: 11.3863 (GAN: 0.6208, L1: 3.4345, Perceptual: 7.3309)\nEpoch [13/50], Batch [190/250] | D Loss: 0.6530 | G Loss: 12.7780 (GAN: 0.9245, L1: 3.9578, Perceptual: 7.8957)\nEpoch [13/50], Batch [200/250] | D Loss: 0.7528 | G Loss: 12.6977 (GAN: 1.0732, L1: 4.2414, Perceptual: 7.3830)\nEpoch [13/50], Batch [210/250] | D Loss: 0.6805 | G Loss: 11.7639 (GAN: 0.7792, L1: 3.6881, Perceptual: 7.2966)\nEpoch [13/50], Batch [220/250] | D Loss: 0.4516 | G Loss: 12.2953 (GAN: 0.6967, L1: 3.8961, Perceptual: 7.7025)\nEpoch [13/50], Batch [230/250] | D Loss: 0.5800 | G Loss: 12.2075 (GAN: 0.8442, L1: 3.9226, Perceptual: 7.4407)\nEpoch [13/50], Batch [240/250] | D Loss: 0.6518 | G Loss: 11.0374 (GAN: 0.6562, L1: 3.2203, Perceptual: 7.1609)\nEpoch [13/50], Batch [250/250] | D Loss: 0.4828 | G Loss: 12.0896 (GAN: 0.8717, L1: 4.0182, Perceptual: 7.1996)\nEpoch 13 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [14/50], Batch [10/250] | D Loss: 0.8826 | G Loss: 11.6722 (GAN: 0.9096, L1: 4.0419, Perceptual: 6.7208)\nEpoch [14/50], Batch [20/250] | D Loss: 0.5279 | G Loss: 13.4840 (GAN: 0.8485, L1: 4.4469, Perceptual: 8.1885)\nEpoch [14/50], Batch [30/250] | D Loss: 0.7002 | G Loss: 12.4675 (GAN: 0.9963, L1: 3.8176, Perceptual: 7.6536)\nEpoch [14/50], Batch [40/250] | D Loss: 0.7002 | G Loss: 11.3246 (GAN: 0.8533, L1: 3.2706, Perceptual: 7.2007)\nEpoch [14/50], Batch [50/250] | D Loss: 0.4913 | G Loss: 12.0780 (GAN: 0.6290, L1: 3.6683, Perceptual: 7.7807)\nEpoch [14/50], Batch [60/250] | D Loss: 0.6003 | G Loss: 13.0945 (GAN: 0.9931, L1: 4.2256, Perceptual: 7.8758)\nEpoch [14/50], Batch [70/250] | D Loss: 0.6988 | G Loss: 12.9819 (GAN: 0.8200, L1: 4.5677, Perceptual: 7.5942)\nEpoch [14/50], Batch [80/250] | D Loss: 0.5898 | G Loss: 12.4702 (GAN: 0.8060, L1: 3.7773, Perceptual: 7.8870)\nEpoch [14/50], Batch [90/250] | D Loss: 0.6672 | G Loss: 11.2692 (GAN: 0.7214, L1: 3.4751, Perceptual: 7.0727)\nEpoch [14/50], Batch [100/250] | D Loss: 0.5700 | G Loss: 11.4117 (GAN: 0.4183, L1: 3.7542, Perceptual: 7.2391)\nEpoch [14/50], Batch [110/250] | D Loss: 0.4984 | G Loss: 12.9377 (GAN: 0.9973, L1: 4.3428, Perceptual: 7.5977)\nEpoch [14/50], Batch [120/250] | D Loss: 0.5503 | G Loss: 11.6751 (GAN: 0.8429, L1: 3.5898, Perceptual: 7.2424)\nEpoch [14/50], Batch [130/250] | D Loss: 0.4954 | G Loss: 11.4796 (GAN: 0.5962, L1: 3.3616, Perceptual: 7.5218)\nEpoch [14/50], Batch [140/250] | D Loss: 0.5940 | G Loss: 11.4171 (GAN: 0.7807, L1: 3.6567, Perceptual: 6.9798)\nEpoch [14/50], Batch [150/250] | D Loss: 0.6039 | G Loss: 11.4462 (GAN: 0.8032, L1: 3.2108, Perceptual: 7.4321)\nEpoch [14/50], Batch [160/250] | D Loss: 0.5190 | G Loss: 11.5143 (GAN: 0.6489, L1: 3.5120, Perceptual: 7.3534)\nEpoch [14/50], Batch [170/250] | D Loss: 0.5955 | G Loss: 12.8143 (GAN: 0.9938, L1: 4.0881, Perceptual: 7.7325)\nEpoch [14/50], Batch [180/250] | D Loss: 0.6926 | G Loss: 11.9359 (GAN: 0.8142, L1: 3.7055, Perceptual: 7.4162)\nEpoch [14/50], Batch [190/250] | D Loss: 0.6994 | G Loss: 12.3139 (GAN: 0.8361, L1: 3.7406, Perceptual: 7.7372)\nEpoch [14/50], Batch [200/250] | D Loss: 0.6772 | G Loss: 10.3115 (GAN: 0.6277, L1: 2.9756, Perceptual: 6.7082)\nEpoch [14/50], Batch [210/250] | D Loss: 0.5915 | G Loss: 11.7166 (GAN: 0.8505, L1: 3.9248, Perceptual: 6.9413)\nEpoch [14/50], Batch [220/250] | D Loss: 0.5516 | G Loss: 11.5570 (GAN: 0.8579, L1: 3.5979, Perceptual: 7.1011)\nEpoch [14/50], Batch [230/250] | D Loss: 0.6211 | G Loss: 12.0483 (GAN: 0.8551, L1: 3.5881, Perceptual: 7.6051)\nEpoch [14/50], Batch [240/250] | D Loss: 0.7578 | G Loss: 12.9882 (GAN: 0.9205, L1: 4.1665, Perceptual: 7.9011)\nEpoch [14/50], Batch [250/250] | D Loss: 0.8652 | G Loss: 11.4480 (GAN: 0.9723, L1: 3.1413, Perceptual: 7.3344)\nEpoch 14 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [15/50], Batch [10/250] | D Loss: 0.5126 | G Loss: 13.0377 (GAN: 0.9641, L1: 4.1206, Perceptual: 7.9531)\nEpoch [15/50], Batch [20/250] | D Loss: 0.4547 | G Loss: 14.8339 (GAN: 1.2187, L1: 4.8897, Perceptual: 8.7255)\nEpoch [15/50], Batch [30/250] | D Loss: 0.4917 | G Loss: 12.3219 (GAN: 0.8525, L1: 4.0563, Perceptual: 7.4131)\nEpoch [15/50], Batch [40/250] | D Loss: 0.5081 | G Loss: 13.6543 (GAN: 0.8764, L1: 4.5022, Perceptual: 8.2757)\nEpoch [15/50], Batch [50/250] | D Loss: 0.7208 | G Loss: 13.0411 (GAN: 1.1133, L1: 4.0178, Perceptual: 7.9100)\nEpoch [15/50], Batch [60/250] | D Loss: 0.5103 | G Loss: 13.6437 (GAN: 0.7867, L1: 4.5505, Perceptual: 8.3065)\nEpoch [15/50], Batch [70/250] | D Loss: 0.5303 | G Loss: 11.0373 (GAN: 0.4877, L1: 3.2513, Perceptual: 7.2983)\nEpoch [15/50], Batch [80/250] | D Loss: 0.6577 | G Loss: 11.7272 (GAN: 0.9301, L1: 3.5831, Perceptual: 7.2140)\nEpoch [15/50], Batch [90/250] | D Loss: 0.6203 | G Loss: 12.1978 (GAN: 0.7139, L1: 3.8496, Perceptual: 7.6343)\nEpoch [15/50], Batch [100/250] | D Loss: 0.6965 | G Loss: 10.7517 (GAN: 0.6931, L1: 3.1223, Perceptual: 6.9363)\nEpoch [15/50], Batch [110/250] | D Loss: 0.6361 | G Loss: 10.7407 (GAN: 0.8161, L1: 2.9856, Perceptual: 6.9390)\nEpoch [15/50], Batch [120/250] | D Loss: 0.6582 | G Loss: 11.0762 (GAN: 1.0886, L1: 3.2052, Perceptual: 6.7824)\nEpoch [15/50], Batch [130/250] | D Loss: 0.4921 | G Loss: 12.0736 (GAN: 0.7602, L1: 3.7416, Perceptual: 7.5717)\nEpoch [15/50], Batch [140/250] | D Loss: 0.5720 | G Loss: 12.2601 (GAN: 0.8804, L1: 3.9643, Perceptual: 7.4153)\nEpoch [15/50], Batch [150/250] | D Loss: 0.6810 | G Loss: 11.0219 (GAN: 0.6290, L1: 3.3426, Perceptual: 7.0503)\nEpoch [15/50], Batch [160/250] | D Loss: 0.5469 | G Loss: 13.1997 (GAN: 1.0460, L1: 3.9783, Perceptual: 8.1754)\nEpoch [15/50], Batch [170/250] | D Loss: 0.4995 | G Loss: 12.1809 (GAN: 0.5846, L1: 3.8896, Perceptual: 7.7067)\nEpoch [15/50], Batch [180/250] | D Loss: 0.7316 | G Loss: 12.1408 (GAN: 0.9856, L1: 3.8962, Perceptual: 7.2590)\nEpoch [15/50], Batch [190/250] | D Loss: 0.7433 | G Loss: 11.3277 (GAN: 0.7534, L1: 3.4768, Perceptual: 7.0976)\nEpoch [15/50], Batch [200/250] | D Loss: 0.5396 | G Loss: 12.5636 (GAN: 0.6521, L1: 4.2712, Perceptual: 7.6403)\nEpoch [15/50], Batch [210/250] | D Loss: 0.6760 | G Loss: 12.5932 (GAN: 0.9861, L1: 3.8448, Perceptual: 7.7624)\nEpoch [15/50], Batch [220/250] | D Loss: 0.5698 | G Loss: 12.6254 (GAN: 0.8605, L1: 4.0168, Perceptual: 7.7482)\nEpoch [15/50], Batch [230/250] | D Loss: 0.5450 | G Loss: 10.6909 (GAN: 0.5476, L1: 3.3116, Perceptual: 6.8317)\nEpoch [15/50], Batch [240/250] | D Loss: 0.7876 | G Loss: 10.6350 (GAN: 0.8063, L1: 2.8618, Perceptual: 6.9668)\nEpoch [15/50], Batch [250/250] | D Loss: 0.6360 | G Loss: 12.8438 (GAN: 0.9065, L1: 4.3702, Perceptual: 7.5670)\nEpoch 15 completed. Current LR G: 0.000200, LR D: 0.000100\n--- Entering save/checkpoint block for epoch 15 ---\nSaving samples and checkpoint for epoch 15...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_15_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [16/50], Batch [10/250] | D Loss: 0.4392 | G Loss: 12.4932 (GAN: 0.7758, L1: 4.0932, Perceptual: 7.6242)\nEpoch [16/50], Batch [20/250] | D Loss: 0.6533 | G Loss: 11.2727 (GAN: 0.7263, L1: 3.5290, Perceptual: 7.0174)\nEpoch [16/50], Batch [30/250] | D Loss: 0.6371 | G Loss: 12.5585 (GAN: 0.8898, L1: 3.8938, Perceptual: 7.7749)\nEpoch [16/50], Batch [40/250] | D Loss: 0.7711 | G Loss: 11.3763 (GAN: 0.7888, L1: 3.4684, Perceptual: 7.1190)\nEpoch [16/50], Batch [50/250] | D Loss: 0.7110 | G Loss: 13.3246 (GAN: 0.8857, L1: 4.5100, Perceptual: 7.9289)\nEpoch [16/50], Batch [60/250] | D Loss: 0.5438 | G Loss: 11.4571 (GAN: 0.8807, L1: 3.3145, Perceptual: 7.2619)\nEpoch [16/50], Batch [70/250] | D Loss: 0.6005 | G Loss: 13.3644 (GAN: 1.3066, L1: 4.0427, Perceptual: 8.0151)\nEpoch [16/50], Batch [80/250] | D Loss: 0.7207 | G Loss: 12.5655 (GAN: 0.9398, L1: 3.6826, Perceptual: 7.9431)\nEpoch [16/50], Batch [90/250] | D Loss: 0.5746 | G Loss: 11.0937 (GAN: 0.9012, L1: 3.3723, Perceptual: 6.8202)\nEpoch [16/50], Batch [100/250] | D Loss: 0.8942 | G Loss: 11.4099 (GAN: 0.6462, L1: 3.6001, Perceptual: 7.1637)\nEpoch [16/50], Batch [110/250] | D Loss: 0.6122 | G Loss: 11.7567 (GAN: 0.8495, L1: 3.5502, Perceptual: 7.3570)\nEpoch [16/50], Batch [120/250] | D Loss: 0.5384 | G Loss: 12.4583 (GAN: 0.9033, L1: 3.7604, Perceptual: 7.7946)\nEpoch [16/50], Batch [130/250] | D Loss: 0.5537 | G Loss: 12.0285 (GAN: 0.6698, L1: 3.8246, Perceptual: 7.5341)\nEpoch [16/50], Batch [140/250] | D Loss: 0.5447 | G Loss: 11.6845 (GAN: 0.9224, L1: 3.6448, Perceptual: 7.1174)\nEpoch [16/50], Batch [150/250] | D Loss: 0.7607 | G Loss: 10.9077 (GAN: 0.7683, L1: 3.4928, Perceptual: 6.6466)\nEpoch [16/50], Batch [160/250] | D Loss: 0.5113 | G Loss: 13.4781 (GAN: 0.8864, L1: 4.0315, Perceptual: 8.5602)\nEpoch [16/50], Batch [170/250] | D Loss: 0.5419 | G Loss: 12.7158 (GAN: 0.8040, L1: 4.2225, Perceptual: 7.6892)\nEpoch [16/50], Batch [180/250] | D Loss: 0.5193 | G Loss: 11.7317 (GAN: 0.6663, L1: 3.8036, Perceptual: 7.2618)\nEpoch [16/50], Batch [190/250] | D Loss: 0.7274 | G Loss: 12.0658 (GAN: 0.9036, L1: 3.7933, Perceptual: 7.3689)\nEpoch [16/50], Batch [200/250] | D Loss: 0.5447 | G Loss: 12.7247 (GAN: 1.0181, L1: 3.9243, Perceptual: 7.7822)\nEpoch [16/50], Batch [210/250] | D Loss: 0.6025 | G Loss: 10.9811 (GAN: 0.7720, L1: 3.3662, Perceptual: 6.8428)\nEpoch [16/50], Batch [220/250] | D Loss: 0.5955 | G Loss: 11.5075 (GAN: 0.8047, L1: 3.3426, Perceptual: 7.3603)\nEpoch [16/50], Batch [230/250] | D Loss: 0.6148 | G Loss: 11.4455 (GAN: 0.6235, L1: 3.6600, Perceptual: 7.1620)\nEpoch [16/50], Batch [240/250] | D Loss: 0.8308 | G Loss: 10.1866 (GAN: 0.7474, L1: 3.1489, Perceptual: 6.2903)\nEpoch [16/50], Batch [250/250] | D Loss: 0.5945 | G Loss: 12.1936 (GAN: 0.9764, L1: 3.4983, Perceptual: 7.7189)\nEpoch 16 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [17/50], Batch [10/250] | D Loss: 0.5654 | G Loss: 12.3604 (GAN: 0.7809, L1: 3.9219, Perceptual: 7.6576)\nEpoch [17/50], Batch [20/250] | D Loss: 0.7230 | G Loss: 13.2289 (GAN: 1.3075, L1: 4.1022, Perceptual: 7.8192)\nEpoch [17/50], Batch [30/250] | D Loss: 0.6061 | G Loss: 12.5160 (GAN: 0.9915, L1: 3.6469, Perceptual: 7.8776)\nEpoch [17/50], Batch [40/250] | D Loss: 0.7416 | G Loss: 12.5460 (GAN: 1.0854, L1: 3.9837, Perceptual: 7.4769)\nEpoch [17/50], Batch [50/250] | D Loss: 0.7531 | G Loss: 11.5479 (GAN: 0.8067, L1: 3.4439, Perceptual: 7.2972)\nEpoch [17/50], Batch [60/250] | D Loss: 0.7394 | G Loss: 11.5022 (GAN: 0.7946, L1: 3.4521, Perceptual: 7.2555)\nEpoch [17/50], Batch [70/250] | D Loss: 0.7680 | G Loss: 11.7772 (GAN: 0.8195, L1: 3.4516, Perceptual: 7.5061)\nEpoch [17/50], Batch [80/250] | D Loss: 0.5216 | G Loss: 12.1171 (GAN: 0.7352, L1: 3.5205, Perceptual: 7.8614)\nEpoch [17/50], Batch [90/250] | D Loss: 0.6713 | G Loss: 11.8192 (GAN: 1.1886, L1: 3.2898, Perceptual: 7.3408)\nEpoch [17/50], Batch [100/250] | D Loss: 0.5927 | G Loss: 11.7729 (GAN: 0.9271, L1: 3.4720, Perceptual: 7.3739)\nEpoch [17/50], Batch [110/250] | D Loss: 0.7363 | G Loss: 11.3671 (GAN: 0.8283, L1: 3.2047, Perceptual: 7.3341)\nEpoch [17/50], Batch [120/250] | D Loss: 0.7713 | G Loss: 11.1782 (GAN: 0.7403, L1: 3.4078, Perceptual: 7.0302)\nEpoch [17/50], Batch [130/250] | D Loss: 0.4686 | G Loss: 11.8559 (GAN: 0.7106, L1: 3.8264, Perceptual: 7.3189)\nEpoch [17/50], Batch [140/250] | D Loss: 0.5243 | G Loss: 13.2303 (GAN: 1.0804, L1: 3.9677, Perceptual: 8.1822)\nEpoch [17/50], Batch [150/250] | D Loss: 0.5502 | G Loss: 12.4485 (GAN: 0.6411, L1: 4.1354, Perceptual: 7.6720)\nEpoch [17/50], Batch [160/250] | D Loss: 0.5951 | G Loss: 12.6258 (GAN: 0.9605, L1: 3.9114, Perceptual: 7.7539)\nEpoch [17/50], Batch [170/250] | D Loss: 0.6730 | G Loss: 12.0891 (GAN: 0.6844, L1: 3.5752, Perceptual: 7.8296)\nEpoch [17/50], Batch [180/250] | D Loss: 0.6224 | G Loss: 12.6219 (GAN: 0.9952, L1: 3.8786, Perceptual: 7.7481)\nEpoch [17/50], Batch [190/250] | D Loss: 0.5131 | G Loss: 11.9492 (GAN: 0.7190, L1: 3.5480, Perceptual: 7.6821)\nEpoch [17/50], Batch [200/250] | D Loss: 0.6855 | G Loss: 12.1755 (GAN: 0.9513, L1: 3.6851, Perceptual: 7.5392)\nEpoch [17/50], Batch [210/250] | D Loss: 0.6799 | G Loss: 11.4087 (GAN: 0.9942, L1: 3.1622, Perceptual: 7.2523)\nEpoch [17/50], Batch [220/250] | D Loss: 0.8125 | G Loss: 11.2183 (GAN: 0.8343, L1: 3.6073, Perceptual: 6.7767)\nEpoch [17/50], Batch [230/250] | D Loss: 0.6518 | G Loss: 10.3869 (GAN: 0.8718, L1: 3.0971, Perceptual: 6.4179)\nEpoch [17/50], Batch [240/250] | D Loss: 0.6141 | G Loss: 12.9037 (GAN: 0.6327, L1: 4.3710, Perceptual: 7.9000)\nEpoch [17/50], Batch [250/250] | D Loss: 0.7203 | G Loss: 11.9744 (GAN: 0.8793, L1: 3.7757, Perceptual: 7.3195)\nEpoch 17 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [18/50], Batch [10/250] | D Loss: 0.5211 | G Loss: 11.2172 (GAN: 0.7232, L1: 3.3557, Perceptual: 7.1384)\nEpoch [18/50], Batch [20/250] | D Loss: 0.6467 | G Loss: 10.5864 (GAN: 0.7530, L1: 3.1705, Perceptual: 6.6629)\nEpoch [18/50], Batch [30/250] | D Loss: 0.5661 | G Loss: 11.6991 (GAN: 0.5630, L1: 3.6325, Perceptual: 7.5036)\nEpoch [18/50], Batch [40/250] | D Loss: 0.5616 | G Loss: 12.8139 (GAN: 0.6780, L1: 4.4038, Perceptual: 7.7321)\nEpoch [18/50], Batch [50/250] | D Loss: 0.5684 | G Loss: 12.5495 (GAN: 0.6500, L1: 4.3114, Perceptual: 7.5882)\nEpoch [18/50], Batch [60/250] | D Loss: 0.7790 | G Loss: 11.0593 (GAN: 1.0674, L1: 3.0097, Perceptual: 6.9822)\nEpoch [18/50], Batch [70/250] | D Loss: 0.6556 | G Loss: 12.5738 (GAN: 1.1931, L1: 3.6173, Perceptual: 7.7634)\nEpoch [18/50], Batch [80/250] | D Loss: 0.6878 | G Loss: 11.9010 (GAN: 0.8452, L1: 3.6795, Perceptual: 7.3764)\nEpoch [18/50], Batch [90/250] | D Loss: 0.5996 | G Loss: 11.3962 (GAN: 0.8498, L1: 3.3986, Perceptual: 7.1478)\nEpoch [18/50], Batch [100/250] | D Loss: 0.5680 | G Loss: 13.4734 (GAN: 0.9338, L1: 4.2234, Perceptual: 8.3161)\nEpoch [18/50], Batch [110/250] | D Loss: 0.5233 | G Loss: 13.0121 (GAN: 1.0378, L1: 4.2666, Perceptual: 7.7077)\nEpoch [18/50], Batch [120/250] | D Loss: 0.4981 | G Loss: 11.6663 (GAN: 0.8932, L1: 3.4075, Perceptual: 7.3656)\nEpoch [18/50], Batch [130/250] | D Loss: 0.6193 | G Loss: 10.9676 (GAN: 0.7713, L1: 3.1963, Perceptual: 7.0000)\nEpoch [18/50], Batch [140/250] | D Loss: 0.6768 | G Loss: 11.5902 (GAN: 0.9225, L1: 3.6235, Perceptual: 7.0442)\nEpoch [18/50], Batch [150/250] | D Loss: 0.6447 | G Loss: 11.0637 (GAN: 0.6754, L1: 3.3920, Perceptual: 6.9963)\nEpoch [18/50], Batch [160/250] | D Loss: 0.7887 | G Loss: 10.8364 (GAN: 0.7414, L1: 3.1923, Perceptual: 6.9027)\nEpoch [18/50], Batch [170/250] | D Loss: 0.6654 | G Loss: 11.7504 (GAN: 0.8687, L1: 3.4880, Perceptual: 7.3937)\nEpoch [18/50], Batch [180/250] | D Loss: 0.6313 | G Loss: 12.8399 (GAN: 0.8730, L1: 3.7734, Perceptual: 8.1935)\nEpoch [18/50], Batch [190/250] | D Loss: 0.6536 | G Loss: 11.9520 (GAN: 0.7315, L1: 3.8490, Perceptual: 7.3715)\nEpoch [18/50], Batch [200/250] | D Loss: 0.5798 | G Loss: 11.8813 (GAN: 0.7609, L1: 4.0082, Perceptual: 7.1122)\nEpoch [18/50], Batch [210/250] | D Loss: 0.5438 | G Loss: 11.2377 (GAN: 0.7534, L1: 3.3477, Perceptual: 7.1366)\nEpoch [18/50], Batch [220/250] | D Loss: 0.7048 | G Loss: 13.0018 (GAN: 0.8630, L1: 4.0840, Perceptual: 8.0548)\nEpoch [18/50], Batch [230/250] | D Loss: 0.8004 | G Loss: 10.5798 (GAN: 0.9038, L1: 3.2756, Perceptual: 6.4004)\nEpoch [18/50], Batch [240/250] | D Loss: 0.4881 | G Loss: 11.3245 (GAN: 0.7521, L1: 3.5694, Perceptual: 7.0030)\nEpoch [18/50], Batch [250/250] | D Loss: 0.5212 | G Loss: 11.2414 (GAN: 0.8446, L1: 3.0795, Perceptual: 7.3172)\nEpoch 18 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [19/50], Batch [10/250] | D Loss: 0.6446 | G Loss: 11.3787 (GAN: 0.7177, L1: 3.5515, Perceptual: 7.1095)\nEpoch [19/50], Batch [20/250] | D Loss: 0.7321 | G Loss: 13.1403 (GAN: 1.1617, L1: 4.0641, Perceptual: 7.9146)\nEpoch [19/50], Batch [30/250] | D Loss: 0.5682 | G Loss: 12.0428 (GAN: 0.7763, L1: 3.7386, Perceptual: 7.5280)\nEpoch [19/50], Batch [40/250] | D Loss: 0.8693 | G Loss: 11.8505 (GAN: 1.2244, L1: 3.4678, Perceptual: 7.1583)\nEpoch [19/50], Batch [50/250] | D Loss: 0.7211 | G Loss: 11.0192 (GAN: 0.6611, L1: 3.3675, Perceptual: 6.9905)\nEpoch [19/50], Batch [60/250] | D Loss: 0.8908 | G Loss: 11.3655 (GAN: 0.7502, L1: 3.8769, Perceptual: 6.7384)\nEpoch [19/50], Batch [70/250] | D Loss: 0.7015 | G Loss: 12.2059 (GAN: 0.8691, L1: 3.8703, Perceptual: 7.4665)\nEpoch [19/50], Batch [80/250] | D Loss: 0.6642 | G Loss: 10.7465 (GAN: 0.8709, L1: 3.2065, Perceptual: 6.6691)\nEpoch [19/50], Batch [90/250] | D Loss: 0.5887 | G Loss: 12.4890 (GAN: 0.9414, L1: 3.7072, Perceptual: 7.8404)\nEpoch [19/50], Batch [100/250] | D Loss: 0.6532 | G Loss: 11.0778 (GAN: 0.9328, L1: 2.9720, Perceptual: 7.1730)\nEpoch [19/50], Batch [110/250] | D Loss: 0.5340 | G Loss: 11.0258 (GAN: 0.7358, L1: 3.4848, Perceptual: 6.8052)\nEpoch [19/50], Batch [120/250] | D Loss: 0.5236 | G Loss: 11.8713 (GAN: 0.9077, L1: 3.2079, Perceptual: 7.7557)\nEpoch [19/50], Batch [130/250] | D Loss: 0.6993 | G Loss: 12.1755 (GAN: 0.9678, L1: 3.6700, Perceptual: 7.5378)\nEpoch [19/50], Batch [140/250] | D Loss: 0.7189 | G Loss: 11.8879 (GAN: 0.9102, L1: 3.9823, Perceptual: 6.9954)\nEpoch [19/50], Batch [150/250] | D Loss: 0.6180 | G Loss: 12.0634 (GAN: 0.8541, L1: 3.6572, Perceptual: 7.5521)\nEpoch [19/50], Batch [160/250] | D Loss: 0.7040 | G Loss: 11.8835 (GAN: 0.8184, L1: 3.8049, Perceptual: 7.2601)\nEpoch [19/50], Batch [170/250] | D Loss: 0.5916 | G Loss: 12.1584 (GAN: 0.6515, L1: 3.6700, Perceptual: 7.8370)\nEpoch [19/50], Batch [180/250] | D Loss: 0.5646 | G Loss: 12.6546 (GAN: 0.7998, L1: 4.1670, Perceptual: 7.6878)\nEpoch [19/50], Batch [190/250] | D Loss: 0.6973 | G Loss: 11.8287 (GAN: 0.8143, L1: 3.5640, Perceptual: 7.4504)\nEpoch [19/50], Batch [200/250] | D Loss: 0.6453 | G Loss: 11.8191 (GAN: 0.8073, L1: 3.5894, Perceptual: 7.4224)\nEpoch [19/50], Batch [210/250] | D Loss: 0.6730 | G Loss: 11.9380 (GAN: 0.8858, L1: 3.5405, Perceptual: 7.5117)\nEpoch [19/50], Batch [220/250] | D Loss: 0.4672 | G Loss: 11.7266 (GAN: 0.6537, L1: 3.4391, Perceptual: 7.6338)\nEpoch [19/50], Batch [230/250] | D Loss: 0.6913 | G Loss: 11.3989 (GAN: 0.9745, L1: 3.2801, Perceptual: 7.1443)\nEpoch [19/50], Batch [240/250] | D Loss: 0.5600 | G Loss: 11.5287 (GAN: 0.5399, L1: 3.5186, Perceptual: 7.4702)\nEpoch [19/50], Batch [250/250] | D Loss: 0.6146 | G Loss: 11.8735 (GAN: 0.7842, L1: 3.4500, Perceptual: 7.6393)\nEpoch 19 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [20/50], Batch [10/250] | D Loss: 0.7412 | G Loss: 11.5134 (GAN: 0.8151, L1: 3.8223, Perceptual: 6.8760)\nEpoch [20/50], Batch [20/250] | D Loss: 0.4620 | G Loss: 12.6768 (GAN: 0.8633, L1: 3.8113, Perceptual: 8.0022)\nEpoch [20/50], Batch [30/250] | D Loss: 0.5909 | G Loss: 13.0131 (GAN: 0.8855, L1: 4.1134, Perceptual: 8.0142)\nEpoch [20/50], Batch [40/250] | D Loss: 0.5692 | G Loss: 11.2334 (GAN: 0.6763, L1: 3.3429, Perceptual: 7.2142)\nEpoch [20/50], Batch [50/250] | D Loss: 0.6073 | G Loss: 11.7898 (GAN: 0.8771, L1: 3.5213, Perceptual: 7.3914)\nEpoch [20/50], Batch [60/250] | D Loss: 0.6442 | G Loss: 11.5546 (GAN: 0.5971, L1: 3.3948, Perceptual: 7.5627)\nEpoch [20/50], Batch [70/250] | D Loss: 0.5697 | G Loss: 12.8197 (GAN: 0.6886, L1: 4.0219, Perceptual: 8.1092)\nEpoch [20/50], Batch [80/250] | D Loss: 0.6733 | G Loss: 11.7351 (GAN: 0.5394, L1: 3.4956, Perceptual: 7.7001)\nEpoch [20/50], Batch [90/250] | D Loss: 0.6504 | G Loss: 12.1977 (GAN: 0.6728, L1: 3.8684, Perceptual: 7.6565)\nEpoch [20/50], Batch [100/250] | D Loss: 0.5066 | G Loss: 11.6039 (GAN: 0.6853, L1: 3.5711, Perceptual: 7.3475)\nEpoch [20/50], Batch [110/250] | D Loss: 0.5177 | G Loss: 12.2669 (GAN: 0.9019, L1: 3.9366, Perceptual: 7.4284)\nEpoch [20/50], Batch [120/250] | D Loss: 0.5902 | G Loss: 11.4386 (GAN: 0.8974, L1: 3.3987, Perceptual: 7.1425)\nEpoch [20/50], Batch [130/250] | D Loss: 0.6938 | G Loss: 11.2959 (GAN: 0.7132, L1: 3.5038, Perceptual: 7.0789)\nEpoch [20/50], Batch [140/250] | D Loss: 0.6214 | G Loss: 12.6443 (GAN: 1.0009, L1: 4.0958, Perceptual: 7.5476)\nEpoch [20/50], Batch [150/250] | D Loss: 0.5116 | G Loss: 12.0302 (GAN: 0.8043, L1: 3.9512, Perceptual: 7.2748)\nEpoch [20/50], Batch [160/250] | D Loss: 0.6010 | G Loss: 10.9952 (GAN: 0.6394, L1: 3.4311, Perceptual: 6.9247)\nEpoch [20/50], Batch [170/250] | D Loss: 0.4975 | G Loss: 11.9424 (GAN: 0.8025, L1: 3.7889, Perceptual: 7.3509)\nEpoch [20/50], Batch [180/250] | D Loss: 0.6118 | G Loss: 12.6496 (GAN: 0.9089, L1: 3.6769, Perceptual: 8.0638)\nEpoch [20/50], Batch [190/250] | D Loss: 0.5946 | G Loss: 11.2658 (GAN: 0.6827, L1: 3.4599, Perceptual: 7.1232)\nEpoch [20/50], Batch [200/250] | D Loss: 0.6372 | G Loss: 13.4484 (GAN: 0.9654, L1: 4.2394, Perceptual: 8.2435)\nEpoch [20/50], Batch [210/250] | D Loss: 0.6285 | G Loss: 11.7703 (GAN: 0.8309, L1: 3.6636, Perceptual: 7.2758)\nEpoch [20/50], Batch [220/250] | D Loss: 0.5555 | G Loss: 11.5470 (GAN: 0.8359, L1: 3.8609, Perceptual: 6.8501)\nEpoch [20/50], Batch [230/250] | D Loss: 0.7622 | G Loss: 10.7010 (GAN: 0.7147, L1: 3.3385, Perceptual: 6.6478)\nEpoch [20/50], Batch [240/250] | D Loss: 0.6629 | G Loss: 11.7715 (GAN: 0.7212, L1: 3.6005, Perceptual: 7.4499)\nEpoch [20/50], Batch [250/250] | D Loss: 0.5349 | G Loss: 11.3613 (GAN: 0.8241, L1: 3.6701, Perceptual: 6.8671)\nEpoch 20 completed. Current LR G: 0.000200, LR D: 0.000100\n--- Entering save/checkpoint block for epoch 20 ---\nSaving samples and checkpoint for epoch 20...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_20_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [21/50], Batch [10/250] | D Loss: 0.8153 | G Loss: 11.0013 (GAN: 0.9851, L1: 3.4533, Perceptual: 6.5628)\nEpoch [21/50], Batch [20/250] | D Loss: 0.6142 | G Loss: 11.4498 (GAN: 0.7976, L1: 3.2778, Perceptual: 7.3743)\nEpoch [21/50], Batch [30/250] | D Loss: 0.5678 | G Loss: 12.0667 (GAN: 0.8913, L1: 4.0610, Perceptual: 7.1144)\nEpoch [21/50], Batch [40/250] | D Loss: 0.7449 | G Loss: 11.6464 (GAN: 0.9415, L1: 3.3396, Perceptual: 7.3653)\nEpoch [21/50], Batch [50/250] | D Loss: 0.5522 | G Loss: 11.6916 (GAN: 0.6813, L1: 3.5135, Perceptual: 7.4969)\nEpoch [21/50], Batch [60/250] | D Loss: 0.4827 | G Loss: 12.6371 (GAN: 0.8510, L1: 4.0840, Perceptual: 7.7021)\nEpoch [21/50], Batch [70/250] | D Loss: 0.7060 | G Loss: 11.5936 (GAN: 0.8645, L1: 3.5561, Perceptual: 7.1730)\nEpoch [21/50], Batch [80/250] | D Loss: 0.6088 | G Loss: 13.4110 (GAN: 0.9945, L1: 4.5913, Perceptual: 7.8251)\nEpoch [21/50], Batch [90/250] | D Loss: 0.7100 | G Loss: 12.6488 (GAN: 0.8964, L1: 4.2555, Perceptual: 7.4969)\nEpoch [21/50], Batch [100/250] | D Loss: 0.6499 | G Loss: 12.2494 (GAN: 0.7793, L1: 3.8903, Perceptual: 7.5799)\nEpoch [21/50], Batch [110/250] | D Loss: 0.7196 | G Loss: 12.0426 (GAN: 0.8845, L1: 3.6769, Perceptual: 7.4812)\nEpoch [21/50], Batch [120/250] | D Loss: 0.5328 | G Loss: 11.8899 (GAN: 0.7583, L1: 3.5551, Perceptual: 7.5765)\nEpoch [21/50], Batch [130/250] | D Loss: 0.4335 | G Loss: 12.8290 (GAN: 0.4948, L1: 4.2867, Perceptual: 8.0474)\nEpoch [21/50], Batch [140/250] | D Loss: 0.5449 | G Loss: 11.7142 (GAN: 0.6897, L1: 3.6807, Perceptual: 7.3438)\nEpoch [21/50], Batch [150/250] | D Loss: 0.6336 | G Loss: 11.6121 (GAN: 0.8128, L1: 3.4306, Perceptual: 7.3687)\nEpoch [21/50], Batch [160/250] | D Loss: 0.6882 | G Loss: 11.7672 (GAN: 0.8176, L1: 3.5537, Perceptual: 7.3959)\nEpoch [21/50], Batch [170/250] | D Loss: 0.6693 | G Loss: 12.9520 (GAN: 0.8063, L1: 4.2149, Perceptual: 7.9308)\nEpoch [21/50], Batch [180/250] | D Loss: 0.5717 | G Loss: 11.7965 (GAN: 0.8867, L1: 3.4996, Perceptual: 7.4101)\nEpoch [21/50], Batch [190/250] | D Loss: 0.5344 | G Loss: 12.1500 (GAN: 0.8462, L1: 3.4007, Perceptual: 7.9031)\nEpoch [21/50], Batch [200/250] | D Loss: 0.6316 | G Loss: 11.5252 (GAN: 1.0465, L1: 3.4104, Perceptual: 7.0683)\nEpoch [21/50], Batch [210/250] | D Loss: 0.4129 | G Loss: 12.6464 (GAN: 0.7781, L1: 3.9721, Perceptual: 7.8962)\nEpoch [21/50], Batch [220/250] | D Loss: 0.6885 | G Loss: 11.2298 (GAN: 0.8628, L1: 3.2899, Perceptual: 7.0770)\nEpoch [21/50], Batch [230/250] | D Loss: 0.6331 | G Loss: 12.0325 (GAN: 0.7351, L1: 3.4179, Perceptual: 7.8795)\nEpoch [21/50], Batch [240/250] | D Loss: 0.6164 | G Loss: 12.6813 (GAN: 0.8644, L1: 4.2211, Perceptual: 7.5958)\nEpoch [21/50], Batch [250/250] | D Loss: 0.8276 | G Loss: 11.0728 (GAN: 1.0158, L1: 3.0887, Perceptual: 6.9682)\nEpoch 21 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [22/50], Batch [10/250] | D Loss: 0.7245 | G Loss: 10.5866 (GAN: 0.7378, L1: 3.1001, Perceptual: 6.7486)\nEpoch [22/50], Batch [20/250] | D Loss: 0.5525 | G Loss: 11.8477 (GAN: 0.7136, L1: 3.7757, Perceptual: 7.3583)\nEpoch [22/50], Batch [30/250] | D Loss: 0.6583 | G Loss: 11.4335 (GAN: 0.8008, L1: 3.5852, Perceptual: 7.0475)\nEpoch [22/50], Batch [40/250] | D Loss: 0.5971 | G Loss: 10.7617 (GAN: 0.9656, L1: 3.0402, Perceptual: 6.7559)\nEpoch [22/50], Batch [50/250] | D Loss: 0.5447 | G Loss: 11.1945 (GAN: 0.6230, L1: 3.3262, Perceptual: 7.2453)\nEpoch [22/50], Batch [60/250] | D Loss: 0.6806 | G Loss: 11.4518 (GAN: 0.6667, L1: 3.4570, Perceptual: 7.3281)\nEpoch [22/50], Batch [70/250] | D Loss: 0.6191 | G Loss: 12.0628 (GAN: 0.8793, L1: 3.4515, Perceptual: 7.7319)\nEpoch [22/50], Batch [80/250] | D Loss: 0.8099 | G Loss: 11.0921 (GAN: 0.9359, L1: 3.1883, Perceptual: 6.9679)\nEpoch [22/50], Batch [90/250] | D Loss: 0.7981 | G Loss: 10.6779 (GAN: 0.9213, L1: 3.1679, Perceptual: 6.5887)\nEpoch [22/50], Batch [100/250] | D Loss: 0.7717 | G Loss: 11.0674 (GAN: 0.7986, L1: 3.2377, Perceptual: 7.0311)\nEpoch [22/50], Batch [110/250] | D Loss: 0.6309 | G Loss: 12.3607 (GAN: 0.9722, L1: 3.8502, Perceptual: 7.5383)\nEpoch [22/50], Batch [120/250] | D Loss: 0.5260 | G Loss: 11.9189 (GAN: 0.6755, L1: 3.6108, Perceptual: 7.6326)\nEpoch [22/50], Batch [130/250] | D Loss: 0.6811 | G Loss: 10.6600 (GAN: 0.7616, L1: 3.0120, Perceptual: 6.8863)\nEpoch [22/50], Batch [140/250] | D Loss: 0.5584 | G Loss: 11.9404 (GAN: 0.8687, L1: 3.4632, Perceptual: 7.6085)\nEpoch [22/50], Batch [150/250] | D Loss: 0.6592 | G Loss: 12.6149 (GAN: 0.9963, L1: 3.3615, Perceptual: 8.2572)\nEpoch [22/50], Batch [160/250] | D Loss: 0.6618 | G Loss: 11.7061 (GAN: 0.8177, L1: 3.5483, Perceptual: 7.3401)\nEpoch [22/50], Batch [170/250] | D Loss: 0.4747 | G Loss: 11.4108 (GAN: 0.5579, L1: 3.8280, Perceptual: 7.0249)\nEpoch [22/50], Batch [180/250] | D Loss: 0.7748 | G Loss: 9.5567 (GAN: 0.7835, L1: 2.7075, Perceptual: 6.0658)\nEpoch [22/50], Batch [190/250] | D Loss: 0.6175 | G Loss: 11.2709 (GAN: 0.5703, L1: 3.3940, Perceptual: 7.3066)\nEpoch [22/50], Batch [200/250] | D Loss: 0.6893 | G Loss: 10.1096 (GAN: 0.6038, L1: 2.8774, Perceptual: 6.6283)\nEpoch [22/50], Batch [210/250] | D Loss: 0.5831 | G Loss: 10.4602 (GAN: 0.8005, L1: 3.0628, Perceptual: 6.5968)\nEpoch [22/50], Batch [220/250] | D Loss: 0.5168 | G Loss: 11.7804 (GAN: 0.7161, L1: 3.8559, Perceptual: 7.2084)\nEpoch [22/50], Batch [230/250] | D Loss: 0.6931 | G Loss: 10.5452 (GAN: 0.6807, L1: 2.9038, Perceptual: 6.9607)\nEpoch [22/50], Batch [240/250] | D Loss: 0.6855 | G Loss: 10.8643 (GAN: 0.7812, L1: 3.2671, Perceptual: 6.8160)\nEpoch [22/50], Batch [250/250] | D Loss: 0.7060 | G Loss: 11.4657 (GAN: 0.8515, L1: 3.4140, Perceptual: 7.2002)\nEpoch 22 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [23/50], Batch [10/250] | D Loss: 0.6004 | G Loss: 11.2568 (GAN: 0.8758, L1: 3.0053, Perceptual: 7.3757)\nEpoch [23/50], Batch [20/250] | D Loss: 0.5928 | G Loss: 11.9059 (GAN: 0.8499, L1: 3.5610, Perceptual: 7.4950)\nEpoch [23/50], Batch [30/250] | D Loss: 0.5398 | G Loss: 12.4687 (GAN: 0.9373, L1: 3.8982, Perceptual: 7.6332)\nEpoch [23/50], Batch [40/250] | D Loss: 0.5509 | G Loss: 11.6514 (GAN: 0.7293, L1: 3.7298, Perceptual: 7.1922)\nEpoch [23/50], Batch [50/250] | D Loss: 0.7209 | G Loss: 10.3769 (GAN: 0.7681, L1: 3.0619, Perceptual: 6.5468)\nEpoch [23/50], Batch [60/250] | D Loss: 0.6606 | G Loss: 12.4712 (GAN: 0.9227, L1: 3.6498, Perceptual: 7.8987)\nEpoch [23/50], Batch [70/250] | D Loss: 0.6742 | G Loss: 11.2964 (GAN: 0.9510, L1: 3.3116, Perceptual: 7.0338)\nEpoch [23/50], Batch [80/250] | D Loss: 0.6977 | G Loss: 11.0250 (GAN: 0.9238, L1: 3.0387, Perceptual: 7.0626)\nEpoch [23/50], Batch [90/250] | D Loss: 0.5081 | G Loss: 12.4690 (GAN: 0.8669, L1: 3.6451, Perceptual: 7.9570)\nEpoch [23/50], Batch [100/250] | D Loss: 0.8400 | G Loss: 11.9723 (GAN: 1.2134, L1: 3.7970, Perceptual: 6.9620)\nEpoch [23/50], Batch [110/250] | D Loss: 0.4776 | G Loss: 11.6372 (GAN: 1.0099, L1: 3.4050, Perceptual: 7.2222)\nEpoch [23/50], Batch [120/250] | D Loss: 0.5991 | G Loss: 11.3313 (GAN: 0.6403, L1: 3.7502, Perceptual: 6.9407)\nEpoch [23/50], Batch [130/250] | D Loss: 0.6694 | G Loss: 11.4758 (GAN: 0.6676, L1: 3.4136, Perceptual: 7.3946)\nEpoch [23/50], Batch [140/250] | D Loss: 0.6011 | G Loss: 11.6256 (GAN: 0.8242, L1: 3.4059, Perceptual: 7.3955)\nEpoch [23/50], Batch [150/250] | D Loss: 0.5384 | G Loss: 11.2216 (GAN: 0.6446, L1: 3.4893, Perceptual: 7.0876)\nEpoch [23/50], Batch [160/250] | D Loss: 0.6054 | G Loss: 12.0777 (GAN: 1.0031, L1: 3.6115, Perceptual: 7.4631)\nEpoch [23/50], Batch [170/250] | D Loss: 0.5601 | G Loss: 11.3578 (GAN: 0.6704, L1: 3.5617, Perceptual: 7.1257)\nEpoch [23/50], Batch [180/250] | D Loss: 0.7149 | G Loss: 10.9579 (GAN: 0.6852, L1: 3.2339, Perceptual: 7.0389)\nEpoch [23/50], Batch [190/250] | D Loss: 0.6198 | G Loss: 12.0972 (GAN: 0.9184, L1: 3.8070, Perceptual: 7.3719)\nEpoch [23/50], Batch [200/250] | D Loss: 0.6428 | G Loss: 10.7106 (GAN: 0.6595, L1: 3.0766, Perceptual: 6.9746)\nEpoch [23/50], Batch [210/250] | D Loss: 0.6647 | G Loss: 11.6181 (GAN: 0.7616, L1: 3.5367, Perceptual: 7.3197)\nEpoch [23/50], Batch [220/250] | D Loss: 0.5328 | G Loss: 11.2644 (GAN: 0.9879, L1: 3.3344, Perceptual: 6.9421)\nEpoch [23/50], Batch [230/250] | D Loss: 0.6421 | G Loss: 11.3567 (GAN: 0.8456, L1: 3.2041, Perceptual: 7.3070)\nEpoch [23/50], Batch [240/250] | D Loss: 0.5326 | G Loss: 12.1535 (GAN: 0.7541, L1: 4.0382, Perceptual: 7.3612)\nEpoch [23/50], Batch [250/250] | D Loss: 0.7650 | G Loss: 11.1139 (GAN: 0.8261, L1: 3.2870, Perceptual: 7.0009)\nEpoch 23 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [24/50], Batch [10/250] | D Loss: 0.5633 | G Loss: 14.1764 (GAN: 0.8925, L1: 4.8833, Perceptual: 8.4006)\nEpoch [24/50], Batch [20/250] | D Loss: 0.5581 | G Loss: 12.0679 (GAN: 0.7696, L1: 3.8889, Perceptual: 7.4094)\nEpoch [24/50], Batch [30/250] | D Loss: 0.6423 | G Loss: 10.9842 (GAN: 0.7337, L1: 3.0974, Perceptual: 7.1530)\nEpoch [24/50], Batch [40/250] | D Loss: 0.4236 | G Loss: 11.5972 (GAN: 0.8291, L1: 3.4547, Perceptual: 7.3134)\nEpoch [24/50], Batch [50/250] | D Loss: 0.5110 | G Loss: 12.8886 (GAN: 0.8441, L1: 4.2168, Perceptual: 7.8277)\nEpoch [24/50], Batch [60/250] | D Loss: 0.8692 | G Loss: 10.6579 (GAN: 0.8584, L1: 3.3457, Perceptual: 6.4538)\nEpoch [24/50], Batch [70/250] | D Loss: 0.5262 | G Loss: 12.5041 (GAN: 0.5865, L1: 3.9183, Perceptual: 7.9993)\nEpoch [24/50], Batch [80/250] | D Loss: 0.5005 | G Loss: 12.2933 (GAN: 0.7560, L1: 3.8481, Perceptual: 7.6893)\nEpoch [24/50], Batch [90/250] | D Loss: 0.6176 | G Loss: 11.6674 (GAN: 0.6997, L1: 3.6550, Perceptual: 7.3128)\nEpoch [24/50], Batch [100/250] | D Loss: 0.6245 | G Loss: 11.6446 (GAN: 0.6879, L1: 3.5747, Perceptual: 7.3820)\nEpoch [24/50], Batch [110/250] | D Loss: 0.5948 | G Loss: 11.9238 (GAN: 0.9294, L1: 3.3472, Perceptual: 7.6471)\nEpoch [24/50], Batch [120/250] | D Loss: 0.8120 | G Loss: 9.4790 (GAN: 0.5682, L1: 2.8680, Perceptual: 6.0429)\nEpoch [24/50], Batch [130/250] | D Loss: 0.6762 | G Loss: 11.8920 (GAN: 0.8316, L1: 3.6533, Perceptual: 7.4071)\nEpoch [24/50], Batch [140/250] | D Loss: 0.7184 | G Loss: 11.1328 (GAN: 0.6321, L1: 3.7455, Perceptual: 6.7552)\nEpoch [24/50], Batch [150/250] | D Loss: 0.4836 | G Loss: 11.8823 (GAN: 0.8091, L1: 3.4647, Perceptual: 7.6086)\nEpoch [24/50], Batch [160/250] | D Loss: 0.7526 | G Loss: 10.5232 (GAN: 0.8515, L1: 2.9701, Perceptual: 6.7016)\nEpoch [24/50], Batch [170/250] | D Loss: 0.6214 | G Loss: 11.7685 (GAN: 0.8617, L1: 3.9534, Perceptual: 6.9535)\nEpoch [24/50], Batch [180/250] | D Loss: 0.5497 | G Loss: 12.3597 (GAN: 0.8696, L1: 3.6726, Perceptual: 7.8175)\nEpoch [24/50], Batch [190/250] | D Loss: 0.3899 | G Loss: 12.2370 (GAN: 0.6002, L1: 4.0693, Perceptual: 7.5675)\nEpoch [24/50], Batch [200/250] | D Loss: 0.6217 | G Loss: 13.0676 (GAN: 1.0794, L1: 3.7496, Perceptual: 8.2386)\nEpoch [24/50], Batch [210/250] | D Loss: 0.5632 | G Loss: 11.2158 (GAN: 0.8080, L1: 3.4204, Perceptual: 6.9874)\nEpoch [24/50], Batch [220/250] | D Loss: 1.0596 | G Loss: 10.5924 (GAN: 1.1125, L1: 3.0498, Perceptual: 6.4301)\nEpoch [24/50], Batch [230/250] | D Loss: 0.5454 | G Loss: 12.0958 (GAN: 0.9685, L1: 3.7286, Perceptual: 7.3987)\nEpoch [24/50], Batch [240/250] | D Loss: 0.7146 | G Loss: 11.0114 (GAN: 0.7763, L1: 3.4239, Perceptual: 6.8112)\nEpoch [24/50], Batch [250/250] | D Loss: 0.6435 | G Loss: 12.1799 (GAN: 0.7803, L1: 3.6840, Perceptual: 7.7156)\nEpoch 24 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [25/50], Batch [10/250] | D Loss: 0.4977 | G Loss: 11.2709 (GAN: 0.6264, L1: 3.4649, Perceptual: 7.1796)\nEpoch [25/50], Batch [20/250] | D Loss: 0.6520 | G Loss: 11.3490 (GAN: 0.7401, L1: 3.4549, Perceptual: 7.1540)\nEpoch [25/50], Batch [30/250] | D Loss: 0.6809 | G Loss: 12.5914 (GAN: 1.0423, L1: 3.7012, Perceptual: 7.8478)\nEpoch [25/50], Batch [40/250] | D Loss: 0.6629 | G Loss: 9.9099 (GAN: 0.5639, L1: 2.9746, Perceptual: 6.3714)\nEpoch [25/50], Batch [50/250] | D Loss: 0.6444 | G Loss: 11.3028 (GAN: 0.7145, L1: 3.4238, Perceptual: 7.1645)\nEpoch [25/50], Batch [60/250] | D Loss: 0.6209 | G Loss: 10.0185 (GAN: 0.5299, L1: 3.3196, Perceptual: 6.1691)\nEpoch [25/50], Batch [70/250] | D Loss: 0.5610 | G Loss: 12.8460 (GAN: 1.0153, L1: 3.8741, Perceptual: 7.9566)\nEpoch [25/50], Batch [80/250] | D Loss: 0.7099 | G Loss: 11.2442 (GAN: 1.0730, L1: 3.2897, Perceptual: 6.8815)\nEpoch [25/50], Batch [90/250] | D Loss: 0.7182 | G Loss: 11.0031 (GAN: 0.6969, L1: 3.5635, Perceptual: 6.7428)\nEpoch [25/50], Batch [100/250] | D Loss: 0.7204 | G Loss: 11.2949 (GAN: 0.7592, L1: 3.4943, Perceptual: 7.0415)\nEpoch [25/50], Batch [110/250] | D Loss: 0.5593 | G Loss: 12.3185 (GAN: 0.6861, L1: 4.0174, Perceptual: 7.6150)\nEpoch [25/50], Batch [120/250] | D Loss: 0.5253 | G Loss: 11.5893 (GAN: 0.6248, L1: 3.6582, Perceptual: 7.3063)\nEpoch [25/50], Batch [130/250] | D Loss: 0.7312 | G Loss: 12.7670 (GAN: 0.9511, L1: 4.2474, Perceptual: 7.5685)\nEpoch [25/50], Batch [140/250] | D Loss: 0.7186 | G Loss: 10.4014 (GAN: 0.8311, L1: 2.8211, Perceptual: 6.7491)\nEpoch [25/50], Batch [150/250] | D Loss: 0.6548 | G Loss: 10.7138 (GAN: 0.8403, L1: 2.9757, Perceptual: 6.8978)\nEpoch [25/50], Batch [160/250] | D Loss: 0.6096 | G Loss: 11.3889 (GAN: 1.0886, L1: 3.2749, Perceptual: 7.0254)\nEpoch [25/50], Batch [170/250] | D Loss: 0.5971 | G Loss: 10.4448 (GAN: 0.7828, L1: 3.1309, Perceptual: 6.5310)\nEpoch [25/50], Batch [180/250] | D Loss: 0.6344 | G Loss: 12.5253 (GAN: 1.0352, L1: 3.8382, Perceptual: 7.6519)\nEpoch [25/50], Batch [190/250] | D Loss: 0.6481 | G Loss: 11.9225 (GAN: 0.9066, L1: 3.7299, Perceptual: 7.2860)\nEpoch [25/50], Batch [200/250] | D Loss: 0.6087 | G Loss: 13.5253 (GAN: 0.8986, L1: 4.3461, Perceptual: 8.2806)\nEpoch [25/50], Batch [210/250] | D Loss: 0.6501 | G Loss: 11.8779 (GAN: 0.8141, L1: 3.5703, Perceptual: 7.4934)\nEpoch [25/50], Batch [220/250] | D Loss: 0.5541 | G Loss: 12.7622 (GAN: 0.7359, L1: 4.1556, Perceptual: 7.8707)\nEpoch [25/50], Batch [230/250] | D Loss: 0.5926 | G Loss: 10.9890 (GAN: 0.7430, L1: 3.1284, Perceptual: 7.1175)\nEpoch [25/50], Batch [240/250] | D Loss: 0.5996 | G Loss: 10.6298 (GAN: 0.6285, L1: 3.2268, Perceptual: 6.7744)\nEpoch [25/50], Batch [250/250] | D Loss: 0.5416 | G Loss: 11.5107 (GAN: 0.6655, L1: 3.7706, Perceptual: 7.0746)\nEpoch 25 completed. Current LR G: 0.000200, LR D: 0.000100\n--- Entering save/checkpoint block for epoch 25 ---\nSaving samples and checkpoint for epoch 25...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_25_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [26/50], Batch [10/250] | D Loss: 0.7221 | G Loss: 10.7064 (GAN: 0.7661, L1: 3.0215, Perceptual: 6.9189)\nEpoch [26/50], Batch [20/250] | D Loss: 0.6216 | G Loss: 12.2106 (GAN: 0.7045, L1: 4.2458, Perceptual: 7.2603)\nEpoch [26/50], Batch [30/250] | D Loss: 0.5687 | G Loss: 11.8519 (GAN: 0.8272, L1: 3.5367, Perceptual: 7.4879)\nEpoch [26/50], Batch [40/250] | D Loss: 0.6203 | G Loss: 11.8689 (GAN: 0.7566, L1: 3.7167, Perceptual: 7.3956)\nEpoch [26/50], Batch [50/250] | D Loss: 0.4683 | G Loss: 12.8175 (GAN: 0.9746, L1: 3.8749, Perceptual: 7.9679)\nEpoch [26/50], Batch [60/250] | D Loss: 0.6710 | G Loss: 11.4447 (GAN: 0.9142, L1: 3.5817, Perceptual: 6.9487)\nEpoch [26/50], Batch [70/250] | D Loss: 0.7009 | G Loss: 11.8574 (GAN: 0.8174, L1: 3.3461, Perceptual: 7.6940)\nEpoch [26/50], Batch [80/250] | D Loss: 0.5358 | G Loss: 11.4442 (GAN: 0.7907, L1: 3.5749, Perceptual: 7.0787)\nEpoch [26/50], Batch [90/250] | D Loss: 0.6941 | G Loss: 11.6040 (GAN: 0.9427, L1: 3.4505, Perceptual: 7.2108)\nEpoch [26/50], Batch [100/250] | D Loss: 0.5195 | G Loss: 12.0505 (GAN: 0.7070, L1: 3.7489, Perceptual: 7.5946)\nEpoch [26/50], Batch [110/250] | D Loss: 0.7752 | G Loss: 11.0899 (GAN: 0.9245, L1: 3.3396, Perceptual: 6.8258)\nEpoch [26/50], Batch [120/250] | D Loss: 0.9374 | G Loss: 11.2997 (GAN: 1.0146, L1: 3.3114, Perceptual: 6.9737)\nEpoch [26/50], Batch [130/250] | D Loss: 0.5437 | G Loss: 12.2187 (GAN: 0.8000, L1: 3.9866, Perceptual: 7.4321)\nEpoch [26/50], Batch [140/250] | D Loss: 0.6835 | G Loss: 11.6321 (GAN: 0.8084, L1: 3.4690, Perceptual: 7.3547)\nEpoch [26/50], Batch [150/250] | D Loss: 0.7265 | G Loss: 11.6181 (GAN: 0.9671, L1: 3.5861, Perceptual: 7.0649)\nEpoch [26/50], Batch [160/250] | D Loss: 0.6279 | G Loss: 10.9642 (GAN: 0.7349, L1: 3.0675, Perceptual: 7.1618)\nEpoch [26/50], Batch [170/250] | D Loss: 0.5143 | G Loss: 12.9924 (GAN: 1.0280, L1: 3.7894, Perceptual: 8.1750)\nEpoch [26/50], Batch [180/250] | D Loss: 0.5634 | G Loss: 11.3770 (GAN: 0.8111, L1: 3.6152, Perceptual: 6.9508)\nEpoch [26/50], Batch [190/250] | D Loss: 0.5462 | G Loss: 11.8284 (GAN: 0.7852, L1: 3.7052, Perceptual: 7.3381)\nEpoch [26/50], Batch [200/250] | D Loss: 0.5796 | G Loss: 11.4659 (GAN: 0.6526, L1: 3.9476, Perceptual: 6.8658)\nEpoch [26/50], Batch [210/250] | D Loss: 0.7124 | G Loss: 10.6975 (GAN: 0.8337, L1: 3.1086, Perceptual: 6.7552)\nEpoch [26/50], Batch [220/250] | D Loss: 0.6465 | G Loss: 10.8399 (GAN: 0.7805, L1: 3.2411, Perceptual: 6.8183)\nEpoch [26/50], Batch [230/250] | D Loss: 0.5369 | G Loss: 12.7208 (GAN: 0.9354, L1: 3.8436, Perceptual: 7.9418)\nEpoch [26/50], Batch [240/250] | D Loss: 0.5954 | G Loss: 11.7817 (GAN: 0.8622, L1: 3.8689, Perceptual: 7.0506)\nEpoch [26/50], Batch [250/250] | D Loss: 0.6741 | G Loss: 11.1190 (GAN: 0.8537, L1: 3.2213, Perceptual: 7.0441)\nEpoch 26 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [27/50], Batch [10/250] | D Loss: 0.6532 | G Loss: 11.4004 (GAN: 0.9082, L1: 3.3213, Perceptual: 7.1709)\nEpoch [27/50], Batch [20/250] | D Loss: 0.5999 | G Loss: 10.9662 (GAN: 0.7764, L1: 3.2674, Perceptual: 6.9224)\nEpoch [27/50], Batch [30/250] | D Loss: 0.6425 | G Loss: 11.8448 (GAN: 0.9634, L1: 3.7263, Perceptual: 7.1551)\nEpoch [27/50], Batch [40/250] | D Loss: 0.8700 | G Loss: 12.3491 (GAN: 0.9130, L1: 3.9552, Perceptual: 7.4809)\nEpoch [27/50], Batch [50/250] | D Loss: 0.6255 | G Loss: 12.6127 (GAN: 1.0503, L1: 3.9748, Perceptual: 7.5876)\nEpoch [27/50], Batch [60/250] | D Loss: 0.5218 | G Loss: 12.2549 (GAN: 0.7469, L1: 3.8586, Perceptual: 7.6495)\nEpoch [27/50], Batch [70/250] | D Loss: 0.6150 | G Loss: 10.6493 (GAN: 0.8857, L1: 3.1049, Perceptual: 6.6587)\nEpoch [27/50], Batch [80/250] | D Loss: 0.6068 | G Loss: 11.1898 (GAN: 0.7247, L1: 3.2823, Perceptual: 7.1828)\nEpoch [27/50], Batch [90/250] | D Loss: 0.5035 | G Loss: 12.4633 (GAN: 0.9390, L1: 3.6540, Perceptual: 7.8703)\nEpoch [27/50], Batch [100/250] | D Loss: 0.6557 | G Loss: 11.8952 (GAN: 0.8277, L1: 3.6870, Perceptual: 7.3805)\nEpoch [27/50], Batch [110/250] | D Loss: 0.5495 | G Loss: 10.8851 (GAN: 0.7250, L1: 3.0326, Perceptual: 7.1274)\nEpoch [27/50], Batch [120/250] | D Loss: 0.6934 | G Loss: 11.5704 (GAN: 0.9639, L1: 3.5159, Perceptual: 7.0906)\nEpoch [27/50], Batch [130/250] | D Loss: 0.7092 | G Loss: 10.6925 (GAN: 0.7081, L1: 3.0479, Perceptual: 6.9365)\nEpoch [27/50], Batch [140/250] | D Loss: 0.7223 | G Loss: 10.0920 (GAN: 0.7225, L1: 2.7961, Perceptual: 6.5734)\nEpoch [27/50], Batch [150/250] | D Loss: 0.6012 | G Loss: 11.3613 (GAN: 0.7833, L1: 3.6147, Perceptual: 6.9632)\nEpoch [27/50], Batch [160/250] | D Loss: 0.6364 | G Loss: 11.2798 (GAN: 0.7240, L1: 3.2381, Perceptual: 7.3178)\nEpoch [27/50], Batch [170/250] | D Loss: 0.7344 | G Loss: 10.6232 (GAN: 0.8100, L1: 2.9910, Perceptual: 6.8222)\nEpoch [27/50], Batch [180/250] | D Loss: 0.7184 | G Loss: 11.4926 (GAN: 0.8342, L1: 3.4414, Perceptual: 7.2170)\nEpoch [27/50], Batch [190/250] | D Loss: 0.5643 | G Loss: 12.0928 (GAN: 0.8336, L1: 3.4787, Perceptual: 7.7805)\nEpoch [27/50], Batch [200/250] | D Loss: 0.8085 | G Loss: 11.7954 (GAN: 0.8758, L1: 3.6112, Perceptual: 7.3084)\nEpoch [27/50], Batch [210/250] | D Loss: 0.6182 | G Loss: 12.2023 (GAN: 0.9996, L1: 3.4291, Perceptual: 7.7736)\nEpoch [27/50], Batch [220/250] | D Loss: 0.6887 | G Loss: 11.6083 (GAN: 0.9355, L1: 3.4759, Perceptual: 7.1969)\nEpoch [27/50], Batch [230/250] | D Loss: 0.9355 | G Loss: 10.8593 (GAN: 0.9530, L1: 3.0516, Perceptual: 6.8547)\nEpoch [27/50], Batch [240/250] | D Loss: 0.5553 | G Loss: 12.2465 (GAN: 0.9619, L1: 3.5515, Perceptual: 7.7332)\nEpoch [27/50], Batch [250/250] | D Loss: 0.8096 | G Loss: 10.3935 (GAN: 0.8956, L1: 2.8975, Perceptual: 6.6004)\nEpoch 27 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [28/50], Batch [10/250] | D Loss: 0.6049 | G Loss: 10.7419 (GAN: 0.7990, L1: 3.2429, Perceptual: 6.7001)\nEpoch [28/50], Batch [20/250] | D Loss: 0.5956 | G Loss: 11.3594 (GAN: 0.6205, L1: 3.7122, Perceptual: 7.0268)\nEpoch [28/50], Batch [30/250] | D Loss: 0.4697 | G Loss: 11.8416 (GAN: 0.8828, L1: 3.5287, Perceptual: 7.4301)\nEpoch [28/50], Batch [40/250] | D Loss: 0.4758 | G Loss: 13.4914 (GAN: 0.9048, L1: 4.5640, Perceptual: 8.0226)\nEpoch [28/50], Batch [50/250] | D Loss: 0.7596 | G Loss: 12.3658 (GAN: 0.7269, L1: 3.7207, Perceptual: 7.9181)\nEpoch [28/50], Batch [60/250] | D Loss: 0.5058 | G Loss: 11.5100 (GAN: 0.7966, L1: 3.5602, Perceptual: 7.1532)\nEpoch [28/50], Batch [70/250] | D Loss: 0.6033 | G Loss: 11.2221 (GAN: 0.6862, L1: 3.2874, Perceptual: 7.2484)\nEpoch [28/50], Batch [80/250] | D Loss: 0.6763 | G Loss: 11.1847 (GAN: 1.0184, L1: 3.4799, Perceptual: 6.6865)\nEpoch [28/50], Batch [90/250] | D Loss: 0.5991 | G Loss: 11.5783 (GAN: 0.8306, L1: 3.4873, Perceptual: 7.2603)\nEpoch [28/50], Batch [100/250] | D Loss: 0.6066 | G Loss: 11.6997 (GAN: 0.5865, L1: 3.5496, Perceptual: 7.5635)\nEpoch [28/50], Batch [110/250] | D Loss: 0.6840 | G Loss: 13.3343 (GAN: 1.1140, L1: 4.3762, Perceptual: 7.8441)\nEpoch [28/50], Batch [120/250] | D Loss: 0.5386 | G Loss: 11.7613 (GAN: 0.8457, L1: 3.4406, Perceptual: 7.4750)\nEpoch [28/50], Batch [130/250] | D Loss: 0.6971 | G Loss: 10.3844 (GAN: 0.7246, L1: 3.0118, Perceptual: 6.6480)\nEpoch [28/50], Batch [140/250] | D Loss: 0.5666 | G Loss: 11.5129 (GAN: 0.8947, L1: 3.5865, Perceptual: 7.0317)\nEpoch [28/50], Batch [150/250] | D Loss: 0.5722 | G Loss: 11.8055 (GAN: 0.9852, L1: 3.3573, Perceptual: 7.4630)\nEpoch [28/50], Batch [160/250] | D Loss: 0.4793 | G Loss: 12.4845 (GAN: 0.9020, L1: 3.9574, Perceptual: 7.6251)\nEpoch [28/50], Batch [170/250] | D Loss: 0.6399 | G Loss: 10.3191 (GAN: 0.6261, L1: 3.0083, Perceptual: 6.6846)\nEpoch [28/50], Batch [180/250] | D Loss: 0.7352 | G Loss: 10.2383 (GAN: 0.8089, L1: 2.7824, Perceptual: 6.6470)\nEpoch [28/50], Batch [190/250] | D Loss: 0.6707 | G Loss: 11.1265 (GAN: 0.7766, L1: 3.4790, Perceptual: 6.8710)\nEpoch [28/50], Batch [200/250] | D Loss: 0.6038 | G Loss: 11.0859 (GAN: 0.8614, L1: 3.2954, Perceptual: 6.9290)\nEpoch [28/50], Batch [210/250] | D Loss: 0.5357 | G Loss: 11.5826 (GAN: 0.6392, L1: 3.4724, Perceptual: 7.4711)\nEpoch [28/50], Batch [220/250] | D Loss: 0.6026 | G Loss: 10.5888 (GAN: 0.7125, L1: 3.1585, Perceptual: 6.7177)\nEpoch [28/50], Batch [230/250] | D Loss: 0.7144 | G Loss: 11.2555 (GAN: 0.7377, L1: 3.2264, Perceptual: 7.2914)\nEpoch [28/50], Batch [240/250] | D Loss: 0.7635 | G Loss: 11.3716 (GAN: 1.0539, L1: 3.2533, Perceptual: 7.0645)\nEpoch [28/50], Batch [250/250] | D Loss: 0.5198 | G Loss: 12.1102 (GAN: 1.0108, L1: 3.5597, Perceptual: 7.5397)\nEpoch 28 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [29/50], Batch [10/250] | D Loss: 0.7157 | G Loss: 10.6670 (GAN: 0.8092, L1: 3.1263, Perceptual: 6.7315)\nEpoch [29/50], Batch [20/250] | D Loss: 0.7573 | G Loss: 11.4909 (GAN: 0.7359, L1: 3.4231, Perceptual: 7.3319)\nEpoch [29/50], Batch [30/250] | D Loss: 0.5296 | G Loss: 10.9112 (GAN: 0.5958, L1: 3.4886, Perceptual: 6.8268)\nEpoch [29/50], Batch [40/250] | D Loss: 0.6768 | G Loss: 11.1063 (GAN: 0.6024, L1: 3.7384, Perceptual: 6.7655)\nEpoch [29/50], Batch [50/250] | D Loss: 0.7081 | G Loss: 10.1486 (GAN: 0.7302, L1: 2.9496, Perceptual: 6.4689)\nEpoch [29/50], Batch [60/250] | D Loss: 0.6601 | G Loss: 11.3445 (GAN: 0.8345, L1: 3.2292, Perceptual: 7.2809)\nEpoch [29/50], Batch [70/250] | D Loss: 0.6143 | G Loss: 13.0451 (GAN: 0.7351, L1: 4.2828, Perceptual: 8.0272)\nEpoch [29/50], Batch [80/250] | D Loss: 0.6928 | G Loss: 10.9131 (GAN: 0.6812, L1: 3.2310, Perceptual: 7.0009)\nEpoch [29/50], Batch [90/250] | D Loss: 0.6436 | G Loss: 11.0533 (GAN: 0.7114, L1: 3.3628, Perceptual: 6.9791)\nEpoch [29/50], Batch [100/250] | D Loss: 0.6038 | G Loss: 11.1093 (GAN: 0.7001, L1: 3.2675, Perceptual: 7.1416)\nEpoch [29/50], Batch [110/250] | D Loss: 0.5467 | G Loss: 11.6692 (GAN: 0.6678, L1: 3.7747, Perceptual: 7.2267)\nEpoch [29/50], Batch [120/250] | D Loss: 0.4987 | G Loss: 11.5538 (GAN: 0.7658, L1: 3.3436, Perceptual: 7.4444)\nEpoch [29/50], Batch [130/250] | D Loss: 0.6596 | G Loss: 11.5384 (GAN: 0.7305, L1: 3.7316, Perceptual: 7.0762)\nEpoch [29/50], Batch [140/250] | D Loss: 0.6670 | G Loss: 10.2932 (GAN: 0.7065, L1: 3.2598, Perceptual: 6.3270)\nEpoch [29/50], Batch [150/250] | D Loss: 0.6678 | G Loss: 12.6210 (GAN: 0.8690, L1: 3.9352, Perceptual: 7.8168)\nEpoch [29/50], Batch [160/250] | D Loss: 0.5261 | G Loss: 11.4115 (GAN: 0.7393, L1: 3.6052, Perceptual: 7.0670)\nEpoch [29/50], Batch [170/250] | D Loss: 0.5221 | G Loss: 13.0207 (GAN: 0.8143, L1: 4.5932, Perceptual: 7.6132)\nEpoch [29/50], Batch [180/250] | D Loss: 0.7026 | G Loss: 11.4146 (GAN: 0.9150, L1: 3.4472, Perceptual: 7.0524)\nEpoch [29/50], Batch [190/250] | D Loss: 0.7297 | G Loss: 10.4835 (GAN: 0.9513, L1: 2.9429, Perceptual: 6.5893)\nEpoch [29/50], Batch [200/250] | D Loss: 0.5717 | G Loss: 11.5263 (GAN: 0.7875, L1: 3.3460, Perceptual: 7.3928)\nEpoch [29/50], Batch [210/250] | D Loss: 0.5094 | G Loss: 11.2462 (GAN: 0.8172, L1: 3.1602, Perceptual: 7.2688)\nEpoch [29/50], Batch [220/250] | D Loss: 0.6963 | G Loss: 11.7962 (GAN: 0.9103, L1: 3.7104, Perceptual: 7.1755)\nEpoch [29/50], Batch [230/250] | D Loss: 0.6251 | G Loss: 11.0552 (GAN: 0.8592, L1: 3.2640, Perceptual: 6.9321)\nEpoch [29/50], Batch [240/250] | D Loss: 0.4906 | G Loss: 10.9379 (GAN: 0.6977, L1: 3.3336, Perceptual: 6.9066)\nEpoch [29/50], Batch [250/250] | D Loss: 0.5171 | G Loss: 10.7499 (GAN: 0.7462, L1: 3.1766, Perceptual: 6.8270)\nEpoch 29 completed. Current LR G: 0.000200, LR D: 0.000100\nEpoch [30/50], Batch [10/250] | D Loss: 0.7121 | G Loss: 10.5454 (GAN: 0.7839, L1: 2.9655, Perceptual: 6.7959)\nEpoch [30/50], Batch [20/250] | D Loss: 0.6989 | G Loss: 11.9454 (GAN: 1.0867, L1: 3.8516, Perceptual: 7.0071)\nEpoch [30/50], Batch [30/250] | D Loss: 0.7517 | G Loss: 10.8196 (GAN: 0.7356, L1: 3.1667, Perceptual: 6.9173)\nEpoch [30/50], Batch [40/250] | D Loss: 0.6980 | G Loss: 10.2697 (GAN: 0.7222, L1: 3.1313, Perceptual: 6.4162)\nEpoch [30/50], Batch [50/250] | D Loss: 0.6949 | G Loss: 11.2350 (GAN: 0.8907, L1: 3.0487, Perceptual: 7.2955)\nEpoch [30/50], Batch [60/250] | D Loss: 0.5545 | G Loss: 10.8491 (GAN: 0.8271, L1: 3.0479, Perceptual: 6.9741)\nEpoch [30/50], Batch [70/250] | D Loss: 0.7090 | G Loss: 12.1338 (GAN: 0.8337, L1: 3.8821, Perceptual: 7.4180)\nEpoch [30/50], Batch [80/250] | D Loss: 0.5658 | G Loss: 11.7023 (GAN: 1.0266, L1: 3.5630, Perceptual: 7.1127)\nEpoch [30/50], Batch [90/250] | D Loss: 0.5250 | G Loss: 11.6573 (GAN: 0.8786, L1: 3.4079, Perceptual: 7.3708)\nEpoch [30/50], Batch [100/250] | D Loss: 0.6921 | G Loss: 11.9470 (GAN: 0.9021, L1: 3.5384, Perceptual: 7.5065)\nEpoch [30/50], Batch [110/250] | D Loss: 0.6312 | G Loss: 11.4968 (GAN: 0.8537, L1: 3.3043, Perceptual: 7.3388)\nEpoch [30/50], Batch [120/250] | D Loss: 0.6420 | G Loss: 12.3165 (GAN: 1.1360, L1: 3.6293, Perceptual: 7.5512)\nEpoch [30/50], Batch [130/250] | D Loss: 0.5468 | G Loss: 11.4685 (GAN: 0.9644, L1: 3.2778, Perceptual: 7.2263)\nEpoch [30/50], Batch [140/250] | D Loss: 0.6613 | G Loss: 10.8288 (GAN: 0.8759, L1: 3.2676, Perceptual: 6.6853)\nEpoch [30/50], Batch [150/250] | D Loss: 0.6510 | G Loss: 11.5947 (GAN: 0.7112, L1: 3.2724, Perceptual: 7.6111)\nEpoch [30/50], Batch [160/250] | D Loss: 0.6198 | G Loss: 11.6699 (GAN: 0.9205, L1: 3.5577, Perceptual: 7.1916)\nEpoch [30/50], Batch [170/250] | D Loss: 0.5335 | G Loss: 12.5165 (GAN: 0.8300, L1: 3.4084, Perceptual: 8.2781)\nEpoch [30/50], Batch [180/250] | D Loss: 0.7228 | G Loss: 11.0881 (GAN: 0.8982, L1: 3.3139, Perceptual: 6.8760)\nEpoch [30/50], Batch [190/250] | D Loss: 0.6806 | G Loss: 10.5470 (GAN: 0.7855, L1: 2.7905, Perceptual: 6.9710)\nEpoch [30/50], Batch [200/250] | D Loss: 0.5931 | G Loss: 10.7934 (GAN: 0.7897, L1: 3.1905, Perceptual: 6.8131)\nEpoch [30/50], Batch [210/250] | D Loss: 0.6612 | G Loss: 12.3423 (GAN: 0.8209, L1: 3.7624, Perceptual: 7.7590)\nEpoch [30/50], Batch [220/250] | D Loss: 0.5813 | G Loss: 11.5570 (GAN: 0.6531, L1: 3.7641, Perceptual: 7.1398)\nEpoch [30/50], Batch [230/250] | D Loss: 0.5531 | G Loss: 12.2998 (GAN: 0.7436, L1: 3.7616, Perceptual: 7.7946)\nEpoch [30/50], Batch [240/250] | D Loss: 0.5580 | G Loss: 12.8016 (GAN: 0.9465, L1: 3.7818, Perceptual: 8.0733)\nEpoch [30/50], Batch [250/250] | D Loss: 0.5732 | G Loss: 10.2865 (GAN: 0.7961, L1: 3.0064, Perceptual: 6.4840)\nEpoch 30 completed. Current LR G: 0.000200, LR D: 0.000100\n--- Entering save/checkpoint block for epoch 30 ---\nSaving samples and checkpoint for epoch 30...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_30_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [31/50], Batch [10/250] | D Loss: 0.4811 | G Loss: 11.9809 (GAN: 0.9063, L1: 3.6998, Perceptual: 7.3747)\nEpoch [31/50], Batch [20/250] | D Loss: 0.5556 | G Loss: 10.1483 (GAN: 0.7138, L1: 2.8371, Perceptual: 6.5974)\nEpoch [31/50], Batch [30/250] | D Loss: 0.6043 | G Loss: 10.5014 (GAN: 0.7886, L1: 2.9750, Perceptual: 6.7378)\nEpoch [31/50], Batch [40/250] | D Loss: 0.6271 | G Loss: 11.0636 (GAN: 0.8017, L1: 3.2035, Perceptual: 7.0585)\nEpoch [31/50], Batch [50/250] | D Loss: 0.5070 | G Loss: 12.4636 (GAN: 0.7454, L1: 3.9454, Perceptual: 7.7728)\nEpoch [31/50], Batch [60/250] | D Loss: 0.6314 | G Loss: 11.1659 (GAN: 0.7222, L1: 3.4346, Perceptual: 7.0090)\nEpoch [31/50], Batch [70/250] | D Loss: 0.6782 | G Loss: 10.3851 (GAN: 0.5167, L1: 3.0339, Perceptual: 6.8346)\nEpoch [31/50], Batch [80/250] | D Loss: 0.8767 | G Loss: 10.4668 (GAN: 0.9306, L1: 2.8749, Perceptual: 6.6613)\nEpoch [31/50], Batch [90/250] | D Loss: 0.6001 | G Loss: 11.4546 (GAN: 0.7646, L1: 3.2322, Perceptual: 7.4577)\nEpoch [31/50], Batch [100/250] | D Loss: 0.6087 | G Loss: 11.2733 (GAN: 0.8150, L1: 3.2916, Perceptual: 7.1666)\nEpoch [31/50], Batch [110/250] | D Loss: 0.7635 | G Loss: 11.7843 (GAN: 0.9249, L1: 3.3671, Perceptual: 7.4923)\nEpoch [31/50], Batch [120/250] | D Loss: 0.8380 | G Loss: 11.8034 (GAN: 0.8703, L1: 3.3478, Perceptual: 7.5853)\nEpoch [31/50], Batch [130/250] | D Loss: 0.7493 | G Loss: 11.5567 (GAN: 0.8302, L1: 3.6850, Perceptual: 7.0415)\nEpoch [31/50], Batch [140/250] | D Loss: 0.6948 | G Loss: 10.4531 (GAN: 0.5806, L1: 3.3484, Perceptual: 6.5241)\nEpoch [31/50], Batch [150/250] | D Loss: 0.6036 | G Loss: 10.1455 (GAN: 0.7330, L1: 2.9086, Perceptual: 6.5039)\nEpoch [31/50], Batch [160/250] | D Loss: 0.7054 | G Loss: 12.6840 (GAN: 1.0185, L1: 4.7402, Perceptual: 6.9253)\nEpoch [31/50], Batch [170/250] | D Loss: 0.5941 | G Loss: 12.7548 (GAN: 0.9979, L1: 3.8643, Perceptual: 7.8925)\nEpoch [31/50], Batch [180/250] | D Loss: 0.8031 | G Loss: 12.0768 (GAN: 0.9201, L1: 3.4381, Perceptual: 7.7186)\nEpoch [31/50], Batch [190/250] | D Loss: 0.5343 | G Loss: 11.8979 (GAN: 0.8724, L1: 3.5869, Perceptual: 7.4386)\nEpoch [31/50], Batch [200/250] | D Loss: 0.6481 | G Loss: 12.3976 (GAN: 0.8159, L1: 3.8689, Perceptual: 7.7128)\nEpoch [31/50], Batch [210/250] | D Loss: 0.5952 | G Loss: 11.3233 (GAN: 0.7118, L1: 3.4764, Perceptual: 7.1350)\nEpoch [31/50], Batch [220/250] | D Loss: 0.4417 | G Loss: 12.0727 (GAN: 0.5031, L1: 3.7227, Perceptual: 7.8469)\nEpoch [31/50], Batch [230/250] | D Loss: 0.5488 | G Loss: 11.4988 (GAN: 0.7812, L1: 3.5138, Perceptual: 7.2038)\nEpoch [31/50], Batch [240/250] | D Loss: 0.6951 | G Loss: 11.7311 (GAN: 0.8517, L1: 3.5726, Perceptual: 7.3068)\nEpoch [31/50], Batch [250/250] | D Loss: 0.8588 | G Loss: 10.9153 (GAN: 1.0168, L1: 3.3372, Perceptual: 6.5612)\nEpoch 31 completed. Current LR G: 0.000190, LR D: 0.000095\nEpoch [32/50], Batch [10/250] | D Loss: 0.9046 | G Loss: 10.5915 (GAN: 0.9348, L1: 2.7913, Perceptual: 6.8654)\nEpoch [32/50], Batch [20/250] | D Loss: 0.6108 | G Loss: 11.1347 (GAN: 0.7084, L1: 3.3385, Perceptual: 7.0878)\nEpoch [32/50], Batch [30/250] | D Loss: 0.5234 | G Loss: 12.3200 (GAN: 0.8343, L1: 3.6495, Perceptual: 7.8362)\nEpoch [32/50], Batch [40/250] | D Loss: 0.6006 | G Loss: 10.1115 (GAN: 0.7381, L1: 2.8372, Perceptual: 6.5362)\nEpoch [32/50], Batch [50/250] | D Loss: 0.4557 | G Loss: 12.4854 (GAN: 0.7784, L1: 3.8505, Perceptual: 7.8566)\nEpoch [32/50], Batch [60/250] | D Loss: 0.6857 | G Loss: 11.0416 (GAN: 0.7357, L1: 3.3516, Perceptual: 6.9544)\nEpoch [32/50], Batch [70/250] | D Loss: 0.5692 | G Loss: 12.4162 (GAN: 0.8959, L1: 4.1319, Perceptual: 7.3885)\nEpoch [32/50], Batch [80/250] | D Loss: 0.5146 | G Loss: 11.7556 (GAN: 0.7144, L1: 3.5723, Perceptual: 7.4689)\nEpoch [32/50], Batch [90/250] | D Loss: 0.5565 | G Loss: 11.8998 (GAN: 0.9984, L1: 3.3627, Perceptual: 7.5386)\nEpoch [32/50], Batch [100/250] | D Loss: 0.5749 | G Loss: 12.7748 (GAN: 0.9524, L1: 4.0529, Perceptual: 7.7694)\nEpoch [32/50], Batch [110/250] | D Loss: 0.5669 | G Loss: 12.0279 (GAN: 0.9742, L1: 3.5989, Perceptual: 7.4547)\nEpoch [32/50], Batch [120/250] | D Loss: 0.6500 | G Loss: 12.3353 (GAN: 0.8348, L1: 3.8847, Perceptual: 7.6158)\nEpoch [32/50], Batch [130/250] | D Loss: 0.8798 | G Loss: 10.0102 (GAN: 0.6724, L1: 2.9942, Perceptual: 6.3437)\nEpoch [32/50], Batch [140/250] | D Loss: 0.7062 | G Loss: 11.3394 (GAN: 0.7881, L1: 3.2459, Perceptual: 7.3053)\nEpoch [32/50], Batch [150/250] | D Loss: 0.6314 | G Loss: 11.2269 (GAN: 0.8769, L1: 3.2695, Perceptual: 7.0805)\nEpoch [32/50], Batch [160/250] | D Loss: 0.6509 | G Loss: 10.6569 (GAN: 0.6510, L1: 3.3345, Perceptual: 6.6715)\nEpoch [32/50], Batch [170/250] | D Loss: 0.6301 | G Loss: 10.9591 (GAN: 0.8632, L1: 3.2867, Perceptual: 6.8091)\nEpoch [32/50], Batch [180/250] | D Loss: 0.6606 | G Loss: 10.4370 (GAN: 0.6344, L1: 3.0525, Perceptual: 6.7501)\nEpoch [32/50], Batch [190/250] | D Loss: 0.5174 | G Loss: 12.3271 (GAN: 1.0274, L1: 3.4264, Perceptual: 7.8733)\nEpoch [32/50], Batch [200/250] | D Loss: 0.5724 | G Loss: 12.8164 (GAN: 0.8473, L1: 4.3085, Perceptual: 7.6605)\nEpoch [32/50], Batch [210/250] | D Loss: 0.6690 | G Loss: 11.9176 (GAN: 0.6947, L1: 3.5406, Perceptual: 7.6823)\nEpoch [32/50], Batch [220/250] | D Loss: 0.7238 | G Loss: 12.0091 (GAN: 0.9117, L1: 3.6182, Perceptual: 7.4792)\nEpoch [32/50], Batch [230/250] | D Loss: 0.7395 | G Loss: 11.0251 (GAN: 0.8361, L1: 3.0682, Perceptual: 7.1208)\nEpoch [32/50], Batch [240/250] | D Loss: 0.7300 | G Loss: 11.6828 (GAN: 0.8235, L1: 3.6121, Perceptual: 7.2472)\nEpoch [32/50], Batch [250/250] | D Loss: 0.5750 | G Loss: 11.2671 (GAN: 0.7373, L1: 3.3890, Perceptual: 7.1408)\nEpoch 32 completed. Current LR G: 0.000180, LR D: 0.000090\nEpoch [33/50], Batch [10/250] | D Loss: 0.5643 | G Loss: 11.5472 (GAN: 0.7711, L1: 3.4639, Perceptual: 7.3122)\nEpoch [33/50], Batch [20/250] | D Loss: 0.6168 | G Loss: 11.4743 (GAN: 0.7306, L1: 3.7552, Perceptual: 6.9885)\nEpoch [33/50], Batch [30/250] | D Loss: 0.7186 | G Loss: 10.7820 (GAN: 0.6875, L1: 3.3620, Perceptual: 6.7326)\nEpoch [33/50], Batch [40/250] | D Loss: 0.6248 | G Loss: 11.7301 (GAN: 0.6246, L1: 3.5724, Perceptual: 7.5331)\nEpoch [33/50], Batch [50/250] | D Loss: 0.6930 | G Loss: 11.5354 (GAN: 0.7758, L1: 3.5667, Perceptual: 7.1930)\nEpoch [33/50], Batch [60/250] | D Loss: 0.4808 | G Loss: 11.9812 (GAN: 0.9052, L1: 3.4642, Perceptual: 7.6118)\nEpoch [33/50], Batch [70/250] | D Loss: 0.7548 | G Loss: 12.9265 (GAN: 1.2523, L1: 3.7975, Perceptual: 7.8767)\nEpoch [33/50], Batch [80/250] | D Loss: 0.8320 | G Loss: 11.5008 (GAN: 0.5944, L1: 3.8137, Perceptual: 7.0927)\nEpoch [33/50], Batch [90/250] | D Loss: 0.7233 | G Loss: 11.8242 (GAN: 0.8571, L1: 3.5837, Perceptual: 7.3833)\nEpoch [33/50], Batch [100/250] | D Loss: 0.4956 | G Loss: 11.0224 (GAN: 0.9396, L1: 3.0994, Perceptual: 6.9833)\nEpoch [33/50], Batch [110/250] | D Loss: 0.7740 | G Loss: 10.6995 (GAN: 0.9733, L1: 3.0105, Perceptual: 6.7156)\nEpoch [33/50], Batch [120/250] | D Loss: 0.7091 | G Loss: 10.7599 (GAN: 0.7200, L1: 3.3366, Perceptual: 6.7033)\nEpoch [33/50], Batch [130/250] | D Loss: 0.5945 | G Loss: 10.6265 (GAN: 0.6776, L1: 3.2769, Perceptual: 6.6720)\nEpoch [33/50], Batch [140/250] | D Loss: 0.6956 | G Loss: 10.4450 (GAN: 0.6547, L1: 3.1501, Perceptual: 6.6401)\nEpoch [33/50], Batch [150/250] | D Loss: 0.5314 | G Loss: 11.7724 (GAN: 0.7476, L1: 3.3806, Perceptual: 7.6442)\nEpoch [33/50], Batch [160/250] | D Loss: 0.6412 | G Loss: 10.7993 (GAN: 0.6926, L1: 3.1692, Perceptual: 6.9375)\nEpoch [33/50], Batch [170/250] | D Loss: 0.5446 | G Loss: 12.5248 (GAN: 1.1511, L1: 3.8729, Perceptual: 7.5009)\nEpoch [33/50], Batch [180/250] | D Loss: 0.5680 | G Loss: 10.5627 (GAN: 0.7246, L1: 2.9328, Perceptual: 6.9054)\nEpoch [33/50], Batch [190/250] | D Loss: 0.6579 | G Loss: 11.3841 (GAN: 0.7242, L1: 3.4894, Perceptual: 7.1705)\nEpoch [33/50], Batch [200/250] | D Loss: 0.6064 | G Loss: 13.0830 (GAN: 1.0712, L1: 3.9480, Perceptual: 8.0638)\nEpoch [33/50], Batch [210/250] | D Loss: 0.5357 | G Loss: 11.6619 (GAN: 0.8333, L1: 3.5251, Perceptual: 7.3036)\nEpoch [33/50], Batch [220/250] | D Loss: 0.6935 | G Loss: 11.0599 (GAN: 0.8902, L1: 3.1444, Perceptual: 7.0254)\nEpoch [33/50], Batch [230/250] | D Loss: 0.7073 | G Loss: 11.2170 (GAN: 0.6779, L1: 3.4504, Perceptual: 7.0888)\nEpoch [33/50], Batch [240/250] | D Loss: 0.5619 | G Loss: 12.3140 (GAN: 0.7140, L1: 4.0974, Perceptual: 7.5025)\nEpoch [33/50], Batch [250/250] | D Loss: 0.7287 | G Loss: 11.0432 (GAN: 0.8940, L1: 3.0870, Perceptual: 7.0622)\nEpoch 33 completed. Current LR G: 0.000170, LR D: 0.000085\nEpoch [34/50], Batch [10/250] | D Loss: 0.5519 | G Loss: 11.4633 (GAN: 0.7123, L1: 3.4555, Perceptual: 7.2954)\nEpoch [34/50], Batch [20/250] | D Loss: 0.5715 | G Loss: 10.3044 (GAN: 0.6948, L1: 3.0611, Perceptual: 6.5486)\nEpoch [34/50], Batch [30/250] | D Loss: 0.5331 | G Loss: 11.0187 (GAN: 0.9439, L1: 3.1204, Perceptual: 6.9543)\nEpoch [34/50], Batch [40/250] | D Loss: 0.5287 | G Loss: 10.7421 (GAN: 0.8814, L1: 3.0605, Perceptual: 6.8002)\nEpoch [34/50], Batch [50/250] | D Loss: 0.6992 | G Loss: 11.9037 (GAN: 0.9167, L1: 3.5758, Perceptual: 7.4112)\nEpoch [34/50], Batch [60/250] | D Loss: 0.5985 | G Loss: 11.0265 (GAN: 0.6666, L1: 3.2225, Perceptual: 7.1374)\nEpoch [34/50], Batch [70/250] | D Loss: 0.9361 | G Loss: 10.8427 (GAN: 0.8279, L1: 3.1547, Perceptual: 6.8601)\nEpoch [34/50], Batch [80/250] | D Loss: 0.4307 | G Loss: 13.0111 (GAN: 0.7898, L1: 4.4849, Perceptual: 7.7364)\nEpoch [34/50], Batch [90/250] | D Loss: 0.5773 | G Loss: 9.9327 (GAN: 0.7323, L1: 2.9053, Perceptual: 6.2950)\nEpoch [34/50], Batch [100/250] | D Loss: 0.6137 | G Loss: 11.6989 (GAN: 0.8034, L1: 3.4964, Perceptual: 7.3991)\nEpoch [34/50], Batch [110/250] | D Loss: 0.5720 | G Loss: 11.9580 (GAN: 0.8322, L1: 3.6048, Perceptual: 7.5211)\nEpoch [34/50], Batch [120/250] | D Loss: 0.5508 | G Loss: 11.1444 (GAN: 0.6968, L1: 3.1681, Perceptual: 7.2795)\nEpoch [34/50], Batch [130/250] | D Loss: 0.6795 | G Loss: 12.0460 (GAN: 1.0251, L1: 3.6305, Perceptual: 7.3904)\nEpoch [34/50], Batch [140/250] | D Loss: 0.5786 | G Loss: 11.4685 (GAN: 0.7529, L1: 3.2722, Perceptual: 7.4434)\nEpoch [34/50], Batch [150/250] | D Loss: 0.7082 | G Loss: 11.5290 (GAN: 0.6749, L1: 3.5485, Perceptual: 7.3056)\nEpoch [34/50], Batch [160/250] | D Loss: 0.7088 | G Loss: 10.9105 (GAN: 0.7044, L1: 3.3840, Perceptual: 6.8222)\nEpoch [34/50], Batch [170/250] | D Loss: 0.5762 | G Loss: 12.3828 (GAN: 1.0314, L1: 3.6070, Perceptual: 7.7444)\nEpoch [34/50], Batch [180/250] | D Loss: 0.5268 | G Loss: 11.6066 (GAN: 0.7436, L1: 3.5876, Perceptual: 7.2755)\nEpoch [34/50], Batch [190/250] | D Loss: 0.4689 | G Loss: 11.8057 (GAN: 0.9394, L1: 3.4024, Perceptual: 7.4638)\nEpoch [34/50], Batch [200/250] | D Loss: 0.7490 | G Loss: 9.8113 (GAN: 0.6617, L1: 2.6807, Perceptual: 6.4689)\nEpoch [34/50], Batch [210/250] | D Loss: 0.6120 | G Loss: 11.7595 (GAN: 0.6917, L1: 3.5848, Perceptual: 7.4830)\nEpoch [34/50], Batch [220/250] | D Loss: 0.3996 | G Loss: 12.1646 (GAN: 0.7325, L1: 3.7068, Perceptual: 7.7253)\nEpoch [34/50], Batch [230/250] | D Loss: 0.6690 | G Loss: 11.9472 (GAN: 0.8247, L1: 3.7987, Perceptual: 7.3239)\nEpoch [34/50], Batch [240/250] | D Loss: 0.4173 | G Loss: 12.2644 (GAN: 0.7121, L1: 3.7454, Perceptual: 7.8069)\nEpoch [34/50], Batch [250/250] | D Loss: 0.8773 | G Loss: 11.0840 (GAN: 0.9255, L1: 3.3840, Perceptual: 6.7745)\nEpoch 34 completed. Current LR G: 0.000160, LR D: 0.000080\nEpoch [35/50], Batch [10/250] | D Loss: 0.7400 | G Loss: 11.6374 (GAN: 0.9516, L1: 3.3898, Perceptual: 7.2961)\nEpoch [35/50], Batch [20/250] | D Loss: 0.5515 | G Loss: 11.6536 (GAN: 0.7809, L1: 3.4471, Perceptual: 7.4257)\nEpoch [35/50], Batch [30/250] | D Loss: 0.4740 | G Loss: 11.7826 (GAN: 0.9206, L1: 3.4431, Perceptual: 7.4189)\nEpoch [35/50], Batch [40/250] | D Loss: 0.5877 | G Loss: 10.7573 (GAN: 0.8605, L1: 3.1673, Perceptual: 6.7295)\nEpoch [35/50], Batch [50/250] | D Loss: 0.6004 | G Loss: 11.7037 (GAN: 0.8894, L1: 3.4054, Perceptual: 7.4089)\nEpoch [35/50], Batch [60/250] | D Loss: 0.5469 | G Loss: 12.1191 (GAN: 0.7522, L1: 3.9724, Perceptual: 7.3946)\nEpoch [35/50], Batch [70/250] | D Loss: 0.9467 | G Loss: 11.1205 (GAN: 0.7808, L1: 3.6542, Perceptual: 6.6856)\nEpoch [35/50], Batch [80/250] | D Loss: 0.7007 | G Loss: 11.4831 (GAN: 0.9032, L1: 3.2650, Perceptual: 7.3149)\nEpoch [35/50], Batch [90/250] | D Loss: 0.6263 | G Loss: 11.0065 (GAN: 0.6263, L1: 3.3261, Perceptual: 7.0542)\nEpoch [35/50], Batch [100/250] | D Loss: 0.5501 | G Loss: 11.4370 (GAN: 0.8230, L1: 3.3397, Perceptual: 7.2743)\nEpoch [35/50], Batch [110/250] | D Loss: 0.5654 | G Loss: 11.1049 (GAN: 0.8007, L1: 3.0199, Perceptual: 7.2843)\nEpoch [35/50], Batch [120/250] | D Loss: 0.4466 | G Loss: 11.8503 (GAN: 1.0089, L1: 3.4181, Perceptual: 7.4233)\nEpoch [35/50], Batch [130/250] | D Loss: 0.7758 | G Loss: 11.3884 (GAN: 0.8332, L1: 3.6060, Perceptual: 6.9492)\nEpoch [35/50], Batch [140/250] | D Loss: 0.5387 | G Loss: 11.0223 (GAN: 0.5819, L1: 3.3701, Perceptual: 7.0704)\nEpoch [35/50], Batch [150/250] | D Loss: 0.6975 | G Loss: 11.8567 (GAN: 0.7579, L1: 3.6495, Perceptual: 7.4494)\nEpoch [35/50], Batch [160/250] | D Loss: 0.5470 | G Loss: 11.9446 (GAN: 0.8201, L1: 3.6620, Perceptual: 7.4625)\nEpoch [35/50], Batch [170/250] | D Loss: 0.5880 | G Loss: 10.7457 (GAN: 0.7405, L1: 2.9099, Perceptual: 7.0953)\nEpoch [35/50], Batch [180/250] | D Loss: 0.4045 | G Loss: 13.0852 (GAN: 1.1143, L1: 4.0257, Perceptual: 7.9452)\nEpoch [35/50], Batch [190/250] | D Loss: 0.6248 | G Loss: 10.5799 (GAN: 0.7507, L1: 2.9258, Perceptual: 6.9034)\nEpoch [35/50], Batch [200/250] | D Loss: 0.6359 | G Loss: 11.5545 (GAN: 1.0291, L1: 3.4886, Perceptual: 7.0368)\nEpoch [35/50], Batch [210/250] | D Loss: 0.5006 | G Loss: 11.9623 (GAN: 0.6778, L1: 3.8320, Perceptual: 7.4525)\nEpoch [35/50], Batch [220/250] | D Loss: 0.6474 | G Loss: 11.6041 (GAN: 1.1014, L1: 3.4028, Perceptual: 7.0998)\nEpoch [35/50], Batch [230/250] | D Loss: 0.6349 | G Loss: 11.7751 (GAN: 0.8932, L1: 3.3710, Perceptual: 7.5108)\nEpoch [35/50], Batch [240/250] | D Loss: 0.5180 | G Loss: 10.6151 (GAN: 0.8181, L1: 3.0690, Perceptual: 6.7280)\nEpoch [35/50], Batch [250/250] | D Loss: 0.6660 | G Loss: 11.6287 (GAN: 0.7374, L1: 3.6131, Perceptual: 7.2782)\nEpoch 35 completed. Current LR G: 0.000150, LR D: 0.000075\n--- Entering save/checkpoint block for epoch 35 ---\nSaving samples and checkpoint for epoch 35...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_35_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [36/50], Batch [10/250] | D Loss: 0.5392 | G Loss: 11.6252 (GAN: 0.8849, L1: 3.5069, Perceptual: 7.2334)\nEpoch [36/50], Batch [20/250] | D Loss: 0.5953 | G Loss: 11.0858 (GAN: 0.8539, L1: 3.1960, Perceptual: 7.0359)\nEpoch [36/50], Batch [30/250] | D Loss: 0.6504 | G Loss: 11.1909 (GAN: 0.8309, L1: 3.2623, Perceptual: 7.0976)\nEpoch [36/50], Batch [40/250] | D Loss: 0.6034 | G Loss: 12.5589 (GAN: 0.8139, L1: 3.8242, Perceptual: 7.9207)\nEpoch [36/50], Batch [50/250] | D Loss: 0.6585 | G Loss: 12.3265 (GAN: 0.9003, L1: 3.8935, Perceptual: 7.5327)\nEpoch [36/50], Batch [60/250] | D Loss: 0.6408 | G Loss: 11.1409 (GAN: 0.7893, L1: 3.1545, Perceptual: 7.1971)\nEpoch [36/50], Batch [70/250] | D Loss: 0.6733 | G Loss: 11.4037 (GAN: 0.6918, L1: 3.4817, Perceptual: 7.2303)\nEpoch [36/50], Batch [80/250] | D Loss: 0.5645 | G Loss: 12.2985 (GAN: 0.8909, L1: 3.3886, Perceptual: 8.0190)\nEpoch [36/50], Batch [90/250] | D Loss: 0.6418 | G Loss: 11.6034 (GAN: 0.6879, L1: 3.7519, Perceptual: 7.1636)\nEpoch [36/50], Batch [100/250] | D Loss: 0.7407 | G Loss: 10.8880 (GAN: 0.6827, L1: 3.0615, Perceptual: 7.1439)\nEpoch [36/50], Batch [110/250] | D Loss: 0.7177 | G Loss: 10.9667 (GAN: 0.8877, L1: 3.1229, Perceptual: 6.9561)\nEpoch [36/50], Batch [120/250] | D Loss: 0.7538 | G Loss: 11.4063 (GAN: 0.9151, L1: 3.8167, Perceptual: 6.6746)\nEpoch [36/50], Batch [130/250] | D Loss: 0.6431 | G Loss: 10.6079 (GAN: 0.7831, L1: 3.0154, Perceptual: 6.8093)\nEpoch [36/50], Batch [140/250] | D Loss: 0.6360 | G Loss: 11.3574 (GAN: 0.6979, L1: 3.4680, Perceptual: 7.1915)\nEpoch [36/50], Batch [150/250] | D Loss: 0.5582 | G Loss: 12.1605 (GAN: 0.7723, L1: 3.8339, Perceptual: 7.5544)\nEpoch [36/50], Batch [160/250] | D Loss: 0.6191 | G Loss: 12.3299 (GAN: 0.9288, L1: 3.5123, Perceptual: 7.8888)\nEpoch [36/50], Batch [170/250] | D Loss: 0.7008 | G Loss: 11.7093 (GAN: 0.8161, L1: 3.5351, Perceptual: 7.3581)\nEpoch [36/50], Batch [180/250] | D Loss: 0.8710 | G Loss: 10.7408 (GAN: 0.8073, L1: 3.2455, Perceptual: 6.6881)\nEpoch [36/50], Batch [190/250] | D Loss: 0.6562 | G Loss: 10.6096 (GAN: 0.8430, L1: 2.9747, Perceptual: 6.7919)\nEpoch [36/50], Batch [200/250] | D Loss: 0.6865 | G Loss: 10.9337 (GAN: 0.8402, L1: 2.9454, Perceptual: 7.1481)\nEpoch [36/50], Batch [210/250] | D Loss: 0.6853 | G Loss: 10.7340 (GAN: 0.8509, L1: 2.8457, Perceptual: 7.0374)\nEpoch [36/50], Batch [220/250] | D Loss: 0.5334 | G Loss: 13.0137 (GAN: 0.7845, L1: 4.0995, Perceptual: 8.1298)\nEpoch [36/50], Batch [230/250] | D Loss: 0.6468 | G Loss: 11.2810 (GAN: 0.9482, L1: 3.3499, Perceptual: 6.9829)\nEpoch [36/50], Batch [240/250] | D Loss: 0.5045 | G Loss: 11.7367 (GAN: 0.7971, L1: 3.3271, Perceptual: 7.6125)\nEpoch [36/50], Batch [250/250] | D Loss: 0.5239 | G Loss: 11.3984 (GAN: 0.7029, L1: 3.6920, Perceptual: 7.0035)\nEpoch 36 completed. Current LR G: 0.000140, LR D: 0.000070\nEpoch [37/50], Batch [10/250] | D Loss: 0.7375 | G Loss: 10.9721 (GAN: 0.7205, L1: 2.9176, Perceptual: 7.3340)\nEpoch [37/50], Batch [20/250] | D Loss: 0.5304 | G Loss: 11.7152 (GAN: 0.8015, L1: 3.4032, Perceptual: 7.5104)\nEpoch [37/50], Batch [30/250] | D Loss: 0.6889 | G Loss: 12.4726 (GAN: 0.9720, L1: 3.7047, Perceptual: 7.7959)\nEpoch [37/50], Batch [40/250] | D Loss: 0.6352 | G Loss: 10.7924 (GAN: 0.6528, L1: 3.1621, Perceptual: 6.9775)\nEpoch [37/50], Batch [50/250] | D Loss: 0.4415 | G Loss: 11.7890 (GAN: 0.8283, L1: 3.4615, Perceptual: 7.4992)\nEpoch [37/50], Batch [60/250] | D Loss: 0.5116 | G Loss: 11.4154 (GAN: 0.8266, L1: 3.2109, Perceptual: 7.3778)\nEpoch [37/50], Batch [70/250] | D Loss: 0.6364 | G Loss: 11.9979 (GAN: 0.8692, L1: 3.3264, Perceptual: 7.8023)\nEpoch [37/50], Batch [80/250] | D Loss: 0.8082 | G Loss: 10.2837 (GAN: 1.0305, L1: 2.6832, Perceptual: 6.5699)\nEpoch [37/50], Batch [90/250] | D Loss: 0.6377 | G Loss: 11.2253 (GAN: 0.6552, L1: 3.1524, Perceptual: 7.4177)\nEpoch [37/50], Batch [100/250] | D Loss: 0.5922 | G Loss: 11.7641 (GAN: 0.8386, L1: 3.6759, Perceptual: 7.2496)\nEpoch [37/50], Batch [110/250] | D Loss: 0.4913 | G Loss: 12.3406 (GAN: 0.5179, L1: 4.0043, Perceptual: 7.8185)\nEpoch [37/50], Batch [120/250] | D Loss: 0.6107 | G Loss: 11.5512 (GAN: 0.8076, L1: 3.4812, Perceptual: 7.2624)\nEpoch [37/50], Batch [130/250] | D Loss: 0.5749 | G Loss: 12.5839 (GAN: 0.7807, L1: 4.0269, Perceptual: 7.7764)\nEpoch [37/50], Batch [140/250] | D Loss: 0.7502 | G Loss: 11.0655 (GAN: 0.8426, L1: 3.3219, Perceptual: 6.9010)\nEpoch [37/50], Batch [150/250] | D Loss: 0.6821 | G Loss: 11.2219 (GAN: 0.8811, L1: 3.2016, Perceptual: 7.1392)\nEpoch [37/50], Batch [160/250] | D Loss: 0.5584 | G Loss: 10.3908 (GAN: 0.6708, L1: 2.9700, Perceptual: 6.7500)\nEpoch [37/50], Batch [170/250] | D Loss: 0.6609 | G Loss: 11.2248 (GAN: 0.8528, L1: 3.0929, Perceptual: 7.2791)\nEpoch [37/50], Batch [180/250] | D Loss: 0.4483 | G Loss: 12.9234 (GAN: 0.8244, L1: 3.8415, Perceptual: 8.2575)\nEpoch [37/50], Batch [190/250] | D Loss: 0.6686 | G Loss: 10.1902 (GAN: 0.8420, L1: 3.0468, Perceptual: 6.3014)\nEpoch [37/50], Batch [200/250] | D Loss: 0.6635 | G Loss: 10.8420 (GAN: 0.8220, L1: 3.0728, Perceptual: 6.9472)\nEpoch [37/50], Batch [210/250] | D Loss: 0.5876 | G Loss: 10.7345 (GAN: 0.9304, L1: 2.9662, Perceptual: 6.8379)\nEpoch [37/50], Batch [220/250] | D Loss: 0.6752 | G Loss: 10.5584 (GAN: 0.7841, L1: 3.1849, Perceptual: 6.5893)\nEpoch [37/50], Batch [230/250] | D Loss: 0.4587 | G Loss: 11.7305 (GAN: 0.8158, L1: 3.5261, Perceptual: 7.3886)\nEpoch [37/50], Batch [240/250] | D Loss: 0.5668 | G Loss: 11.8017 (GAN: 0.8700, L1: 3.1809, Perceptual: 7.7509)\nEpoch [37/50], Batch [250/250] | D Loss: 0.5628 | G Loss: 12.0921 (GAN: 0.6907, L1: 3.6789, Perceptual: 7.7225)\nEpoch 37 completed. Current LR G: 0.000130, LR D: 0.000065\nEpoch [38/50], Batch [10/250] | D Loss: 0.6538 | G Loss: 9.8798 (GAN: 0.6639, L1: 2.9066, Perceptual: 6.3093)\nEpoch [38/50], Batch [20/250] | D Loss: 0.5518 | G Loss: 11.2413 (GAN: 0.9386, L1: 3.1578, Perceptual: 7.1449)\nEpoch [38/50], Batch [30/250] | D Loss: 0.6026 | G Loss: 11.1643 (GAN: 0.7874, L1: 3.1515, Perceptual: 7.2254)\nEpoch [38/50], Batch [40/250] | D Loss: 0.5778 | G Loss: 11.7298 (GAN: 0.8762, L1: 3.5822, Perceptual: 7.2714)\nEpoch [38/50], Batch [50/250] | D Loss: 0.7829 | G Loss: 9.8386 (GAN: 0.7959, L1: 2.5483, Perceptual: 6.4943)\nEpoch [38/50], Batch [60/250] | D Loss: 0.4920 | G Loss: 13.1855 (GAN: 0.8196, L1: 3.8414, Perceptual: 8.5245)\nEpoch [38/50], Batch [70/250] | D Loss: 0.4311 | G Loss: 12.0984 (GAN: 0.8986, L1: 3.4930, Perceptual: 7.7068)\nEpoch [38/50], Batch [80/250] | D Loss: 0.5677 | G Loss: 11.5459 (GAN: 0.7671, L1: 3.5965, Perceptual: 7.1823)\nEpoch [38/50], Batch [90/250] | D Loss: 0.6077 | G Loss: 12.1635 (GAN: 0.8417, L1: 3.7459, Perceptual: 7.5760)\nEpoch [38/50], Batch [100/250] | D Loss: 0.6578 | G Loss: 11.2239 (GAN: 0.7744, L1: 3.1500, Perceptual: 7.2995)\nEpoch [38/50], Batch [110/250] | D Loss: 0.6537 | G Loss: 11.0515 (GAN: 0.8416, L1: 3.1170, Perceptual: 7.0928)\nEpoch [38/50], Batch [120/250] | D Loss: 0.5543 | G Loss: 11.9702 (GAN: 1.0365, L1: 3.5615, Perceptual: 7.3722)\nEpoch [38/50], Batch [130/250] | D Loss: 0.6846 | G Loss: 11.1882 (GAN: 0.7697, L1: 3.4551, Perceptual: 6.9634)\nEpoch [38/50], Batch [140/250] | D Loss: 0.4997 | G Loss: 11.7583 (GAN: 1.0247, L1: 3.2404, Perceptual: 7.4931)\nEpoch [38/50], Batch [150/250] | D Loss: 0.5771 | G Loss: 11.7588 (GAN: 0.7766, L1: 3.5410, Perceptual: 7.4412)\nEpoch [38/50], Batch [160/250] | D Loss: 0.6614 | G Loss: 10.9621 (GAN: 0.8368, L1: 3.1407, Perceptual: 6.9846)\nEpoch [38/50], Batch [170/250] | D Loss: 0.6909 | G Loss: 12.3595 (GAN: 0.9944, L1: 3.7598, Perceptual: 7.6053)\nEpoch [38/50], Batch [180/250] | D Loss: 0.5868 | G Loss: 10.9775 (GAN: 0.9546, L1: 3.3270, Perceptual: 6.6958)\nEpoch [38/50], Batch [190/250] | D Loss: 0.7669 | G Loss: 11.7369 (GAN: 0.6594, L1: 4.0462, Perceptual: 7.0313)\nEpoch [38/50], Batch [200/250] | D Loss: 0.5768 | G Loss: 10.8410 (GAN: 0.8489, L1: 3.3223, Perceptual: 6.6698)\nEpoch [38/50], Batch [210/250] | D Loss: 0.6261 | G Loss: 10.8715 (GAN: 0.7749, L1: 3.1262, Perceptual: 6.9705)\nEpoch [38/50], Batch [220/250] | D Loss: 0.6138 | G Loss: 11.6891 (GAN: 0.7212, L1: 3.9321, Perceptual: 7.0358)\nEpoch [38/50], Batch [230/250] | D Loss: 0.6322 | G Loss: 11.8118 (GAN: 0.8292, L1: 3.3628, Perceptual: 7.6198)\nEpoch [38/50], Batch [240/250] | D Loss: 0.6552 | G Loss: 10.4719 (GAN: 1.0222, L1: 2.7471, Perceptual: 6.7026)\nEpoch [38/50], Batch [250/250] | D Loss: 0.4827 | G Loss: 11.3783 (GAN: 0.8955, L1: 3.2455, Perceptual: 7.2373)\nEpoch 38 completed. Current LR G: 0.000120, LR D: 0.000060\nEpoch [39/50], Batch [10/250] | D Loss: 0.5923 | G Loss: 11.3701 (GAN: 0.9212, L1: 3.3701, Perceptual: 7.0788)\nEpoch [39/50], Batch [20/250] | D Loss: 0.7255 | G Loss: 11.0084 (GAN: 0.8981, L1: 3.2356, Perceptual: 6.8747)\nEpoch [39/50], Batch [30/250] | D Loss: 0.6015 | G Loss: 12.2251 (GAN: 0.7316, L1: 3.9644, Perceptual: 7.5291)\nEpoch [39/50], Batch [40/250] | D Loss: 0.6750 | G Loss: 10.2072 (GAN: 0.7340, L1: 3.0020, Perceptual: 6.4712)\nEpoch [39/50], Batch [50/250] | D Loss: 0.5276 | G Loss: 10.9421 (GAN: 0.7403, L1: 3.1322, Perceptual: 7.0696)\nEpoch [39/50], Batch [60/250] | D Loss: 0.5963 | G Loss: 12.1116 (GAN: 0.9347, L1: 3.8418, Perceptual: 7.3351)\nEpoch [39/50], Batch [70/250] | D Loss: 0.6354 | G Loss: 12.3815 (GAN: 0.8811, L1: 3.7316, Perceptual: 7.7688)\nEpoch [39/50], Batch [80/250] | D Loss: 0.4641 | G Loss: 12.0736 (GAN: 0.5899, L1: 3.8468, Perceptual: 7.6369)\nEpoch [39/50], Batch [90/250] | D Loss: 0.5610 | G Loss: 12.7309 (GAN: 0.8849, L1: 3.9424, Perceptual: 7.9036)\nEpoch [39/50], Batch [100/250] | D Loss: 0.5055 | G Loss: 11.4406 (GAN: 0.7568, L1: 3.4917, Perceptual: 7.1921)\nEpoch [39/50], Batch [110/250] | D Loss: 0.6790 | G Loss: 11.2645 (GAN: 1.0190, L1: 3.2112, Perceptual: 7.0343)\nEpoch [39/50], Batch [120/250] | D Loss: 0.6865 | G Loss: 11.7972 (GAN: 0.7879, L1: 3.6572, Perceptual: 7.3521)\nEpoch [39/50], Batch [130/250] | D Loss: 0.5663 | G Loss: 10.6162 (GAN: 0.6762, L1: 3.1894, Perceptual: 6.7505)\nEpoch [39/50], Batch [140/250] | D Loss: 0.5831 | G Loss: 11.0848 (GAN: 0.7791, L1: 3.3330, Perceptual: 6.9727)\nEpoch [39/50], Batch [150/250] | D Loss: 0.6603 | G Loss: 12.2033 (GAN: 1.1121, L1: 3.7564, Perceptual: 7.3348)\nEpoch [39/50], Batch [160/250] | D Loss: 0.5313 | G Loss: 11.7599 (GAN: 0.8671, L1: 3.4620, Perceptual: 7.4309)\nEpoch [39/50], Batch [170/250] | D Loss: 0.6704 | G Loss: 11.6725 (GAN: 0.8890, L1: 3.2474, Perceptual: 7.5361)\nEpoch [39/50], Batch [180/250] | D Loss: 0.6016 | G Loss: 11.4598 (GAN: 0.7599, L1: 3.4396, Perceptual: 7.2602)\nEpoch [39/50], Batch [190/250] | D Loss: 0.6829 | G Loss: 10.4159 (GAN: 0.6562, L1: 2.9564, Perceptual: 6.8033)\nEpoch [39/50], Batch [200/250] | D Loss: 0.6362 | G Loss: 11.0901 (GAN: 0.7792, L1: 3.3085, Perceptual: 7.0024)\nEpoch [39/50], Batch [210/250] | D Loss: 0.7627 | G Loss: 11.0159 (GAN: 0.7896, L1: 3.1389, Perceptual: 7.0874)\nEpoch [39/50], Batch [220/250] | D Loss: 0.6455 | G Loss: 10.9476 (GAN: 0.6868, L1: 3.1715, Perceptual: 7.0893)\nEpoch [39/50], Batch [230/250] | D Loss: 0.6488 | G Loss: 11.2471 (GAN: 0.7994, L1: 3.1418, Perceptual: 7.3059)\nEpoch [39/50], Batch [240/250] | D Loss: 0.6103 | G Loss: 12.5450 (GAN: 1.0321, L1: 3.5168, Perceptual: 7.9961)\nEpoch [39/50], Batch [250/250] | D Loss: 0.7220 | G Loss: 11.4550 (GAN: 1.0708, L1: 3.2411, Perceptual: 7.1430)\nEpoch 39 completed. Current LR G: 0.000110, LR D: 0.000055\nEpoch [40/50], Batch [10/250] | D Loss: 0.6124 | G Loss: 11.9479 (GAN: 0.7228, L1: 3.3711, Perceptual: 7.8539)\nEpoch [40/50], Batch [20/250] | D Loss: 0.6856 | G Loss: 10.9390 (GAN: 0.8508, L1: 3.1937, Perceptual: 6.8945)\nEpoch [40/50], Batch [30/250] | D Loss: 0.6439 | G Loss: 11.2699 (GAN: 0.8342, L1: 3.3535, Perceptual: 7.0822)\nEpoch [40/50], Batch [40/250] | D Loss: 0.5972 | G Loss: 11.5965 (GAN: 0.7170, L1: 3.3441, Perceptual: 7.5354)\nEpoch [40/50], Batch [50/250] | D Loss: 0.8114 | G Loss: 10.0703 (GAN: 0.9444, L1: 2.7390, Perceptual: 6.3870)\nEpoch [40/50], Batch [60/250] | D Loss: 0.6034 | G Loss: 11.0269 (GAN: 1.0036, L1: 3.0293, Perceptual: 6.9941)\nEpoch [40/50], Batch [70/250] | D Loss: 0.5639 | G Loss: 11.1934 (GAN: 0.8828, L1: 3.0642, Perceptual: 7.2464)\nEpoch [40/50], Batch [80/250] | D Loss: 0.4953 | G Loss: 12.9827 (GAN: 0.8653, L1: 4.2742, Perceptual: 7.8432)\nEpoch [40/50], Batch [90/250] | D Loss: 0.4822 | G Loss: 11.5787 (GAN: 0.8614, L1: 3.3606, Perceptual: 7.3567)\nEpoch [40/50], Batch [100/250] | D Loss: 0.6915 | G Loss: 10.3289 (GAN: 0.7612, L1: 2.9048, Perceptual: 6.6629)\nEpoch [40/50], Batch [110/250] | D Loss: 0.6166 | G Loss: 11.6088 (GAN: 0.7140, L1: 3.6903, Perceptual: 7.2045)\nEpoch [40/50], Batch [120/250] | D Loss: 0.6215 | G Loss: 10.6731 (GAN: 0.7686, L1: 3.2785, Perceptual: 6.6260)\nEpoch [40/50], Batch [130/250] | D Loss: 0.5418 | G Loss: 11.8203 (GAN: 0.9211, L1: 3.5676, Perceptual: 7.3316)\nEpoch [40/50], Batch [140/250] | D Loss: 0.6912 | G Loss: 10.2362 (GAN: 0.7543, L1: 2.7782, Perceptual: 6.7037)\nEpoch [40/50], Batch [150/250] | D Loss: 0.7036 | G Loss: 11.0507 (GAN: 0.7852, L1: 3.1817, Perceptual: 7.0838)\nEpoch [40/50], Batch [160/250] | D Loss: 0.6565 | G Loss: 11.7959 (GAN: 0.6990, L1: 3.5174, Perceptual: 7.5795)\nEpoch [40/50], Batch [170/250] | D Loss: 0.6634 | G Loss: 11.6395 (GAN: 0.7214, L1: 3.8544, Perceptual: 7.0637)\nEpoch [40/50], Batch [180/250] | D Loss: 0.5979 | G Loss: 11.2049 (GAN: 0.7799, L1: 3.1043, Perceptual: 7.3207)\nEpoch [40/50], Batch [190/250] | D Loss: 0.7999 | G Loss: 10.7383 (GAN: 0.7115, L1: 2.9565, Perceptual: 7.0702)\nEpoch [40/50], Batch [200/250] | D Loss: 0.6112 | G Loss: 11.9549 (GAN: 0.7202, L1: 3.4031, Perceptual: 7.8316)\nEpoch [40/50], Batch [210/250] | D Loss: 0.6732 | G Loss: 11.4857 (GAN: 0.8386, L1: 3.4474, Perceptual: 7.1997)\nEpoch [40/50], Batch [220/250] | D Loss: 0.5538 | G Loss: 13.2083 (GAN: 0.8061, L1: 4.0970, Perceptual: 8.3052)\nEpoch [40/50], Batch [230/250] | D Loss: 0.4582 | G Loss: 12.0482 (GAN: 0.8483, L1: 3.2270, Perceptual: 7.9728)\nEpoch [40/50], Batch [240/250] | D Loss: 0.6168 | G Loss: 11.7005 (GAN: 0.9050, L1: 3.2256, Perceptual: 7.5700)\nEpoch [40/50], Batch [250/250] | D Loss: 0.5876 | G Loss: 11.5199 (GAN: 0.8389, L1: 3.3780, Perceptual: 7.3030)\nEpoch 40 completed. Current LR G: 0.000100, LR D: 0.000050\n--- Entering save/checkpoint block for epoch 40 ---\nSaving samples and checkpoint for epoch 40...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_40_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [41/50], Batch [10/250] | D Loss: 0.5109 | G Loss: 10.9954 (GAN: 0.8279, L1: 3.1365, Perceptual: 7.0310)\nEpoch [41/50], Batch [20/250] | D Loss: 0.7415 | G Loss: 10.7104 (GAN: 0.8154, L1: 3.0222, Perceptual: 6.8728)\nEpoch [41/50], Batch [30/250] | D Loss: 0.6419 | G Loss: 10.8107 (GAN: 1.0056, L1: 3.0545, Perceptual: 6.7506)\nEpoch [41/50], Batch [40/250] | D Loss: 0.6073 | G Loss: 11.4502 (GAN: 0.8046, L1: 3.3040, Perceptual: 7.3415)\nEpoch [41/50], Batch [50/250] | D Loss: 0.7343 | G Loss: 10.9242 (GAN: 0.9318, L1: 2.9572, Perceptual: 7.0352)\nEpoch [41/50], Batch [60/250] | D Loss: 0.6866 | G Loss: 10.7769 (GAN: 0.7116, L1: 3.0843, Perceptual: 6.9809)\nEpoch [41/50], Batch [70/250] | D Loss: 0.6224 | G Loss: 12.1717 (GAN: 0.8603, L1: 3.6819, Perceptual: 7.6294)\nEpoch [41/50], Batch [80/250] | D Loss: 0.5875 | G Loss: 11.5579 (GAN: 0.9279, L1: 3.2090, Perceptual: 7.4210)\nEpoch [41/50], Batch [90/250] | D Loss: 0.6180 | G Loss: 10.2438 (GAN: 0.7726, L1: 2.8643, Perceptual: 6.6069)\nEpoch [41/50], Batch [100/250] | D Loss: 0.5539 | G Loss: 11.5280 (GAN: 0.9330, L1: 3.3008, Perceptual: 7.2942)\nEpoch [41/50], Batch [110/250] | D Loss: 0.5935 | G Loss: 11.6129 (GAN: 0.8069, L1: 3.3414, Perceptual: 7.4645)\nEpoch [41/50], Batch [120/250] | D Loss: 0.7156 | G Loss: 11.5185 (GAN: 0.8877, L1: 3.3159, Perceptual: 7.3148)\nEpoch [41/50], Batch [130/250] | D Loss: 0.6055 | G Loss: 12.0805 (GAN: 0.7813, L1: 3.7708, Perceptual: 7.5285)\nEpoch [41/50], Batch [140/250] | D Loss: 0.6027 | G Loss: 11.5562 (GAN: 0.8628, L1: 3.2003, Perceptual: 7.4931)\nEpoch [41/50], Batch [150/250] | D Loss: 0.5239 | G Loss: 13.7992 (GAN: 0.8660, L1: 4.4695, Perceptual: 8.4637)\nEpoch [41/50], Batch [160/250] | D Loss: 0.5237 | G Loss: 12.7542 (GAN: 0.9148, L1: 3.7667, Perceptual: 8.0728)\nEpoch [41/50], Batch [170/250] | D Loss: 0.6485 | G Loss: 11.8547 (GAN: 0.9014, L1: 3.2151, Perceptual: 7.7383)\nEpoch [41/50], Batch [180/250] | D Loss: 0.6776 | G Loss: 11.5420 (GAN: 0.9431, L1: 3.4024, Perceptual: 7.1965)\nEpoch [41/50], Batch [190/250] | D Loss: 0.5562 | G Loss: 11.2432 (GAN: 0.8383, L1: 3.2122, Perceptual: 7.1927)\nEpoch [41/50], Batch [200/250] | D Loss: 0.6322 | G Loss: 11.6246 (GAN: 0.7901, L1: 3.6334, Perceptual: 7.2011)\nEpoch [41/50], Batch [210/250] | D Loss: 0.6146 | G Loss: 12.1122 (GAN: 0.7377, L1: 3.8240, Perceptual: 7.5505)\nEpoch [41/50], Batch [220/250] | D Loss: 0.4719 | G Loss: 12.4015 (GAN: 0.8532, L1: 3.8795, Perceptual: 7.6688)\nEpoch [41/50], Batch [230/250] | D Loss: 0.7865 | G Loss: 10.6530 (GAN: 0.9165, L1: 2.8972, Perceptual: 6.8393)\nEpoch [41/50], Batch [240/250] | D Loss: 0.5743 | G Loss: 12.5551 (GAN: 0.7264, L1: 3.9583, Perceptual: 7.8705)\nEpoch [41/50], Batch [250/250] | D Loss: 0.5432 | G Loss: 12.0735 (GAN: 0.7631, L1: 3.7633, Perceptual: 7.5471)\nEpoch 41 completed. Current LR G: 0.000090, LR D: 0.000045\nEpoch [42/50], Batch [10/250] | D Loss: 0.6093 | G Loss: 12.1672 (GAN: 0.8322, L1: 3.7944, Perceptual: 7.5405)\nEpoch [42/50], Batch [20/250] | D Loss: 0.6950 | G Loss: 11.3436 (GAN: 0.8659, L1: 3.3451, Perceptual: 7.1326)\nEpoch [42/50], Batch [30/250] | D Loss: 0.5760 | G Loss: 11.4342 (GAN: 0.7979, L1: 3.3725, Perceptual: 7.2638)\nEpoch [42/50], Batch [40/250] | D Loss: 0.8638 | G Loss: 11.8687 (GAN: 0.7684, L1: 3.5746, Perceptual: 7.5257)\nEpoch [42/50], Batch [50/250] | D Loss: 0.5627 | G Loss: 11.6345 (GAN: 0.7993, L1: 3.3228, Perceptual: 7.5124)\nEpoch [42/50], Batch [60/250] | D Loss: 0.4867 | G Loss: 12.2503 (GAN: 0.7833, L1: 3.7545, Perceptual: 7.7124)\nEpoch [42/50], Batch [70/250] | D Loss: 0.5257 | G Loss: 12.1149 (GAN: 0.7351, L1: 3.6804, Perceptual: 7.6995)\nEpoch [42/50], Batch [80/250] | D Loss: 0.4148 | G Loss: 11.6762 (GAN: 0.7780, L1: 3.1273, Perceptual: 7.7709)\nEpoch [42/50], Batch [90/250] | D Loss: 0.5525 | G Loss: 12.1751 (GAN: 0.8007, L1: 3.7259, Perceptual: 7.6485)\nEpoch [42/50], Batch [100/250] | D Loss: 0.6242 | G Loss: 11.4411 (GAN: 0.7783, L1: 3.2656, Perceptual: 7.3972)\nEpoch [42/50], Batch [110/250] | D Loss: 0.5760 | G Loss: 11.5298 (GAN: 0.9410, L1: 3.2674, Perceptual: 7.3213)\nEpoch [42/50], Batch [120/250] | D Loss: 0.5945 | G Loss: 11.5134 (GAN: 0.8145, L1: 3.4289, Perceptual: 7.2700)\nEpoch [42/50], Batch [130/250] | D Loss: 0.5275 | G Loss: 11.1107 (GAN: 0.7471, L1: 3.1287, Perceptual: 7.2349)\nEpoch [42/50], Batch [140/250] | D Loss: 0.6742 | G Loss: 11.2677 (GAN: 0.9678, L1: 3.2077, Perceptual: 7.0922)\nEpoch [42/50], Batch [150/250] | D Loss: 0.4607 | G Loss: 13.2434 (GAN: 0.9284, L1: 3.9302, Perceptual: 8.3849)\nEpoch [42/50], Batch [160/250] | D Loss: 0.6101 | G Loss: 11.5129 (GAN: 0.9747, L1: 3.2870, Perceptual: 7.2513)\nEpoch [42/50], Batch [170/250] | D Loss: 0.5957 | G Loss: 12.0328 (GAN: 0.7074, L1: 3.5111, Perceptual: 7.8143)\nEpoch [42/50], Batch [180/250] | D Loss: 0.6163 | G Loss: 10.9642 (GAN: 0.7611, L1: 3.1461, Perceptual: 7.0570)\nEpoch [42/50], Batch [190/250] | D Loss: 0.6481 | G Loss: 11.3471 (GAN: 0.7637, L1: 3.7033, Perceptual: 6.8801)\nEpoch [42/50], Batch [200/250] | D Loss: 0.6385 | G Loss: 11.9155 (GAN: 0.7686, L1: 3.5402, Perceptual: 7.6067)\nEpoch [42/50], Batch [210/250] | D Loss: 0.7770 | G Loss: 11.8135 (GAN: 1.1095, L1: 3.5882, Perceptual: 7.1158)\nEpoch [42/50], Batch [220/250] | D Loss: 0.4521 | G Loss: 13.2733 (GAN: 0.8514, L1: 4.3770, Perceptual: 8.0448)\nEpoch [42/50], Batch [230/250] | D Loss: 0.7774 | G Loss: 10.9613 (GAN: 0.9048, L1: 3.2276, Perceptual: 6.8290)\nEpoch [42/50], Batch [240/250] | D Loss: 0.4955 | G Loss: 11.7085 (GAN: 0.7729, L1: 3.3875, Perceptual: 7.5481)\nEpoch [42/50], Batch [250/250] | D Loss: 0.6357 | G Loss: 12.2863 (GAN: 0.9269, L1: 3.5690, Perceptual: 7.7903)\nEpoch 42 completed. Current LR G: 0.000080, LR D: 0.000040\nEpoch [43/50], Batch [10/250] | D Loss: 0.5736 | G Loss: 11.0396 (GAN: 0.6280, L1: 3.1807, Perceptual: 7.2309)\nEpoch [43/50], Batch [20/250] | D Loss: 0.7776 | G Loss: 12.6344 (GAN: 0.8076, L1: 4.1408, Perceptual: 7.6860)\nEpoch [43/50], Batch [30/250] | D Loss: 0.6972 | G Loss: 13.1359 (GAN: 0.6740, L1: 4.5296, Perceptual: 7.9323)\nEpoch [43/50], Batch [40/250] | D Loss: 0.5387 | G Loss: 11.3400 (GAN: 0.8906, L1: 3.3438, Perceptual: 7.1056)\nEpoch [43/50], Batch [50/250] | D Loss: 0.6243 | G Loss: 12.3639 (GAN: 1.0759, L1: 3.6669, Perceptual: 7.6211)\nEpoch [43/50], Batch [60/250] | D Loss: 0.5286 | G Loss: 11.7116 (GAN: 0.8396, L1: 3.7598, Perceptual: 7.1122)\nEpoch [43/50], Batch [70/250] | D Loss: 0.4957 | G Loss: 11.6861 (GAN: 0.8036, L1: 3.5257, Perceptual: 7.3568)\nEpoch [43/50], Batch [80/250] | D Loss: 0.6667 | G Loss: 11.2142 (GAN: 0.8895, L1: 3.0755, Perceptual: 7.2492)\nEpoch [43/50], Batch [90/250] | D Loss: 0.5543 | G Loss: 11.9975 (GAN: 0.8960, L1: 3.4151, Perceptual: 7.6864)\nEpoch [43/50], Batch [100/250] | D Loss: 0.6304 | G Loss: 10.4445 (GAN: 0.7907, L1: 3.0340, Perceptual: 6.6198)\nEpoch [43/50], Batch [110/250] | D Loss: 0.6344 | G Loss: 10.6087 (GAN: 0.6482, L1: 3.0512, Perceptual: 6.9093)\nEpoch [43/50], Batch [120/250] | D Loss: 0.6682 | G Loss: 10.3025 (GAN: 0.8300, L1: 2.8458, Perceptual: 6.6267)\nEpoch [43/50], Batch [130/250] | D Loss: 0.4781 | G Loss: 12.0890 (GAN: 0.7615, L1: 3.7408, Perceptual: 7.5868)\nEpoch [43/50], Batch [140/250] | D Loss: 0.6197 | G Loss: 11.0892 (GAN: 0.9319, L1: 3.0778, Perceptual: 7.0795)\nEpoch [43/50], Batch [150/250] | D Loss: 0.5960 | G Loss: 11.8107 (GAN: 0.7555, L1: 3.5881, Perceptual: 7.4671)\nEpoch [43/50], Batch [160/250] | D Loss: 0.6291 | G Loss: 10.9701 (GAN: 1.0063, L1: 2.9201, Perceptual: 7.0437)\nEpoch [43/50], Batch [170/250] | D Loss: 0.6134 | G Loss: 11.5342 (GAN: 0.8473, L1: 3.3615, Perceptual: 7.3254)\nEpoch [43/50], Batch [180/250] | D Loss: 0.6621 | G Loss: 10.3684 (GAN: 0.8324, L1: 3.0655, Perceptual: 6.4704)\nEpoch [43/50], Batch [190/250] | D Loss: 0.7734 | G Loss: 14.2445 (GAN: 1.0058, L1: 4.5080, Perceptual: 8.7308)\nEpoch [43/50], Batch [200/250] | D Loss: 0.6295 | G Loss: 10.5215 (GAN: 0.6797, L1: 3.1368, Perceptual: 6.7051)\nEpoch [43/50], Batch [210/250] | D Loss: 0.6514 | G Loss: 10.5947 (GAN: 0.7908, L1: 2.9814, Perceptual: 6.8225)\nEpoch [43/50], Batch [220/250] | D Loss: 0.6594 | G Loss: 11.5855 (GAN: 0.7966, L1: 3.5172, Perceptual: 7.2717)\nEpoch [43/50], Batch [230/250] | D Loss: 0.5153 | G Loss: 11.8479 (GAN: 0.8241, L1: 3.3522, Perceptual: 7.6715)\nEpoch [43/50], Batch [240/250] | D Loss: 0.5561 | G Loss: 12.0016 (GAN: 0.7863, L1: 3.6784, Perceptual: 7.5369)\nEpoch [43/50], Batch [250/250] | D Loss: 0.6141 | G Loss: 10.6728 (GAN: 0.8682, L1: 2.9284, Perceptual: 6.8763)\nEpoch 43 completed. Current LR G: 0.000070, LR D: 0.000035\nEpoch [44/50], Batch [10/250] | D Loss: 0.5660 | G Loss: 11.3866 (GAN: 0.8722, L1: 3.1236, Perceptual: 7.3907)\nEpoch [44/50], Batch [20/250] | D Loss: 0.5064 | G Loss: 11.9595 (GAN: 0.8978, L1: 3.7747, Perceptual: 7.2869)\nEpoch [44/50], Batch [30/250] | D Loss: 0.6374 | G Loss: 11.0000 (GAN: 0.8179, L1: 3.2761, Perceptual: 6.9061)\nEpoch [44/50], Batch [40/250] | D Loss: 0.6017 | G Loss: 11.9288 (GAN: 0.7244, L1: 3.5253, Perceptual: 7.6791)\nEpoch [44/50], Batch [50/250] | D Loss: 0.6314 | G Loss: 12.1646 (GAN: 1.0068, L1: 3.6841, Perceptual: 7.4737)\nEpoch [44/50], Batch [60/250] | D Loss: 0.6740 | G Loss: 11.3695 (GAN: 0.9456, L1: 3.1205, Perceptual: 7.3035)\nEpoch [44/50], Batch [70/250] | D Loss: 0.6715 | G Loss: 12.1614 (GAN: 0.7719, L1: 3.7196, Perceptual: 7.6699)\nEpoch [44/50], Batch [80/250] | D Loss: 0.5061 | G Loss: 12.2264 (GAN: 0.8391, L1: 3.9128, Perceptual: 7.4745)\nEpoch [44/50], Batch [90/250] | D Loss: 0.6134 | G Loss: 11.0662 (GAN: 0.7811, L1: 3.1258, Perceptual: 7.1593)\nEpoch [44/50], Batch [100/250] | D Loss: 0.7137 | G Loss: 11.1071 (GAN: 0.9426, L1: 3.0992, Perceptual: 7.0653)\nEpoch [44/50], Batch [110/250] | D Loss: 0.7231 | G Loss: 11.1521 (GAN: 1.0481, L1: 3.0523, Perceptual: 7.0517)\nEpoch [44/50], Batch [120/250] | D Loss: 0.5588 | G Loss: 9.9501 (GAN: 0.8099, L1: 2.8755, Perceptual: 6.2647)\nEpoch [44/50], Batch [130/250] | D Loss: 0.6379 | G Loss: 11.4911 (GAN: 0.6892, L1: 3.4717, Perceptual: 7.3302)\nEpoch [44/50], Batch [140/250] | D Loss: 0.6835 | G Loss: 10.4629 (GAN: 0.9083, L1: 2.8067, Perceptual: 6.7478)\nEpoch [44/50], Batch [150/250] | D Loss: 0.7074 | G Loss: 10.0787 (GAN: 0.8323, L1: 2.8730, Perceptual: 6.3733)\nEpoch [44/50], Batch [160/250] | D Loss: 0.6448 | G Loss: 12.5133 (GAN: 0.8174, L1: 3.7624, Perceptual: 7.9336)\nEpoch [44/50], Batch [170/250] | D Loss: 0.7260 | G Loss: 10.6787 (GAN: 0.7368, L1: 2.9931, Perceptual: 6.9488)\nEpoch [44/50], Batch [180/250] | D Loss: 0.6509 | G Loss: 11.4668 (GAN: 0.7954, L1: 3.2598, Perceptual: 7.4116)\nEpoch [44/50], Batch [190/250] | D Loss: 0.6187 | G Loss: 11.6957 (GAN: 0.8885, L1: 3.4901, Perceptual: 7.3170)\nEpoch [44/50], Batch [200/250] | D Loss: 0.6091 | G Loss: 12.0217 (GAN: 0.7519, L1: 3.5259, Perceptual: 7.7440)\nEpoch [44/50], Batch [210/250] | D Loss: 0.4971 | G Loss: 12.6600 (GAN: 0.9344, L1: 3.7524, Perceptual: 7.9731)\nEpoch [44/50], Batch [220/250] | D Loss: 0.4523 | G Loss: 11.5613 (GAN: 0.9813, L1: 3.3492, Perceptual: 7.2308)\nEpoch [44/50], Batch [230/250] | D Loss: 0.6969 | G Loss: 11.5906 (GAN: 0.7921, L1: 3.8471, Perceptual: 6.9513)\nEpoch [44/50], Batch [240/250] | D Loss: 0.5394 | G Loss: 11.1589 (GAN: 0.8414, L1: 3.2643, Perceptual: 7.0532)\nEpoch [44/50], Batch [250/250] | D Loss: 0.5526 | G Loss: 12.4040 (GAN: 0.8287, L1: 3.8242, Perceptual: 7.7511)\nEpoch 44 completed. Current LR G: 0.000060, LR D: 0.000030\nEpoch [45/50], Batch [10/250] | D Loss: 0.4801 | G Loss: 12.4343 (GAN: 1.0168, L1: 3.7401, Perceptual: 7.6774)\nEpoch [45/50], Batch [20/250] | D Loss: 0.4940 | G Loss: 12.4366 (GAN: 0.7261, L1: 3.8995, Perceptual: 7.8111)\nEpoch [45/50], Batch [30/250] | D Loss: 0.6083 | G Loss: 11.4531 (GAN: 0.7464, L1: 3.5422, Perceptual: 7.1645)\nEpoch [45/50], Batch [40/250] | D Loss: 0.5009 | G Loss: 12.0907 (GAN: 0.8338, L1: 3.4311, Perceptual: 7.8258)\nEpoch [45/50], Batch [50/250] | D Loss: 0.5768 | G Loss: 11.3556 (GAN: 0.9189, L1: 3.1352, Perceptual: 7.3015)\nEpoch [45/50], Batch [60/250] | D Loss: 0.6414 | G Loss: 12.5342 (GAN: 0.8750, L1: 3.9415, Perceptual: 7.7177)\nEpoch [45/50], Batch [70/250] | D Loss: 0.5041 | G Loss: 11.5430 (GAN: 0.7391, L1: 3.5512, Perceptual: 7.2527)\nEpoch [45/50], Batch [80/250] | D Loss: 0.6029 | G Loss: 11.3250 (GAN: 0.8263, L1: 3.4329, Perceptual: 7.0658)\nEpoch [45/50], Batch [90/250] | D Loss: 0.7552 | G Loss: 10.5232 (GAN: 0.8001, L1: 2.8863, Perceptual: 6.8368)\nEpoch [45/50], Batch [100/250] | D Loss: 0.6194 | G Loss: 12.6138 (GAN: 0.8446, L1: 3.7928, Perceptual: 7.9764)\nEpoch [45/50], Batch [110/250] | D Loss: 0.4706 | G Loss: 11.7117 (GAN: 0.8540, L1: 3.5248, Perceptual: 7.3329)\nEpoch [45/50], Batch [120/250] | D Loss: 0.5437 | G Loss: 12.7006 (GAN: 0.8249, L1: 3.9251, Perceptual: 7.9506)\nEpoch [45/50], Batch [130/250] | D Loss: 0.6316 | G Loss: 12.3376 (GAN: 0.8364, L1: 4.0666, Perceptual: 7.4347)\nEpoch [45/50], Batch [140/250] | D Loss: 0.7683 | G Loss: 11.7706 (GAN: 0.7802, L1: 3.6834, Perceptual: 7.3069)\nEpoch [45/50], Batch [150/250] | D Loss: 0.6954 | G Loss: 11.0679 (GAN: 0.7504, L1: 3.4175, Perceptual: 6.9001)\nEpoch [45/50], Batch [160/250] | D Loss: 0.6537 | G Loss: 10.7813 (GAN: 0.7902, L1: 3.0637, Perceptual: 6.9273)\nEpoch [45/50], Batch [170/250] | D Loss: 0.6531 | G Loss: 10.8487 (GAN: 0.9150, L1: 3.0152, Perceptual: 6.9186)\nEpoch [45/50], Batch [180/250] | D Loss: 0.5820 | G Loss: 11.1321 (GAN: 0.8366, L1: 3.1806, Perceptual: 7.1150)\nEpoch [45/50], Batch [190/250] | D Loss: 0.4734 | G Loss: 12.8699 (GAN: 0.8197, L1: 3.8093, Perceptual: 8.2410)\nEpoch [45/50], Batch [200/250] | D Loss: 0.5135 | G Loss: 12.0674 (GAN: 0.8147, L1: 3.5239, Perceptual: 7.7289)\nEpoch [45/50], Batch [210/250] | D Loss: 0.6013 | G Loss: 11.1352 (GAN: 0.8640, L1: 3.0495, Perceptual: 7.2217)\nEpoch [45/50], Batch [220/250] | D Loss: 0.6711 | G Loss: 10.9598 (GAN: 0.7883, L1: 3.1085, Perceptual: 7.0631)\nEpoch [45/50], Batch [230/250] | D Loss: 0.6510 | G Loss: 11.5029 (GAN: 0.7074, L1: 3.5948, Perceptual: 7.2007)\nEpoch [45/50], Batch [240/250] | D Loss: 0.5698 | G Loss: 13.0772 (GAN: 0.7584, L1: 4.0628, Perceptual: 8.2560)\nEpoch [45/50], Batch [250/250] | D Loss: 0.4824 | G Loss: 11.1764 (GAN: 0.8772, L1: 3.0432, Perceptual: 7.2560)\nEpoch 45 completed. Current LR G: 0.000050, LR D: 0.000025\n--- Entering save/checkpoint block for epoch 45 ---\nSaving samples and checkpoint for epoch 45...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_45_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nEpoch [46/50], Batch [10/250] | D Loss: 0.6812 | G Loss: 10.6128 (GAN: 1.0080, L1: 3.0152, Perceptual: 6.5897)\nEpoch [46/50], Batch [20/250] | D Loss: 0.5771 | G Loss: 11.6965 (GAN: 0.8345, L1: 3.4725, Perceptual: 7.3895)\nEpoch [46/50], Batch [30/250] | D Loss: 0.5630 | G Loss: 11.7291 (GAN: 0.8287, L1: 3.6859, Perceptual: 7.2144)\nEpoch [46/50], Batch [40/250] | D Loss: 0.6642 | G Loss: 10.9573 (GAN: 0.9602, L1: 2.9381, Perceptual: 7.0590)\nEpoch [46/50], Batch [50/250] | D Loss: 0.4412 | G Loss: 12.3404 (GAN: 0.9412, L1: 3.8034, Perceptual: 7.5959)\nEpoch [46/50], Batch [60/250] | D Loss: 0.5526 | G Loss: 12.6088 (GAN: 0.7428, L1: 3.7949, Perceptual: 8.0711)\nEpoch [46/50], Batch [70/250] | D Loss: 0.6126 | G Loss: 11.7277 (GAN: 1.0153, L1: 3.3636, Perceptual: 7.3488)\nEpoch [46/50], Batch [80/250] | D Loss: 0.6623 | G Loss: 11.4637 (GAN: 0.7876, L1: 3.2700, Perceptual: 7.4061)\nEpoch [46/50], Batch [90/250] | D Loss: 0.5101 | G Loss: 11.1934 (GAN: 0.8789, L1: 3.1849, Perceptual: 7.1295)\nEpoch [46/50], Batch [100/250] | D Loss: 0.8192 | G Loss: 10.9645 (GAN: 0.9705, L1: 3.1598, Perceptual: 6.8342)\nEpoch [46/50], Batch [110/250] | D Loss: 0.5602 | G Loss: 10.9503 (GAN: 0.9742, L1: 3.1018, Perceptual: 6.8744)\nEpoch [46/50], Batch [120/250] | D Loss: 0.6284 | G Loss: 11.9202 (GAN: 0.8121, L1: 3.4242, Perceptual: 7.6839)\nEpoch [46/50], Batch [130/250] | D Loss: 0.7556 | G Loss: 10.3458 (GAN: 0.8910, L1: 2.7674, Perceptual: 6.6874)\nEpoch [46/50], Batch [140/250] | D Loss: 0.6266 | G Loss: 10.5939 (GAN: 0.7168, L1: 2.8628, Perceptual: 7.0143)\nEpoch [46/50], Batch [150/250] | D Loss: 0.6343 | G Loss: 10.9098 (GAN: 0.7435, L1: 3.1023, Perceptual: 7.0640)\nEpoch [46/50], Batch [160/250] | D Loss: 0.6026 | G Loss: 12.0731 (GAN: 1.0063, L1: 3.5223, Perceptual: 7.5445)\nEpoch [46/50], Batch [170/250] | D Loss: 0.6568 | G Loss: 10.8659 (GAN: 0.9050, L1: 3.0366, Perceptual: 6.9244)\nEpoch [46/50], Batch [180/250] | D Loss: 0.6834 | G Loss: 11.8247 (GAN: 0.7324, L1: 3.4997, Perceptual: 7.5927)\nEpoch [46/50], Batch [190/250] | D Loss: 0.5197 | G Loss: 11.6470 (GAN: 0.8795, L1: 3.2785, Perceptual: 7.4890)\nEpoch [46/50], Batch [200/250] | D Loss: 0.4921 | G Loss: 11.9989 (GAN: 0.7608, L1: 3.5547, Perceptual: 7.6834)\nEpoch [46/50], Batch [210/250] | D Loss: 0.6272 | G Loss: 12.4454 (GAN: 0.7182, L1: 3.7928, Perceptual: 7.9344)\nEpoch [46/50], Batch [220/250] | D Loss: 0.6060 | G Loss: 11.2162 (GAN: 0.9949, L1: 3.3715, Perceptual: 6.8498)\nEpoch [46/50], Batch [230/250] | D Loss: 0.5569 | G Loss: 12.5763 (GAN: 0.7081, L1: 3.8707, Perceptual: 7.9975)\nEpoch [46/50], Batch [240/250] | D Loss: 0.5884 | G Loss: 11.6281 (GAN: 0.7601, L1: 3.3639, Perceptual: 7.5042)\nEpoch [46/50], Batch [250/250] | D Loss: 0.5294 | G Loss: 12.0563 (GAN: 0.8797, L1: 3.8192, Perceptual: 7.3574)\nEpoch 46 completed. Current LR G: 0.000040, LR D: 0.000020\nEpoch [47/50], Batch [10/250] | D Loss: 0.6002 | G Loss: 12.1538 (GAN: 0.6249, L1: 3.6840, Perceptual: 7.8450)\nEpoch [47/50], Batch [20/250] | D Loss: 0.7668 | G Loss: 10.2993 (GAN: 0.9515, L1: 2.8427, Perceptual: 6.5051)\nEpoch [47/50], Batch [30/250] | D Loss: 0.6084 | G Loss: 11.6758 (GAN: 0.8091, L1: 3.8324, Perceptual: 7.0343)\nEpoch [47/50], Batch [40/250] | D Loss: 0.6709 | G Loss: 12.1748 (GAN: 0.9364, L1: 3.7490, Perceptual: 7.4895)\nEpoch [47/50], Batch [50/250] | D Loss: 0.5092 | G Loss: 12.5847 (GAN: 0.8962, L1: 3.8638, Perceptual: 7.8248)\nEpoch [47/50], Batch [60/250] | D Loss: 0.6673 | G Loss: 12.5158 (GAN: 0.9251, L1: 4.1246, Perceptual: 7.4661)\nEpoch [47/50], Batch [70/250] | D Loss: 0.6722 | G Loss: 10.5224 (GAN: 0.7699, L1: 3.1541, Perceptual: 6.5985)\nEpoch [47/50], Batch [80/250] | D Loss: 0.6608 | G Loss: 11.2766 (GAN: 0.7783, L1: 3.3891, Perceptual: 7.1092)\nEpoch [47/50], Batch [90/250] | D Loss: 0.5317 | G Loss: 11.6562 (GAN: 0.6980, L1: 3.7464, Perceptual: 7.2119)\nEpoch [47/50], Batch [100/250] | D Loss: 0.5308 | G Loss: 11.5526 (GAN: 0.8418, L1: 3.4021, Perceptual: 7.3087)\nEpoch [47/50], Batch [110/250] | D Loss: 0.5656 | G Loss: 10.9347 (GAN: 1.0355, L1: 3.2538, Perceptual: 6.6454)\nEpoch [47/50], Batch [120/250] | D Loss: 0.5561 | G Loss: 11.3095 (GAN: 0.8565, L1: 3.2271, Perceptual: 7.2259)\nEpoch [47/50], Batch [130/250] | D Loss: 0.5848 | G Loss: 12.4016 (GAN: 0.6488, L1: 3.8963, Perceptual: 7.8565)\nEpoch [47/50], Batch [140/250] | D Loss: 0.6561 | G Loss: 11.2777 (GAN: 0.9809, L1: 3.3304, Perceptual: 6.9664)\nEpoch [47/50], Batch [150/250] | D Loss: 0.5743 | G Loss: 12.4993 (GAN: 0.7748, L1: 3.9020, Perceptual: 7.8224)\nEpoch [47/50], Batch [160/250] | D Loss: 0.6713 | G Loss: 10.8362 (GAN: 0.8816, L1: 3.3266, Perceptual: 6.6281)\nEpoch [47/50], Batch [170/250] | D Loss: 0.5725 | G Loss: 11.6791 (GAN: 0.7776, L1: 3.7040, Perceptual: 7.1976)\nEpoch [47/50], Batch [180/250] | D Loss: 0.5178 | G Loss: 11.9088 (GAN: 0.9349, L1: 3.5160, Perceptual: 7.4579)\nEpoch [47/50], Batch [190/250] | D Loss: 0.6901 | G Loss: 11.5423 (GAN: 0.8544, L1: 3.7145, Perceptual: 6.9733)\nEpoch [47/50], Batch [200/250] | D Loss: 0.6246 | G Loss: 11.3032 (GAN: 0.9651, L1: 3.1744, Perceptual: 7.1637)\nEpoch [47/50], Batch [210/250] | D Loss: 0.5057 | G Loss: 12.1972 (GAN: 0.8902, L1: 3.8655, Perceptual: 7.4415)\nEpoch [47/50], Batch [220/250] | D Loss: 0.6278 | G Loss: 12.4613 (GAN: 0.8949, L1: 3.8618, Perceptual: 7.7046)\nEpoch [47/50], Batch [230/250] | D Loss: 0.6587 | G Loss: 11.3544 (GAN: 0.8892, L1: 3.1724, Perceptual: 7.2928)\nEpoch [47/50], Batch [240/250] | D Loss: 0.6326 | G Loss: 11.7765 (GAN: 0.8349, L1: 3.3916, Perceptual: 7.5500)\nEpoch [47/50], Batch [250/250] | D Loss: 0.5881 | G Loss: 11.6796 (GAN: 0.9246, L1: 3.4616, Perceptual: 7.2934)\nEpoch 47 completed. Current LR G: 0.000030, LR D: 0.000015\nEpoch [48/50], Batch [10/250] | D Loss: 0.4704 | G Loss: 12.4755 (GAN: 0.9004, L1: 3.8632, Perceptual: 7.7120)\nEpoch [48/50], Batch [20/250] | D Loss: 0.6443 | G Loss: 11.4896 (GAN: 0.7481, L1: 3.4514, Perceptual: 7.2901)\nEpoch [48/50], Batch [30/250] | D Loss: 0.6981 | G Loss: 12.1514 (GAN: 0.7290, L1: 3.7146, Perceptual: 7.7078)\nEpoch [48/50], Batch [40/250] | D Loss: 0.6450 | G Loss: 11.3027 (GAN: 0.8591, L1: 3.2337, Perceptual: 7.2098)\nEpoch [48/50], Batch [50/250] | D Loss: 0.5275 | G Loss: 11.8040 (GAN: 0.8509, L1: 3.4974, Perceptual: 7.4556)\nEpoch [48/50], Batch [60/250] | D Loss: 0.6126 | G Loss: 11.2980 (GAN: 0.6540, L1: 3.3750, Perceptual: 7.2689)\nEpoch [48/50], Batch [70/250] | D Loss: 0.5493 | G Loss: 11.6258 (GAN: 0.9136, L1: 3.2380, Perceptual: 7.4741)\nEpoch [48/50], Batch [80/250] | D Loss: 0.5588 | G Loss: 12.1981 (GAN: 0.7631, L1: 3.6488, Perceptual: 7.7862)\nEpoch [48/50], Batch [90/250] | D Loss: 0.6073 | G Loss: 11.1844 (GAN: 1.0430, L1: 3.1413, Perceptual: 7.0001)\nEpoch [48/50], Batch [100/250] | D Loss: 0.7168 | G Loss: 11.2955 (GAN: 0.9118, L1: 3.0853, Perceptual: 7.2983)\nEpoch [48/50], Batch [110/250] | D Loss: 0.5133 | G Loss: 12.0759 (GAN: 0.8187, L1: 3.7245, Perceptual: 7.5326)\nEpoch [48/50], Batch [120/250] | D Loss: 0.5467 | G Loss: 11.7450 (GAN: 0.8416, L1: 3.3044, Perceptual: 7.5989)\nEpoch [48/50], Batch [130/250] | D Loss: 0.6750 | G Loss: 10.4009 (GAN: 0.6840, L1: 3.2495, Perceptual: 6.4675)\nEpoch [48/50], Batch [140/250] | D Loss: 0.5203 | G Loss: 11.9742 (GAN: 0.8551, L1: 3.5828, Perceptual: 7.5364)\nEpoch [48/50], Batch [150/250] | D Loss: 0.7171 | G Loss: 12.3312 (GAN: 0.6088, L1: 4.0500, Perceptual: 7.6724)\nEpoch [48/50], Batch [160/250] | D Loss: 0.4576 | G Loss: 12.1526 (GAN: 0.9093, L1: 3.6075, Perceptual: 7.6359)\nEpoch [48/50], Batch [170/250] | D Loss: 0.6549 | G Loss: 11.6259 (GAN: 0.9659, L1: 3.4595, Perceptual: 7.2005)\nEpoch [48/50], Batch [180/250] | D Loss: 0.6143 | G Loss: 11.6570 (GAN: 0.9012, L1: 3.2492, Perceptual: 7.5065)\nEpoch [48/50], Batch [190/250] | D Loss: 0.7493 | G Loss: 11.9605 (GAN: 0.6998, L1: 3.9428, Perceptual: 7.3179)\nEpoch [48/50], Batch [200/250] | D Loss: 0.5648 | G Loss: 10.9723 (GAN: 0.9715, L1: 2.9515, Perceptual: 7.0493)\nEpoch [48/50], Batch [210/250] | D Loss: 0.6911 | G Loss: 10.9379 (GAN: 0.7549, L1: 3.1369, Perceptual: 7.0461)\nEpoch [48/50], Batch [220/250] | D Loss: 0.6098 | G Loss: 11.4788 (GAN: 0.7715, L1: 3.2838, Perceptual: 7.4235)\nEpoch [48/50], Batch [230/250] | D Loss: 0.6786 | G Loss: 10.2448 (GAN: 0.8717, L1: 2.6616, Perceptual: 6.7115)\nEpoch [48/50], Batch [240/250] | D Loss: 0.6147 | G Loss: 11.5717 (GAN: 0.9194, L1: 3.3017, Perceptual: 7.3506)\nEpoch [48/50], Batch [250/250] | D Loss: 0.7315 | G Loss: 10.8600 (GAN: 0.7974, L1: 3.3306, Perceptual: 6.7320)\nEpoch 48 completed. Current LR G: 0.000020, LR D: 0.000010\nEpoch [49/50], Batch [10/250] | D Loss: 0.4619 | G Loss: 13.5302 (GAN: 0.8516, L1: 4.2050, Perceptual: 8.4736)\nEpoch [49/50], Batch [20/250] | D Loss: 0.5233 | G Loss: 12.1238 (GAN: 0.9185, L1: 3.8207, Perceptual: 7.3845)\nEpoch [49/50], Batch [30/250] | D Loss: 0.5992 | G Loss: 11.8754 (GAN: 1.0920, L1: 3.5858, Perceptual: 7.1976)\nEpoch [49/50], Batch [40/250] | D Loss: 0.6695 | G Loss: 11.7701 (GAN: 0.9421, L1: 3.3142, Perceptual: 7.5138)\nEpoch [49/50], Batch [50/250] | D Loss: 0.6487 | G Loss: 11.4999 (GAN: 0.7050, L1: 3.4799, Perceptual: 7.3150)\nEpoch [49/50], Batch [60/250] | D Loss: 0.5839 | G Loss: 12.1311 (GAN: 0.9641, L1: 3.5469, Perceptual: 7.6201)\nEpoch [49/50], Batch [70/250] | D Loss: 0.5722 | G Loss: 11.2651 (GAN: 0.8387, L1: 3.1950, Perceptual: 7.2314)\nEpoch [49/50], Batch [80/250] | D Loss: 0.6672 | G Loss: 11.8911 (GAN: 0.6725, L1: 3.6507, Perceptual: 7.5679)\nEpoch [49/50], Batch [90/250] | D Loss: 0.5757 | G Loss: 11.1626 (GAN: 1.1810, L1: 3.0158, Perceptual: 6.9658)\nEpoch [49/50], Batch [100/250] | D Loss: 0.5895 | G Loss: 11.7956 (GAN: 0.8329, L1: 3.4094, Perceptual: 7.5533)\nEpoch [49/50], Batch [110/250] | D Loss: 0.5792 | G Loss: 10.9001 (GAN: 0.8956, L1: 3.0031, Perceptual: 7.0013)\nEpoch [49/50], Batch [120/250] | D Loss: 0.6479 | G Loss: 11.3554 (GAN: 0.6489, L1: 3.2302, Perceptual: 7.4762)\nEpoch [49/50], Batch [130/250] | D Loss: 0.6642 | G Loss: 11.6352 (GAN: 0.7783, L1: 3.4836, Perceptual: 7.3733)\nEpoch [49/50], Batch [140/250] | D Loss: 0.6246 | G Loss: 10.9138 (GAN: 1.0215, L1: 2.9697, Perceptual: 6.9226)\nEpoch [49/50], Batch [150/250] | D Loss: 0.6889 | G Loss: 11.5824 (GAN: 0.9863, L1: 3.8092, Perceptual: 6.7869)\nEpoch [49/50], Batch [160/250] | D Loss: 0.5799 | G Loss: 10.9493 (GAN: 1.0345, L1: 3.2365, Perceptual: 6.6783)\nEpoch [49/50], Batch [170/250] | D Loss: 0.5977 | G Loss: 13.2160 (GAN: 0.6544, L1: 4.2519, Perceptual: 8.3098)\nEpoch [49/50], Batch [180/250] | D Loss: 0.4969 | G Loss: 11.3481 (GAN: 1.0055, L1: 3.4404, Perceptual: 6.9022)\nEpoch [49/50], Batch [190/250] | D Loss: 0.5477 | G Loss: 10.6957 (GAN: 0.9087, L1: 3.1505, Perceptual: 6.6364)\nEpoch [49/50], Batch [200/250] | D Loss: 0.7187 | G Loss: 11.9692 (GAN: 0.8489, L1: 3.5917, Perceptual: 7.5285)\nEpoch [49/50], Batch [210/250] | D Loss: 0.5819 | G Loss: 11.5420 (GAN: 0.8484, L1: 3.4986, Perceptual: 7.1950)\nEpoch [49/50], Batch [220/250] | D Loss: 0.5305 | G Loss: 12.7062 (GAN: 1.0091, L1: 3.9093, Perceptual: 7.7878)\nEpoch [49/50], Batch [230/250] | D Loss: 0.7044 | G Loss: 10.6724 (GAN: 0.8802, L1: 3.1030, Perceptual: 6.6892)\nEpoch [49/50], Batch [240/250] | D Loss: 0.5543 | G Loss: 11.5055 (GAN: 0.7588, L1: 3.2826, Perceptual: 7.4641)\nEpoch [49/50], Batch [250/250] | D Loss: 0.6464 | G Loss: 11.2743 (GAN: 0.7623, L1: 3.1491, Perceptual: 7.3629)\nEpoch 49 completed. Current LR G: 0.000010, LR D: 0.000005\nEpoch [50/50], Batch [10/250] | D Loss: 0.4762 | G Loss: 12.3619 (GAN: 0.8021, L1: 3.6904, Perceptual: 7.8694)\nEpoch [50/50], Batch [20/250] | D Loss: 0.5753 | G Loss: 12.4422 (GAN: 0.8443, L1: 3.5213, Perceptual: 8.0766)\nEpoch [50/50], Batch [30/250] | D Loss: 0.6494 | G Loss: 11.6078 (GAN: 0.7999, L1: 3.6996, Perceptual: 7.1083)\nEpoch [50/50], Batch [40/250] | D Loss: 0.5554 | G Loss: 11.3337 (GAN: 0.8226, L1: 3.2159, Perceptual: 7.2951)\nEpoch [50/50], Batch [50/250] | D Loss: 0.4670 | G Loss: 12.0933 (GAN: 0.9252, L1: 3.6589, Perceptual: 7.5091)\nEpoch [50/50], Batch [60/250] | D Loss: 0.6423 | G Loss: 11.4893 (GAN: 1.0269, L1: 3.3271, Perceptual: 7.1353)\nEpoch [50/50], Batch [70/250] | D Loss: 0.5522 | G Loss: 12.0222 (GAN: 0.7683, L1: 3.6149, Perceptual: 7.6390)\nEpoch [50/50], Batch [80/250] | D Loss: 0.5750 | G Loss: 12.0883 (GAN: 0.6238, L1: 3.8721, Perceptual: 7.5924)\nEpoch [50/50], Batch [90/250] | D Loss: 0.6545 | G Loss: 12.1150 (GAN: 1.0197, L1: 3.7153, Perceptual: 7.3800)\nEpoch [50/50], Batch [100/250] | D Loss: 0.5243 | G Loss: 11.8345 (GAN: 0.8587, L1: 3.3643, Perceptual: 7.6115)\nEpoch [50/50], Batch [110/250] | D Loss: 0.5766 | G Loss: 11.7805 (GAN: 0.8218, L1: 3.4098, Perceptual: 7.5489)\nEpoch [50/50], Batch [120/250] | D Loss: 0.7635 | G Loss: 10.0226 (GAN: 1.0370, L1: 2.7860, Perceptual: 6.1996)\nEpoch [50/50], Batch [130/250] | D Loss: 0.6102 | G Loss: 12.3244 (GAN: 0.7847, L1: 3.6391, Perceptual: 7.9006)\nEpoch [50/50], Batch [140/250] | D Loss: 0.6321 | G Loss: 11.5483 (GAN: 0.8977, L1: 3.6059, Perceptual: 7.0447)\nEpoch [50/50], Batch [150/250] | D Loss: 0.4915 | G Loss: 12.6165 (GAN: 0.9759, L1: 3.9633, Perceptual: 7.6773)\nEpoch [50/50], Batch [160/250] | D Loss: 0.3825 | G Loss: 13.2506 (GAN: 0.8527, L1: 4.1206, Perceptual: 8.2773)\nEpoch [50/50], Batch [170/250] | D Loss: 0.5668 | G Loss: 11.1755 (GAN: 0.7877, L1: 3.4099, Perceptual: 6.9779)\nEpoch [50/50], Batch [180/250] | D Loss: 0.6219 | G Loss: 11.4288 (GAN: 0.8283, L1: 3.2957, Perceptual: 7.3048)\nEpoch [50/50], Batch [190/250] | D Loss: 0.7204 | G Loss: 12.0860 (GAN: 0.8861, L1: 3.8456, Perceptual: 7.3542)\nEpoch [50/50], Batch [200/250] | D Loss: 0.5237 | G Loss: 12.6294 (GAN: 0.7194, L1: 3.9112, Perceptual: 7.9988)\nEpoch [50/50], Batch [210/250] | D Loss: 0.6753 | G Loss: 10.9231 (GAN: 0.8758, L1: 3.0741, Perceptual: 6.9732)\nEpoch [50/50], Batch [220/250] | D Loss: 0.6183 | G Loss: 10.9748 (GAN: 0.8641, L1: 3.2926, Perceptual: 6.8182)\nEpoch [50/50], Batch [230/250] | D Loss: 0.6127 | G Loss: 10.9478 (GAN: 0.7334, L1: 3.0503, Perceptual: 7.1640)\nEpoch [50/50], Batch [240/250] | D Loss: 0.6111 | G Loss: 11.8745 (GAN: 0.7941, L1: 3.6708, Perceptual: 7.4095)\nEpoch [50/50], Batch [250/250] | D Loss: 0.5320 | G Loss: 12.9170 (GAN: 0.7286, L1: 4.2223, Perceptual: 7.9662)\nEpoch 50 completed. Current LR G: 0.000000, LR D: 0.000000\n--- Entering save/checkpoint block for epoch 50 ---\nSaving samples and checkpoint for epoch 50...\nSuccessfully saved image to: /kaggle/working/generated_images/epoch_50_batch_250.png\nModels saved to /kaggle/working/generated_images/checkpoints\nTraining complete.\n\nStarting Evaluation...\nEval Batch [10/250] | Avg PSNR: 28.91 | Avg SSIM: 0.7820\nEval Batch [20/250] | Avg PSNR: 28.93 | Avg SSIM: 0.7830\nEval Batch [30/250] | Avg PSNR: 29.10 | Avg SSIM: 0.7885\nEval Batch [40/250] | Avg PSNR: 29.07 | Avg SSIM: 0.7882\nEval Batch [50/250] | Avg PSNR: 29.12 | Avg SSIM: 0.7896\nEval Batch [60/250] | Avg PSNR: 29.11 | Avg SSIM: 0.7899\nEval Batch [70/250] | Avg PSNR: 29.13 | Avg SSIM: 0.7914\nEval Batch [80/250] | Avg PSNR: 29.17 | Avg SSIM: 0.7921\nEval Batch [90/250] | Avg PSNR: 29.18 | Avg SSIM: 0.7932\nEval Batch [100/250] | Avg PSNR: 29.17 | Avg SSIM: 0.7921\nEval Batch [110/250] | Avg PSNR: 29.15 | Avg SSIM: 0.7916\nEval Batch [120/250] | Avg PSNR: 29.15 | Avg SSIM: 0.7917\nEval Batch [130/250] | Avg PSNR: 29.17 | Avg SSIM: 0.7923\nEval Batch [140/250] | Avg PSNR: 29.11 | Avg SSIM: 0.7907\nEval Batch [150/250] | Avg PSNR: 29.13 | Avg SSIM: 0.7912\nEval Batch [160/250] | Avg PSNR: 29.11 | Avg SSIM: 0.7905\nEval Batch [170/250] | Avg PSNR: 29.11 | Avg SSIM: 0.7908\nEval Batch [180/250] | Avg PSNR: 29.12 | Avg SSIM: 0.7915\nEval Batch [190/250] | Avg PSNR: 29.13 | Avg SSIM: 0.7917\nEval Batch [200/250] | Avg PSNR: 29.16 | Avg SSIM: 0.7923\nEval Batch [210/250] | Avg PSNR: 29.16 | Avg SSIM: 0.7925\nEval Batch [220/250] | Avg PSNR: 29.15 | Avg SSIM: 0.7922\nEval Batch [230/250] | Avg PSNR: 29.16 | Avg SSIM: 0.7925\nEval Batch [240/250] | Avg PSNR: 29.15 | Avg SSIM: 0.7923\nEval Batch [250/250] | Avg PSNR: 29.13 | Avg SSIM: 0.7916\n\n--- Evaluation Results ---\nAverage PSNR: 29.13\nAverage SSIM: 0.7916\nEvaluation complete.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class Config:\n    DATA_ROOT = 'kvasirseg/Kvasir-SEG/images' # Path to the high-quality images\n\nprint(f\"Checking images in: {Config.DATA_ROOT}\")\n\nif not os.path.exists(Config.DATA_ROOT):\n    print(f\"Error: The specified data root directory does NOT exist: {Config.DATA_ROOT}\")\n    print(\"Please ensure your Kvasir-SEG dataset is correctly extracted and placed.\")\n    print(\"Expected structure: /kaggle/working/kvasirseg/Kvasir-SEG/images/\")\nelse:\n    image_files = [f for f in os.listdir(Config.DATA_ROOT) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    if len(image_files) == 0:\n        print(f\"Warning: No image files (jpg, jpeg, png) found in {Config.DATA_ROOT}.\")\n        print(\"Please verify the contents of this directory.\")\n    else:\n        print(f\"Found {len(image_files)} image files. Here are the first 5:\")\n        for i, filename in enumerate(image_files[:5]):\n            print(os.path.join(Config.DATA_ROOT, filename))\n        print(\"Dataset path appears correct and images are found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T11:06:32.353963Z","iopub.execute_input":"2025-06-05T11:06:32.354202Z","iopub.status.idle":"2025-06-05T11:06:32.361172Z","shell.execute_reply.started":"2025-06-05T11:06:32.354178Z","shell.execute_reply":"2025-06-05T11:06:32.360475Z"}},"outputs":[{"name":"stdout","text":"Checking images in: kvasirseg/Kvasir-SEG/images\nError: The specified data root directory does NOT exist: kvasirseg/Kvasir-SEG/images\nPlease ensure your Kvasir-SEG dataset is correctly extracted and placed.\nExpected structure: /kaggle/working/kvasirseg/Kvasir-SEG/images/\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}