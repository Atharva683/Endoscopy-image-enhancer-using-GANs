{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Enhancement GAN - Local Setup\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a Generative Adversarial Network (GAN) for enhancing medical images from the Kvasir-SEG dataset. The model takes degraded medical images and restores them to high quality using a U-Net generator and discriminator architecture.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Install Required Dependencies\n",
    "```bash\n",
    "pip install torch torchvision matplotlib pillow numpy scikit-image\n",
    "```\n",
    "\n",
    "### 2. Download and Prepare Dataset\n",
    "1. Download the Kvasir-SEG dataset from: https://datasets.simula.no/kvasir-seg/\n",
    "2. Extract the dataset in your project directory\n",
    "3. Ensure the folder structure is:\n",
    "   ```\n",
    "   your_project/\n",
    "   ├── data/\n",
    "   │   └── Kvasir-SEG/\n",
    "   │       └── images/\n",
    "   │           ├── image_00001.jpg\n",
    "   │           ├── image_00002.jpg\n",
    "   │           └── ...\n",
    "   ├── output/ (will be created automatically)\n",
    "   └── ganproject (2).ipynb\n",
    "   ```\n",
    "\n",
    "### 3. System Requirements\n",
    "- GPU recommended (CUDA compatible) for faster training\n",
    "- At least 8GB RAM\n",
    "- Python 3.7+ with PyTorch installed\n",
    "\n",
    "### 4. Expected Output\n",
    "- Training will create enhanced images in `output/generated_images/`\n",
    "- Model checkpoints will be saved in `output/generated_images/checkpoints/`\n",
    "- Training progress will be displayed with loss metrics and sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:47.871735Z",
     "iopub.status.busy": "2025-06-05T08:30:47.871221Z",
     "iopub.status.idle": "2025-06-05T08:30:53.495744Z",
     "shell.execute_reply": "2025-06-05T08:30:53.495155Z",
     "shell.execute_reply.started": "2025-06-05T08:30:47.871710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg19\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Code Explanation\n",
    "\n",
    "### 1. Import Libraries Analysis\n",
    "\n",
    "Let's break down each import and understand its purpose in our GAN implementation:\n",
    "\n",
    "**Deep Learning Framework:**\n",
    "- `torch`: Core PyTorch library for tensor operations and neural networks\n",
    "- `torch.nn`: Neural network modules (layers, activation functions, loss functions)\n",
    "- `torch.optim`: Optimization algorithms (Adam, SGD, etc.)\n",
    "\n",
    "**Data Handling:**\n",
    "- `Dataset`: Base class for creating custom datasets\n",
    "- `DataLoader`: Efficient data loading with batching and shuffling\n",
    "- `transforms`: Image preprocessing and augmentation utilities\n",
    "- `vgg19`: Pre-trained VGG19 model for perceptual loss calculation\n",
    "\n",
    "**Image Processing:**\n",
    "- `PIL.Image`: Loading and basic image operations\n",
    "- `numpy`: Numerical operations and array manipulations\n",
    "- `skimage.metrics`: Image quality metrics (PSNR, SSIM)\n",
    "\n",
    "**Utilities:**\n",
    "- `matplotlib.pyplot`: Plotting and visualization\n",
    "- `os`: File system operations\n",
    "- `random`: Random number generation for data augmentation\n",
    "- `shutil`: High-level file operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.497312Z",
     "iopub.status.busy": "2025-06-05T08:30:53.496916Z",
     "iopub.status.idle": "2025-06-05T08:30:53.584816Z",
     "shell.execute_reply": "2025-06-05T08:30:53.584028Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.497291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Device Configuration\n",
    "\n",
    "This line automatically detects the best available hardware for training:\n",
    "\n",
    "- **CUDA GPU**: If available, uses GPU acceleration (much faster for deep learning)\n",
    "- **CPU Fallback**: Uses CPU if no GPU is detected (slower but still functional)\n",
    "\n",
    "**Why GPU is Important:**\n",
    "- Neural networks involve massive matrix operations\n",
    "- GPUs have thousands of cores optimized for parallel computation\n",
    "- Training time can be 10-100x faster on GPU vs CPU\n",
    "- Essential for practical deep learning model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.586056Z",
     "iopub.status.busy": "2025-06-05T08:30:53.585771Z",
     "iopub.status.idle": "2025-06-05T08:30:53.597077Z",
     "shell.execute_reply": "2025-06-05T08:30:53.596396Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.586031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    # Local paths for running on your device\n",
    "    DATA_ROOT = 'kvasir-seg\\Kvasir-SEG\\images'  # Local path to the dataset\n",
    "    OUTPUT_DIR = 'output/generated_images'  # Local output directory\n",
    "    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
    "\n",
    "    IMAGE_SIZE = 256\n",
    "    NUM_CHANNELS = 3\n",
    "\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE_G = 2e-4\n",
    "    LEARNING_RATE_D = 1e-4\n",
    "    BETA1 = 0.5\n",
    "    LAMBDA_L1 = 70.0\n",
    "    LAMBDA_PERCEPTUAL = 10.0\n",
    "\n",
    "   \n",
    "    G_TRAIN_MULTIPLIER = 3 \n",
    "\n",
    "    NOISE_STD_DEV = 0.15\n",
    "    NOISE_STD_DEV_VARIATION = 0.05\n",
    "    \n",
    "    SALT_VS_PEPPER_RATIO = 0.5\n",
    "    SP_NOISE_PROB = 0.01\n",
    "    \n",
    "    BLUR_KERNEL_SIZE = 5\n",
    "    BRIGHTNESS_FACTOR = 0.2\n",
    "    CONTRAST_FACTOR = 0.2\n",
    "    SATURATION_FACTOR = 0.2\n",
    "    HUE_FACTOR = 0.05\n",
    "\n",
    "    \n",
    "    LR_DECAY_START_EPOCH = 30 \n",
    "    SAVE_EVERY_N_EPOCHS = 5\n",
    "    LOG_EVERY_N_BATCHES = 10\n",
    "\n",
    "    DEVICE = DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration Class - Detailed Parameter Analysis\n",
    "\n",
    "The Config class centralizes all hyperparameters and settings. Let's examine each category:\n",
    "\n",
    "#### **A. File Paths & Directory Structure**\n",
    "```python\n",
    "DATA_ROOT = 'kvasir-seg\\Kvasir-SEG\\images'  # Input dataset location\n",
    "OUTPUT_DIR = 'output/generated_images'       # Where results are saved\n",
    "CHECKPOINT_DIR = ...                         # Model weights storage\n",
    "```\n",
    "\n",
    "#### **B. Image Processing Parameters**\n",
    "```python\n",
    "IMAGE_SIZE = 256        # All images resized to 256x256 pixels\n",
    "NUM_CHANNELS = 3        # RGB color channels (Red, Green, Blue)\n",
    "```\n",
    "- **Why 256x256?** Good balance between detail preservation and computational efficiency\n",
    "- **RGB channels:** Standard color representation for medical images\n",
    "\n",
    "#### **C. Training Hyperparameters**\n",
    "```python\n",
    "BATCH_SIZE = 4              # Process 4 images simultaneously\n",
    "NUM_EPOCHS = 50             # Complete 50 training cycles\n",
    "LEARNING_RATE_G = 2e-4      # Generator learning rate (0.0002)\n",
    "LEARNING_RATE_D = 1e-4      # Discriminator learning rate (0.0001)\n",
    "BETA1 = 0.5                 # Adam optimizer momentum parameter\n",
    "```\n",
    "\n",
    "**Critical Details:**\n",
    "- **Different Learning Rates:** Generator learns faster than discriminator to prevent discriminator from becoming too powerful\n",
    "- **Small Batch Size:** Accommodates limited GPU memory, especially important for medical imaging\n",
    "- **Adam Optimizer:** Adaptive learning rate algorithm, excellent for GANs\n",
    "\n",
    "#### **D. Loss Function Weights**\n",
    "```python\n",
    "LAMBDA_L1 = 70.0           # Pixel-wise reconstruction importance\n",
    "LAMBDA_PERCEPTUAL = 10.0   # High-level feature similarity importance\n",
    "G_TRAIN_MULTIPLIER = 3     # Train generator 3x more than discriminator\n",
    "```\n",
    "\n",
    "**Why These Weights Matter:**\n",
    "- **High L1 weight (70.0):** Ensures generated images are pixel-accurate\n",
    "- **Perceptual weight (10.0):** Maintains visual realism and texture quality\n",
    "- **Generator multiplier:** Prevents discriminator from overpowering generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.598211Z",
     "iopub.status.busy": "2025-06-05T08:30:53.597940Z",
     "iopub.status.idle": "2025-06-05T08:30:53.609662Z",
     "shell.execute_reply": "2025-06-05T08:30:53.609022Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.598187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_degradation(image_tensor, config):\n",
    "    image_tensor_cpu = image_tensor.cpu()\n",
    "    degraded_tensor = image_tensor_cpu.clone() \n",
    "\n",
    "    current_noise_std_dev = config.NOISE_STD_DEV + random.uniform(-config.NOISE_STD_DEV_VARIATION, config.NOISE_STD_DEV_VARIATION)\n",
    "    current_noise_std_dev = max(0, current_noise_std_dev)\n",
    "\n",
    "    gaussian_noise = torch.randn_like(degraded_tensor) * current_noise_std_dev\n",
    "    degraded_tensor = degraded_tensor + gaussian_noise\n",
    "    degraded_tensor = torch.clamp(degraded_tensor, 0, 1)\n",
    "\n",
    "\n",
    "    num_pixels = degraded_tensor.numel()\n",
    "    num_sp_noise_pixels = int(num_pixels * config.SP_NOISE_PROB)\n",
    "\n",
    "    noise_indices = torch.randperm(num_pixels)[:num_sp_noise_pixels]\n",
    "\n",
    "    salt_mask = torch.rand(num_sp_noise_pixels) < config.SALT_VS_PEPPER_RATIO\n",
    "    pepper_mask = ~salt_mask\n",
    "\n",
    "    degraded_tensor_flat = degraded_tensor.view(-1)\n",
    "\n",
    "    # Apply pepper noise (set to 0)\n",
    "    degraded_tensor_flat[noise_indices[pepper_mask]] = 0.0\n",
    "\n",
    "    # Apply salt noise (set to 1)\n",
    "    degraded_tensor_flat[noise_indices[salt_mask]] = 1.0\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    degraded_tensor = degraded_tensor_flat.view(degraded_tensor.shape)\n",
    "\n",
    "\n",
    "    if config.BLUR_KERNEL_SIZE > 0:\n",
    "        blur_transform = transforms.GaussianBlur(\n",
    "            kernel_size=(config.BLUR_KERNEL_SIZE, config.BLUR_KERNEL_SIZE),\n",
    "            sigma=(0.1, 2.0)\n",
    "        )\n",
    "        degraded_tensor = blur_transform(degraded_tensor)\n",
    "\n",
    "    jitter_transform = transforms.ColorJitter(\n",
    "        brightness=config.BRIGHTNESS_FACTOR,\n",
    "        contrast=config.CONTRAST_FACTOR,\n",
    "        saturation=config.SATURATION_FACTOR,\n",
    "        hue=config.HUE_FACTOR\n",
    "    )\n",
    "    degraded_tensor = jitter_transform(degraded_tensor)\n",
    "\n",
    "    return degraded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Image Degradation Function - Simulating Real-World Problems\n",
    "\n",
    "This function artificially degrades high-quality images to create training pairs. Here's the step-by-step process:\n",
    "\n",
    "#### **Step 1: Gaussian Noise Addition**\n",
    "```python\n",
    "current_noise_std_dev = config.NOISE_STD_DEV + random.uniform(...)\n",
    "gaussian_noise = torch.randn_like(degraded_tensor) * current_noise_std_dev\n",
    "```\n",
    "**Purpose:** Simulates sensor noise, electrical interference, or compression artifacts\n",
    "**Implementation:** Adds random values from normal distribution to each pixel\n",
    "**Randomization:** Varies noise intensity for each image to increase training diversity\n",
    "\n",
    "#### **Step 2: Salt & Pepper Noise**\n",
    "```python\n",
    "num_sp_noise_pixels = int(num_pixels * config.SP_NOISE_PROB)  # 1% of pixels\n",
    "salt_mask = torch.rand(num_sp_noise_pixels) < config.SALT_VS_PEPPER_RATIO\n",
    "```\n",
    "**Salt Noise:** Random pixels set to maximum value (white spots)\n",
    "**Pepper Noise:** Random pixels set to minimum value (black spots)\n",
    "**Medical Relevance:** Simulates dead pixels, dust on lens, or transmission errors\n",
    "\n",
    "#### **Step 3: Gaussian Blur**\n",
    "```python\n",
    "blur_transform = transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0))\n",
    "```\n",
    "**Purpose:** Simulates motion blur, focus issues, or optical aberrations\n",
    "**Kernel Size:** 5x5 pixel neighborhood for blur calculation\n",
    "**Variable Sigma:** Random blur intensity for training variety\n",
    "\n",
    "#### **Step 4: Color Jitter**\n",
    "```python\n",
    "jitter_transform = transforms.ColorJitter(\n",
    "    brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05\n",
    ")\n",
    "```\n",
    "**Brightness:** ±20% illumination changes\n",
    "**Contrast:** ±20% contrast variations\n",
    "**Saturation:** ±20% color intensity changes\n",
    "**Hue:** ±5% color shift (small to preserve medical accuracy)\n",
    "\n",
    "**Why This Approach?**\n",
    "- Creates realistic training data without needing actual degraded medical images\n",
    "- Allows controlled experimentation with different degradation types\n",
    "- Ensures the model learns to handle various real-world imaging problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.611669Z",
     "iopub.status.busy": "2025-06-05T08:30:53.611397Z",
     "iopub.status.idle": "2025-06-05T08:30:53.624904Z",
     "shell.execute_reply": "2025-06-05T08:30:53.624298Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.611651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_images(low_res, real_high_res, fake_high_res, epoch, batch_idx, output_dir):\n",
    "    low_res_np = (low_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    real_high_res_np = (real_high_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    fake_high_res_np = (fake_high_res * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(Config.BATCH_SIZE, 3, figsize=(9, 3 * Config.BATCH_SIZE))\n",
    "    fig.suptitle(f'Epoch {epoch}, Batch {batch_idx}', fontsize=16)\n",
    "\n",
    "    for i in range(Config.BATCH_SIZE):\n",
    "        axes[i, 0].imshow(low_res_np[i])\n",
    "        axes[i, 0].set_title('Low-Quality Input')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(real_high_res_np[i])\n",
    "        axes[i, 1].set_title('Real High-Quality')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(fake_high_res_np[i])\n",
    "        axes[i, 2].set_title('Generated High-Quality')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    filename = os.path.join(output_dir, f'epoch_{epoch}_batch_{batch_idx}.png')\n",
    "    \n",
    "    try:\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Successfully saved image to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image {filename}: {e}\")\n",
    "    finally:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.625880Z",
     "iopub.status.busy": "2025-06-05T08:30:53.625639Z",
     "iopub.status.idle": "2025-06-05T08:30:53.636995Z",
     "shell.execute_reply": "2025-06-05T08:30:53.636402Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.625863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(real_images, fake_images):\n",
    "    real_images_np = (real_images * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    fake_images_np = (fake_images * 0.5 + 0.5).cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "\n",
    "    for i in range(real_images_np.shape[0]):\n",
    "        psnr = peak_signal_noise_ratio(real_images_np[i], fake_images_np[i], data_range=1.0)\n",
    "        ssim = structural_similarity(real_images_np[i], fake_images_np[i], data_range=1.0, channel_axis=-1)\n",
    "        psnr_scores.append(psnr)\n",
    "        ssim_scores.append(ssim)\n",
    "    \n",
    "    return np.mean(psnr_scores), np.mean(ssim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.637827Z",
     "iopub.status.busy": "2025-06-05T08:30:53.637646Z",
     "iopub.status.idle": "2025-06-05T08:30:53.646914Z",
     "shell.execute_reply": "2025-06-05T08:30:53.646268Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.637812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KvasirSEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size, transform=None, degradation_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.transform = transform\n",
    "        self.degradation_transform = degradation_transform\n",
    "\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.image_files)} images from {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            high_quality_image = self.transform(image)\n",
    "\n",
    "            high_quality_image_0_1 = (high_quality_image * 0.5 + 0.5)\n",
    "            \n",
    "            if self.degradation_transform:\n",
    "                low_quality_image_0_1 = self.degradation_transform(high_quality_image_0_1)\n",
    "            else:\n",
    "                low_quality_image_0_1 = high_quality_image_0_1\n",
    "\n",
    "            low_quality_image = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])(low_quality_image_0_1)\n",
    "\n",
    "            return low_quality_image, high_quality_image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a dummy tensor if image loading fails\n",
    "            dummy_tensor = torch.zeros(3, self.image_size, self.image_size)\n",
    "            return dummy_tensor, dummy_tensor\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.648267Z",
     "iopub.status.busy": "2025-06-05T08:30:53.647820Z",
     "iopub.status.idle": "2025-06-05T08:30:53.663750Z",
     "shell.execute_reply": "2025-06-05T08:30:53.663066Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.648238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. U-Net Building Blocks - Architecture Deep Dive\n",
    "\n",
    "#### **UNetDown: Encoder Block Analysis**\n",
    "```python\n",
    "layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n",
    "```\n",
    "\n",
    "**Convolutional Layer Parameters:**\n",
    "- **Kernel Size 4:** 4x4 filter for feature detection\n",
    "- **Stride 2:** Moves filter 2 pixels at a time → halves image dimensions\n",
    "- **Padding 1:** Adds 1 pixel border to maintain spatial relationships\n",
    "- **bias=False:** Batch normalization handles bias, so we disable it here\n",
    "\n",
    "**Layer Sequence:**\n",
    "1. **Convolution:** Feature extraction and downsampling\n",
    "2. **Batch Normalization:** Stabilizes training, prevents gradient problems\n",
    "3. **LeakyReLU:** Activation function allowing small negative gradients\n",
    "4. **Dropout:** Randomly zeros neurons to prevent overfitting\n",
    "\n",
    "#### **UNetUp: Decoder Block Analysis**\n",
    "```python\n",
    "nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "```\n",
    "\n",
    "**Transposed Convolution (Upsampling):**\n",
    "- **Purpose:** Increases image resolution while learning features\n",
    "- **Kernel Size 4:** Maintains symmetry with encoder\n",
    "- **Stride 2:** Doubles image dimensions\n",
    "- **Skip Connection:** `torch.cat((x, skip_input), 1)` preserves fine details\n",
    "\n",
    "**Why Skip Connections?**\n",
    "- **Problem:** Deep networks lose fine details during downsampling\n",
    "- **Solution:** Directly connect encoder features to decoder\n",
    "- **Result:** Combines high-level understanding with low-level details\n",
    "- **Medical Importance:** Preserves critical anatomical structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.664800Z",
     "iopub.status.busy": "2025-06-05T08:30:53.664496Z",
     "iopub.status.idle": "2025-06-05T08:30:53.678162Z",
     "shell.execute_reply": "2025-06-05T08:30:53.677497Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.664778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Discriminator Architecture - PatchGAN Deep Analysis\n",
    "\n",
    "#### **Input Processing:**\n",
    "```python\n",
    "def forward(self, img_A, img_B):\n",
    "    img_input = torch.cat((img_A, img_B), 1)  # Concatenate along channel dimension\n",
    "```\n",
    "**img_A:** Degraded input image (3 channels)\n",
    "**img_B:** High-quality target/generated image (3 channels)\n",
    "**Result:** 6-channel input (concatenated RGB + RGB)\n",
    "\n",
    "#### **Network Architecture Breakdown:**\n",
    "\n",
    "**Layer 1:** `Conv2d(6→64, kernel=4, stride=2)` \n",
    "- Input: 256×256×6 → Output: 128×128×64\n",
    "- No batch norm for first layer (common GAN practice)\n",
    "\n",
    "**Layer 2:** `Conv2d(64→128)` + BatchNorm + LeakyReLU\n",
    "- Input: 128×128×64 → Output: 64×64×128\n",
    "- Batch normalization stabilizes training\n",
    "\n",
    "**Layer 3:** `Conv2d(128→256)` + BatchNorm + LeakyReLU  \n",
    "- Input: 64×64×128 → Output: 32×32×256\n",
    "- Increasing feature channels, decreasing spatial resolution\n",
    "\n",
    "**Layer 4:** `Conv2d(256→512, stride=1)` + BatchNorm + LeakyReLU\n",
    "- Input: 32×32×256 → Output: 31×31×512\n",
    "- Stride=1 maintains resolution for final classification\n",
    "\n",
    "**Layer 5:** `Conv2d(512→1, stride=1)` \n",
    "- Input: 31×31×512 → Output: 30×30×1\n",
    "- Final classification layer (no activation - uses BCEWithLogitsLoss)\n",
    "\n",
    "#### **PatchGAN Concept:**\n",
    "- **Traditional Discriminator:** Single \"real/fake\" decision for entire image\n",
    "- **PatchGAN:** 30×30 = 900 separate \"real/fake\" decisions per image\n",
    "- **Advantage:** Focuses on local texture and detail quality\n",
    "- **Medical Benefit:** Ensures fine anatomical structures are preserved accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.679039Z",
     "iopub.status.busy": "2025-06-05T08:30:53.678814Z",
     "iopub.status.idle": "2025-06-05T08:30:53.686805Z",
     "shell.execute_reply": "2025-06-05T08:30:53.686170Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.679023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(pretrained=True).features\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            vgg[:2],\n",
    "            vgg[2:7],\n",
    "            vgg[7:12],\n",
    "            vgg[12:21]\n",
    "        ).to(DEVICE).eval()\n",
    "\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, fake_img, real_img):\n",
    "        normalize_vgg = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        fake_img_norm = normalize_vgg((fake_img * 0.5 + 0.5))\n",
    "        real_img_norm = normalize_vgg((real_img * 0.5 + 0.5))\n",
    "\n",
    "        fake_features = self.features(fake_img_norm)\n",
    "        real_features = self.features(real_img_norm)\n",
    "        \n",
    "        return self.criterion(fake_features, real_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Perceptual Loss - Advanced Feature Matching\n",
    "\n",
    "#### **Core Concept:**\n",
    "Instead of comparing pixels directly, perceptual loss compares high-level features extracted by a pre-trained neural network.\n",
    "\n",
    "#### **VGG19 Feature Extraction:**\n",
    "```python\n",
    "vgg = vgg19(pretrained=True).features  # Load pre-trained ImageNet weights\n",
    "```\n",
    "\n",
    "**Why VGG19?**\n",
    "- Trained on millions of natural images\n",
    "- Learns hierarchical feature representations\n",
    "- Lower layers: edges, textures, colors\n",
    "- Higher layers: shapes, objects, semantic content\n",
    "\n",
    "#### **Feature Layer Selection:**\n",
    "```python\n",
    "self.features = nn.Sequential(\n",
    "    vgg[:2],    # Early conv layers - basic edges and textures\n",
    "    vgg[2:7],   # Mid conv layers - simple patterns\n",
    "    vgg[7:12],  # Higher conv layers - complex patterns  \n",
    "    vgg[12:21]  # Deep conv layers - semantic features\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Normalization for VGG:**\n",
    "```python\n",
    "normalize_vgg = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "```\n",
    "**Purpose:** VGG was trained with ImageNet normalization\n",
    "**Process:** Convert from GAN range [-1,1] to ImageNet range [0,1], then apply VGG normalization\n",
    "\n",
    "#### **Loss Calculation:**\n",
    "```python\n",
    "fake_features = self.features(fake_img_norm)\n",
    "real_features = self.features(real_img_norm)\n",
    "return self.criterion(fake_features, real_features)  # L1 Loss between features\n",
    "```\n",
    "\n",
    "#### **Medical Imaging Benefits:**\n",
    "1. **Texture Preservation:** Ensures tissue textures look natural\n",
    "2. **Structural Consistency:** Maintains anatomical relationships\n",
    "3. **Visual Realism:** Generated images appear more convincing to medical professionals\n",
    "4. **Detail Enhancement:** Focuses on perceptually important features rather than every pixel\n",
    "\n",
    "#### **Mathematical Formulation:**\n",
    "```\n",
    "Perceptual Loss = ||φ(generated_image) - φ(target_image)||₁\n",
    "```\n",
    "Where φ represents the VGG feature extraction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.688104Z",
     "iopub.status.busy": "2025-06-05T08:30:53.687691Z",
     "iopub.status.idle": "2025-06-05T08:30:53.704177Z",
     "shell.execute_reply": "2025-06-05T08:30:53.703438Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.688087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    dataset = KvasirSEGDataset(\n",
    "        root_dir=Config.DATA_ROOT,\n",
    "        image_size=Config.IMAGE_SIZE,\n",
    "        degradation_transform=lambda img_tensor: apply_degradation(img_tensor, Config)\n",
    "    )\n",
    "    # Reduced num_workers for local execution (use 0 for Windows compatibility)\n",
    "    dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "    if len(dataloader) == 0:\n",
    "        print(f\"Error: Dataloader is empty. No images found in {Config.DATA_ROOT} or batch size is too large for the dataset.\")\n",
    "        print(\"Please ensure the Kvasir-SEG dataset is correctly placed and contains images.\")\n",
    "        return # Exit if no data is found\n",
    "\n",
    "    generator = UnetGenerator(Config.NUM_CHANNELS, Config.NUM_CHANNELS).to(DEVICE)\n",
    "    discriminator = Discriminator(Config.NUM_CHANNELS).to(DEVICE)\n",
    "\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "    perceptual_loss_fn = PerceptualLoss()\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=Config.LEARNING_RATE_G, betas=(Config.BETA1, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=Config.LEARNING_RATE_D, betas=(Config.BETA1, 0.999))\n",
    "\n",
    "    def lambda_rule(epoch):\n",
    "        if epoch < Config.LR_DECAY_START_EPOCH:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 1.0 - (epoch - Config.LR_DECAY_START_EPOCH) / (Config.NUM_EPOCHS - Config.LR_DECAY_START_EPOCH)\n",
    "\n",
    "    scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
    "    scheduler_D = optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=lambda_rule)\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        for i, (low_res_img, high_res_img) in enumerate(dataloader):\n",
    "            low_res_img = low_res_img.to(DEVICE)\n",
    "            high_res_img = high_res_img.to(DEVICE)\n",
    "\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            real_labels = torch.ones(low_res_img.size(0), 1, 30, 30, device=DEVICE)\n",
    "            fake_high_res_for_D = generator(low_res_img)\n",
    "            \n",
    "            output_real = discriminator(low_res_img, high_res_img)\n",
    "            loss_D_real = criterion_GAN(output_real, real_labels)\n",
    "\n",
    "            fake_labels = torch.zeros(low_res_img.size(0), 1, 30, 30, device=DEVICE)\n",
    "            output_fake = discriminator(low_res_img, fake_high_res_for_D.detach())\n",
    "            loss_D_fake = criterion_GAN(output_fake, fake_labels)\n",
    "\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "\n",
    "            for _ in range(Config.G_TRAIN_MULTIPLIER):\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                fake_high_res_img = generator(low_res_img)\n",
    "\n",
    "                output_fake_for_G = discriminator(low_res_img, fake_high_res_img)\n",
    "                loss_G_GAN = criterion_GAN(output_fake_for_G, real_labels)\n",
    "\n",
    "                loss_G_L1 = criterion_L1(fake_high_res_img, high_res_img) * Config.LAMBDA_L1\n",
    "\n",
    "                loss_G_perceptual = perceptual_loss_fn(fake_high_res_img, high_res_img) * Config.LAMBDA_PERCEPTUAL\n",
    "\n",
    "                loss_G = loss_G_GAN + loss_G_L1 + loss_G_perceptual\n",
    "                loss_G.backward() \n",
    "                optimizer_G.step()\n",
    "\n",
    "            if (i + 1) % Config.LOG_EVERY_N_BATCHES == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{Config.NUM_EPOCHS}], Batch [{i+1}/{len(dataloader)}] | \"\n",
    "                      f\"D Loss: {loss_D.item():.4f} | G Loss: {loss_G.item():.4f} \"\n",
    "                      f\"(GAN: {loss_G_GAN.item():.4f}, L1: {loss_G_L1.item():.4f}, Perceptual: {loss_G_perceptual.item():.4f})\")\n",
    "        \n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        print(f\"Epoch {epoch+1} completed. Current LR G: {optimizer_G.param_groups[0]['lr']:.6f}, LR D: {optimizer_D.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "        if (epoch + 1) % Config.SAVE_EVERY_N_EPOCHS == 0 or (epoch + 1) == Config.NUM_EPOCHS:\n",
    "            print(f\"--- Entering save/checkpoint block for epoch {epoch+1} ---\")\n",
    "            print(f\"Saving samples and checkpoint for epoch {epoch+1}...\")\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    sample_low_res, sample_high_res = next(iter(dataloader))\n",
    "                    sample_low_res = sample_low_res.to(DEVICE)\n",
    "                    sample_high_res = sample_high_res.to(DEVICE)\n",
    "                    \n",
    "                    generated_high_res = generator(sample_low_res) \n",
    "                    save_images(sample_low_res, sample_high_res, generated_high_res, epoch + 1, i + 1, Config.OUTPUT_DIR)\n",
    "                except StopIteration:\n",
    "                    print(f\"Warning: Dataloader exhausted at epoch {epoch+1} for sample saving. Skipping image save for this epoch.\")\n",
    "\n",
    "\n",
    "            try:\n",
    "                torch.save(generator.state_dict(), os.path.join(Config.CHECKPOINT_DIR, f'generator_epoch_{epoch+1}.pth'))\n",
    "                torch.save(discriminator.state_dict(), os.path.join(Config.CHECKPOINT_DIR, f'discriminator_epoch_{epoch+1}.pth'))\n",
    "                print(f\"Models saved to {Config.CHECKPOINT_DIR}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model checkpoints for epoch {epoch+1}: {e}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training Process - GAN Training Dynamics Explained\n",
    "\n",
    "#### **A. Training Loop Structure**\n",
    "\n",
    "The training alternates between two competing networks:\n",
    "\n",
    "```python\n",
    "for epoch in range(Config.NUM_EPOCHS):\n",
    "    for i, (low_res_img, high_res_img) in enumerate(dataloader):\n",
    "        # 1. Train Discriminator (1 step)\n",
    "        # 2. Train Generator (3 steps) - G_TRAIN_MULTIPLIER = 3\n",
    "```\n",
    "\n",
    "#### **B. Discriminator Training Phase**\n",
    "\n",
    "**Step 1: Real Image Classification**\n",
    "```python\n",
    "output_real = discriminator(low_res_img, high_res_img)\n",
    "loss_D_real = criterion_GAN(output_real, real_labels)  # Should output \"1\" (real)\n",
    "```\n",
    "\n",
    "**Step 2: Fake Image Classification**  \n",
    "```python\n",
    "fake_high_res_for_D = generator(low_res_img)\n",
    "output_fake = discriminator(low_res_img, fake_high_res_for_D.detach())\n",
    "loss_D_fake = criterion_GAN(output_fake, fake_labels)  # Should output \"0\" (fake)\n",
    "```\n",
    "\n",
    "**Key Detail:** `.detach()` prevents gradients from flowing to generator during discriminator training.\n",
    "\n",
    "**Combined Discriminator Loss:**\n",
    "```python\n",
    "loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "```\n",
    "\n",
    "#### **C. Generator Training Phase (3x per discriminator step)**\n",
    "\n",
    "**Multi-Component Loss Function:**\n",
    "\n",
    "**1. Adversarial Loss:**\n",
    "```python\n",
    "output_fake_for_G = discriminator(low_res_img, fake_high_res_img)\n",
    "loss_G_GAN = criterion_GAN(output_fake_for_G, real_labels)  # Fool discriminator\n",
    "```\n",
    "*Goal:* Make discriminator classify generated images as \"real\"\n",
    "\n",
    "**2. L1 Reconstruction Loss:**\n",
    "```python\n",
    "loss_G_L1 = criterion_L1(fake_high_res_img, high_res_img) * Config.LAMBDA_L1\n",
    "```\n",
    "*Goal:* Pixel-wise accuracy (weighted by λ=70.0)\n",
    "\n",
    "**3. Perceptual Loss:**\n",
    "```python\n",
    "loss_G_perceptual = perceptual_loss_fn(fake_high_res_img, high_res_img) * Config.LAMBDA_PERCEPTUAL\n",
    "```\n",
    "*Goal:* High-level feature similarity (weighted by λ=10.0)\n",
    "\n",
    "**Total Generator Loss:**\n",
    "```python\n",
    "loss_G = loss_G_GAN + loss_G_L1 + loss_G_perceptual\n",
    "```\n",
    "\n",
    "#### **D. Learning Rate Scheduling**\n",
    "\n",
    "```python\n",
    "def lambda_rule(epoch):\n",
    "    if epoch < Config.LR_DECAY_START_EPOCH:  # First 30 epochs\n",
    "        return 1.0  # Keep original learning rate\n",
    "    else:  # Epochs 30-50\n",
    "        return 1.0 - (epoch - 30) / (50 - 30)  # Linear decay to 0\n",
    "```\n",
    "\n",
    "**Purpose:** Stabilizes training in later epochs, prevents oscillation around optimal solution.\n",
    "\n",
    "#### **E. Why Multiple Generator Updates?**\n",
    "\n",
    "**Problem:** Discriminator can become too strong, making generator unable to learn\n",
    "**Solution:** Train generator 3 times per discriminator update\n",
    "**Result:** Balanced competition between networks\n",
    "\n",
    "#### **F. Training Monitoring**\n",
    "\n",
    "Every 10 batches, the code logs:\n",
    "- Discriminator loss (should stabilize around 0.5)\n",
    "- Generator GAN loss (should decrease over time) \n",
    "- Generator L1 loss (pixel accuracy)\n",
    "- Generator perceptual loss (feature similarity)\n",
    "\n",
    "**Healthy Training Signs:**\n",
    "- Losses neither increase nor decrease dramatically\n",
    "- Generated images gradually improve in quality\n",
    "- No mode collapse (generator producing identical images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.705118Z",
     "iopub.status.busy": "2025-06-05T08:30:53.704905Z",
     "iopub.status.idle": "2025-06-05T08:30:53.715757Z",
     "shell.execute_reply": "2025-06-05T08:30:53.715029Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.705102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net Generator architecture for image-to-image translation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False) # 256 -> 128\n",
    "        self.down2 = UNetDown(64, 128) # 128 -> 64\n",
    "        self.down3 = UNetDown(128, 256) # 64 -> 32\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5) # 32 -> 16\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5) # 16 -> 8\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5) # 8 -> 4\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5) # 4 -> 2\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5) # 2 -> 1\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5) # 1 -> 2 (concat with down7)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5) # 2 -> 4 (concat with down6)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5) # 4 -> 8 (concat with down5)\n",
    "        self.up4 = UNetUp(1024, 512) # 8 -> 16 (concat with down4)\n",
    "        self.up5 = UNetUp(1024, 256) # 16 -> 32 (concat with down3)\n",
    "        self.up6 = UNetUp(512, 128) # 32 -> 64 (concat with down2)\n",
    "        self.up7 = UNetUp(256, 64) # 64 -> 128 (concat with down1)\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1), # 128 -> 256\n",
    "            nn.Tanh() # Output pixels in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final_conv(u7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Complete U-Net Generator - Architectural Masterpiece\n",
    "\n",
    "#### **U-Net Design Philosophy**\n",
    "- **U-Shape:** Symmetric encoder-decoder with skip connections\n",
    "- **Medical Heritage:** Originally designed for biomedical image segmentation\n",
    "- **Key Insight:** Combine global context (from bottleneck) with local details (from skip connections)\n",
    "\n",
    "#### **Encoder Path (Contracting Path)**\n",
    "```\n",
    "Input: 256×256×3 (RGB medical image)\n",
    "    ↓ down1: Conv 3→64, stride=2\n",
    "128×128×64\n",
    "    ↓ down2: Conv 64→128, stride=2  \n",
    "64×64×128\n",
    "    ↓ down3: Conv 128→256, stride=2\n",
    "32×32×256\n",
    "    ↓ down4: Conv 256→512, stride=2 + Dropout(0.5)\n",
    "16×16×512\n",
    "    ↓ down5: Conv 512→512, stride=2 + Dropout(0.5)\n",
    "8×8×512\n",
    "    ↓ down6: Conv 512→512, stride=2 + Dropout(0.5)\n",
    "4×4×512\n",
    "    ↓ down7: Conv 512→512, stride=2 + Dropout(0.5)\n",
    "2×2×512\n",
    "    ↓ down8: Conv 512→512, stride=2 + Dropout(0.5)\n",
    "1×1×512 (Bottleneck - Global Context)\n",
    "```\n",
    "\n",
    "#### **Decoder Path (Expansive Path)**\n",
    "```\n",
    "1×1×512 (Bottleneck)\n",
    "    ↑ up1: TransConv 512→512 + concat(down7) = 1024 channels\n",
    "2×2×1024\n",
    "    ↑ up2: TransConv 1024→512 + concat(down6) = 1024 channels  \n",
    "4×4×1024\n",
    "    ↑ up3: TransConv 1024→512 + concat(down5) = 1024 channels\n",
    "8×8×1024\n",
    "    ↑ up4: TransConv 1024→512 + concat(down4) = 1024 channels\n",
    "16×16×1024\n",
    "    ↑ up5: TransConv 1024→256 + concat(down3) = 512 channels\n",
    "32×32×512\n",
    "    ↑ up6: TransConv 512→128 + concat(down2) = 256 channels\n",
    "64×64×256\n",
    "    ↑ up7: TransConv 256→64 + concat(down1) = 128 channels\n",
    "128×128×128\n",
    "    ↑ final_conv: TransConv 128→3 + Tanh()\n",
    "Output: 256×256×3 (Enhanced medical image)\n",
    "```\n",
    "\n",
    "#### **Skip Connection Magic**\n",
    "```python\n",
    "u1 = self.up1(d8, d7)  # Concatenate bottleneck with down7 features\n",
    "```\n",
    "\n",
    "**Information Flow:**\n",
    "- **Encoder:** Captures \"what\" (semantic understanding)\n",
    "- **Decoder:** Reconstructs \"where\" (spatial localization) \n",
    "- **Skip Connections:** Preserve \"how\" (fine-grained details)\n",
    "\n",
    "#### **Dropout Strategy**\n",
    "- **Deep Layers (down4-down8):** 50% dropout prevents overfitting\n",
    "- **Shallow Layers:** No dropout preserves important low-level features\n",
    "- **Medical Rationale:** Ensures model generalizes across different imaging conditions\n",
    "\n",
    "#### **Channel Progression Logic**\n",
    "- **Encoder:** 3→64→128→256→512→512... (increasing feature complexity)\n",
    "- **Decoder:** ...512→256→128→64→3 (decreasing to output channels)\n",
    "- **Symmetry:** Enables precise reconstruction with learned enhancements\n",
    "\n",
    "#### **Output Activation: Tanh()**\n",
    "```python\n",
    "nn.Tanh()  # Output pixels in [-1, 1]\n",
    "```\n",
    "**Purpose:** Matches the normalized input range, enables stable training dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.716683Z",
     "iopub.status.busy": "2025-06-05T08:30:53.716434Z",
     "iopub.status.idle": "2025-06-05T08:30:53.727347Z",
     "shell.execute_reply": "2025-06-05T08:30:53.726795Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.716663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(generator_path):\n",
    "    print(\"\\nStarting Evaluation...\")\n",
    "    generator = UnetGenerator(Config.NUM_CHANNELS, Config.NUM_CHANNELS).to(DEVICE)\n",
    "    generator.load_state_dict(torch.load(generator_path, map_location=DEVICE))\n",
    "    generator.eval()\n",
    "\n",
    "    dataset = KvasirSEGDataset(\n",
    "        root_dir=Config.DATA_ROOT,\n",
    "        image_size=Config.IMAGE_SIZE,\n",
    "        degradation_transform=lambda img_tensor: apply_degradation(img_tensor, Config)\n",
    "    )\n",
    "    # Reduced num_workers for local execution\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "    if len(eval_dataloader) == 0:\n",
    "        print(f\"Error: Evaluation Dataloader is empty. Cannot perform evaluation.\")\n",
    "        return\n",
    "\n",
    "    all_psnr = []\n",
    "    all_ssim = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (low_res_img, high_res_img) in enumerate(eval_dataloader):\n",
    "            low_res_img = low_res_img.to(DEVICE)\n",
    "            high_res_img = high_res_img.to(DEVICE)\n",
    "\n",
    "            fake_high_res_img = generator(low_res_img)\n",
    "            \n",
    "            psnr, ssim = calculate_metrics(high_res_img, fake_high_res_img)\n",
    "            all_psnr.append(psnr)\n",
    "            all_ssim.append(ssim)\n",
    "\n",
    "            if (i + 1) % Config.LOG_EVERY_N_BATCHES == 0:\n",
    "                print(f\"Eval Batch [{i+1}/{len(eval_dataloader)}] | Avg PSNR: {np.mean(all_psnr):.2f} | Avg SSIM: {np.mean(all_ssim):.4f}\")\n",
    "\n",
    "    avg_psnr = np.mean(all_psnr)\n",
    "    avg_ssim = np.mean(all_ssim)\n",
    "    print(f\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.2f}\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "    print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Model Evaluation - Quantitative Quality Assessment\n",
    "\n",
    "#### **Evaluation Process Overview**\n",
    "1. Load trained generator model\n",
    "2. Process test images through degradation → enhancement pipeline  \n",
    "3. Compare enhanced images with original high-quality images\n",
    "4. Calculate objective quality metrics\n",
    "\n",
    "#### **PSNR (Peak Signal-to-Noise Ratio)**\n",
    "```python\n",
    "psnr = peak_signal_noise_ratio(real_images_np[i], fake_images_np[i], data_range=1.0)\n",
    "```\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "PSNR = 20 × log₁₀(MAX_I / √MSE)\n",
    "```\n",
    "Where:\n",
    "- MAX_I = maximum possible pixel value (1.0 for normalized images)\n",
    "- MSE = Mean Squared Error between images\n",
    "\n",
    "**Interpretation:**\n",
    "- **Higher = Better** (typically 20-40 dB for good quality)\n",
    "- Measures pixel-level accuracy\n",
    "- Medical relevance: Quantifies preservation of fine anatomical details\n",
    "\n",
    "#### **SSIM (Structural Similarity Index)**\n",
    "```python\n",
    "ssim = structural_similarity(real_images_np[i], fake_images_np[i], \n",
    "                           data_range=1.0, channel_axis=-1)\n",
    "```\n",
    "\n",
    "**SSIM Components:**\n",
    "1. **Luminance:** `l(x,y) = (2μₓμᵧ + c₁)/(μₓ² + μᵧ² + c₁)`\n",
    "2. **Contrast:** `c(x,y) = (2σₓσᵧ + c₂)/(σₓ² + σᵧ² + c₂)`  \n",
    "3. **Structure:** `s(x,y) = (σₓᵧ + c₃)/(σₓσᵧ + c₃)`\n",
    "\n",
    "**Combined SSIM:**\n",
    "```\n",
    "SSIM(x,y) = l(x,y) × c(x,y) × s(x,y)\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **Range:** -1 to 1 (1 = identical images)\n",
    "- **Advantage:** Correlates better with human perception than PSNR\n",
    "- **Medical relevance:** Ensures structural integrity of anatomical features\n",
    "\n",
    "#### **Why Both Metrics?**\n",
    "- **PSNR:** Pixel-perfect accuracy (important for diagnostic details)\n",
    "- **SSIM:** Perceptual quality (important for visual assessment)\n",
    "- **Combined:** Comprehensive quality evaluation\n",
    "\n",
    "#### **Evaluation Best Practices**\n",
    "```python\n",
    "generator.eval()  # Disable dropout and batch norm training mode\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "```\n",
    "\n",
    "**No-Gradient Context:**\n",
    "- Speeds up inference significantly\n",
    "- Prevents accidental weight updates during evaluation\n",
    "- Reduces memory consumption\n",
    "\n",
    "#### **Expected Results for Medical Images:**\n",
    "- **Good PSNR:** > 25 dB (sharp, detailed reconstruction)\n",
    "- **Good SSIM:** > 0.85 (perceptually similar to original)\n",
    "- **Excellent Performance:** PSNR > 30 dB, SSIM > 0.90\n",
    "\n",
    "#### **Clinical Validation Considerations:**\n",
    "While PSNR and SSIM provide objective measures, clinical validation would require:\n",
    "- Radiologist assessment of diagnostic quality\n",
    "- Comparison with original diagnosis accuracy\n",
    "- Evaluation across diverse pathological conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:30:53.730066Z",
     "iopub.status.busy": "2025-06-05T08:30:53.729680Z",
     "iopub.status.idle": "2025-06-05T11:06:32.352399Z",
     "shell.execute_reply": "2025-06-05T11:06:32.351355Z",
     "shell.execute_reply.started": "2025-06-05T08:30:53.730049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory 'output/generated_images' created and ready.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput directory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig\u001b[38;5;241m.\u001b[39mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created and ready.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     19\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(Config.DATA_ROOT):\n",
    "        print(f\"Error: Data root directory not found at {Config.DATA_ROOT}\")\n",
    "        print(\"Please download the Kvasir-SEG dataset and extract it such that\")\n",
    "        print(f\"the image files are located under '{Config.DATA_ROOT}'\")\n",
    "        print(\"For example, if you download 'Kvasir-SEG.zip', extract it,\")\n",
    "        print(\"you should have a structure like: kvasirseg/Kvasir-SEG/images/image_00001.jpg\")\n",
    "        exit()\n",
    "\n",
    "    if os.path.exists(Config.OUTPUT_DIR):\n",
    "        print(f\"Clearing existing output directory: {Config.OUTPUT_DIR}\")\n",
    "        shutil.rmtree(Config.OUTPUT_DIR)\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "    print(f\"Output directory '{Config.OUTPUT_DIR}' created and ready.\")\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    train_model()\n",
    "\n",
    "    last_generator_checkpoint = os.path.join(Config.CHECKPOINT_DIR, f'generator_epoch_{Config.NUM_EPOCHS}.pth')\n",
    "    if os.path.exists(last_generator_checkpoint):\n",
    "        evaluate_model(last_generator_checkpoint)\n",
    "    else:\n",
    "        print(f\"Could not find the last generator checkpoint at {last_generator_checkpoint} for evaluation.\")\n",
    "        print(\"Ensure training completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Complete Execution Pipeline - Putting It All Together\n",
    "\n",
    "#### **A. Pre-Execution Validation**\n",
    "```python\n",
    "if not os.path.exists(Config.DATA_ROOT):\n",
    "    print(f\"Error: Data root directory not found at {Config.DATA_ROOT}\")\n",
    "    exit()\n",
    "```\n",
    "**Safety Check:** Prevents training from starting with missing dataset\n",
    "**User Guidance:** Provides clear instructions for dataset setup\n",
    "\n",
    "#### **B. Environment Preparation**\n",
    "```python\n",
    "if os.path.exists(Config.OUTPUT_DIR):\n",
    "    shutil.rmtree(Config.OUTPUT_DIR)  # Clean slate for new training\n",
    "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "```\n",
    "\n",
    "**Clean Start Strategy:**\n",
    "- Removes previous training artifacts\n",
    "- Ensures consistent output organization\n",
    "- Prevents confusion from mixed training runs\n",
    "\n",
    "#### **C. Reproducibility Setup**\n",
    "```python\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "```\n",
    "\n",
    "**Reproducibility Components:**\n",
    "- **CPU Random Seeds:** NumPy, Python random, PyTorch CPU operations\n",
    "- **GPU Random Seeds:** CUDA operations across all devices\n",
    "- **cuDNN Settings:** \n",
    "  - `deterministic=True`: Ensures identical results across runs\n",
    "  - `benchmark=False`: Disables optimization that introduces randomness\n",
    "\n",
    "**Trade-off:** Slight performance reduction for exact reproducibility\n",
    "\n",
    "#### **D. Training Pipeline Execution**\n",
    "```python\n",
    "train_model()  # Main training loop (50 epochs, ~several hours)\n",
    "```\n",
    "\n",
    "**What Happens During Training:**\n",
    "1. **Data Loading:** Batch-wise image loading with degradation\n",
    "2. **Model Updates:** Alternating discriminator/generator training\n",
    "3. **Monitoring:** Loss logging every 10 batches\n",
    "4. **Checkpointing:** Model saving every 5 epochs\n",
    "5. **Visualization:** Sample image generation for progress tracking\n",
    "\n",
    "#### **E. Automatic Evaluation**\n",
    "```python\n",
    "last_generator_checkpoint = os.path.join(Config.CHECKPOINT_DIR, f'generator_epoch_{Config.NUM_EPOCHS}.pth')\n",
    "if os.path.exists(last_generator_checkpoint):\n",
    "    evaluate_model(last_generator_checkpoint)\n",
    "```\n",
    "\n",
    "**Smart Evaluation:**\n",
    "- Only runs if training completed successfully\n",
    "- Uses the final trained model (epoch 50)\n",
    "- Provides quantitative assessment of enhancement quality\n",
    "\n",
    "#### **F. Expected Training Timeline**\n",
    "**Hardware Dependent:**\n",
    "- **GPU (GTX 1060+):** ~2-4 hours for 50 epochs\n",
    "- **CPU Only:** ~12-24 hours for 50 epochs\n",
    "- **High-End GPU (RTX 3080+):** ~30-60 minutes for 50 epochs\n",
    "\n",
    "#### **G. Output Structure After Completion**\n",
    "```\n",
    "output/\n",
    "└── generated_images/\n",
    "    ├── epoch_5_batch_X.png     # Training progress images\n",
    "    ├── epoch_10_batch_X.png\n",
    "    ├── ...\n",
    "    └── checkpoints/\n",
    "        ├── generator_epoch_5.pth     # Model weights\n",
    "        ├── discriminator_epoch_5.pth\n",
    "        ├── ...\n",
    "        ├── generator_epoch_50.pth    # Final trained model\n",
    "        └── discriminator_epoch_50.pth\n",
    "```\n",
    "\n",
    "#### **H. Success Indicators**\n",
    "**During Training:**\n",
    "- Steadily decreasing generator L1 loss (pixel accuracy improving)\n",
    "- Stable discriminator loss around 0.5 (balanced competition)\n",
    "- Visual improvement in generated sample images\n",
    "\n",
    "**After Training:**\n",
    "- PSNR > 25 dB (good pixel-level accuracy)\n",
    "- SSIM > 0.85 (good perceptual quality)\n",
    "- Checkpoints saved without errors\n",
    "\n",
    "#### **I. Troubleshooting Common Issues**\n",
    "**\"CUDA out of memory\":** Reduce BATCH_SIZE from 4 to 2 or 1\n",
    "**\"Dataset empty\":** Verify Kvasir-SEG folder structure\n",
    "**\"Training very slow\":** Ensure GPU is properly detected and utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T11:06:32.354202Z",
     "iopub.status.busy": "2025-06-05T11:06:32.353963Z",
     "iopub.status.idle": "2025-06-05T11:06:32.361172Z",
     "shell.execute_reply": "2025-06-05T11:06:32.360475Z",
     "shell.execute_reply.started": "2025-06-05T11:06:32.354178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking images in: kvasirseg/Kvasir-SEG/images\n",
      "Error: The specified data root directory does NOT exist: kvasirseg/Kvasir-SEG/images\n",
      "Please ensure your Kvasir-SEG dataset is correctly extracted and placed.\n",
      "Expected structure: /kaggle/working/kvasirseg/Kvasir-SEG/images/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    DATA_ROOT = 'data/Kvasir-SEG/images'  # Local path to the dataset\n",
    "\n",
    "print(f\"Checking images in: {Config.DATA_ROOT}\")\n",
    "\n",
    "if not os.path.exists(Config.DATA_ROOT):\n",
    "    print(f\"Error: The specified data root directory does NOT exist: {Config.DATA_ROOT}\")\n",
    "    print(\"Please ensure your Kvasir-SEG dataset is correctly extracted and placed.\")\n",
    "    print(\"Expected structure: data/Kvasir-SEG/images/\")\n",
    "    print(\"\\nTo set up the dataset:\")\n",
    "    print(\"1. Download the Kvasir-SEG dataset\")\n",
    "    print(\"2. Extract it to create a 'data' folder in your project directory\")\n",
    "    print(\"3. Ensure the structure is: data/Kvasir-SEG/images/image_*.jpg\")\n",
    "else:\n",
    "    image_files = [f for f in os.listdir(Config.DATA_ROOT) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if len(image_files) == 0:\n",
    "        print(f\"Warning: No image files (jpg, jpeg, png) found in {Config.DATA_ROOT}.\")\n",
    "        print(\"Please verify the contents of this directory.\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} image files. Here are the first 5:\")\n",
    "        for i, filename in enumerate(image_files[:5]):\n",
    "            print(os.path.join(Config.DATA_ROOT, filename))\n",
    "        print(\"Dataset path appears correct and images are found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Clinical Applications & Research Impact\n",
    "\n",
    "#### **Medical Imaging Enhancement Applications**\n",
    "\n",
    "**A. Endoscopic Imaging Improvements**\n",
    "- **Challenge:** Poor lighting conditions during procedures\n",
    "- **Solution:** GAN enhances visibility of tissue structures\n",
    "- **Benefit:** Improved polyp detection and diagnostic accuracy\n",
    "\n",
    "**B. Historical Image Archive Enhancement**  \n",
    "- **Challenge:** Older medical images with lower quality\n",
    "- **Solution:** Retroactively enhance archived images\n",
    "- **Benefit:** Improved analysis of longitudinal patient data\n",
    "\n",
    "**C. Real-Time Clinical Integration**\n",
    "- **Challenge:** Live imaging quality during procedures\n",
    "- **Solution:** Real-time enhancement for better visualization\n",
    "- **Benefit:** Immediate diagnostic improvement\n",
    "\n",
    "#### **Technical Achievements of This Implementation**\n",
    "\n",
    "**1. Multi-Loss Architecture:**\n",
    "- Combines pixel-level accuracy (L1) with perceptual quality (VGG features)\n",
    "- Adversarial training ensures realistic image generation\n",
    "- Balanced approach for medical imaging requirements\n",
    "\n",
    "**2. Robust Degradation Simulation:**\n",
    "- Gaussian noise: Sensor imperfections\n",
    "- Salt/pepper noise: Transmission errors  \n",
    "- Blur: Motion artifacts\n",
    "- Color variations: Lighting inconsistencies\n",
    "\n",
    "**3. Medical-Specific Optimizations:**\n",
    "- Conservative color adjustments (hue ±5% vs ±20% for brightness)\n",
    "- High L1 loss weight (70.0) for diagnostic accuracy\n",
    "- U-Net architecture preserves fine anatomical details\n",
    "\n",
    "#### **Research Extensions**\n",
    "\n",
    "**A. Multi-Modal Enhancement:**\n",
    "- Extend to CT, MRI, X-ray modalities\n",
    "- Modality-specific degradation patterns\n",
    "- Cross-modal knowledge transfer\n",
    "\n",
    "**B. Pathology-Aware Enhancement:**\n",
    "- Condition-specific training datasets\n",
    "- Pathology preservation during enhancement\n",
    "- Clinical validation studies\n",
    "\n",
    "**C. Real-Time Deployment:**\n",
    "- Model optimization for edge devices\n",
    "- Integration with endoscopic equipment\n",
    "- Live enhancement during procedures\n",
    "\n",
    "#### **Ethical Considerations**\n",
    "\n",
    "**A. Diagnostic Responsibility:**\n",
    "- Enhanced images should supplement, not replace, original data\n",
    "- Clear labeling of AI-enhanced content\n",
    "- Radiologist training on AI-enhanced interpretation\n",
    "\n",
    "**B. Validation Requirements:**\n",
    "- Clinical trials comparing diagnostic accuracy\n",
    "- Inter-observer agreement studies\n",
    "- Long-term patient outcome analysis\n",
    "\n",
    "**C. Regulatory Compliance:**\n",
    "- FDA/CE marking for clinical deployment\n",
    "- Quality assurance protocols\n",
    "- Audit trails for enhanced images\n",
    "\n",
    "#### **Performance Benchmarks**\n",
    "\n",
    "**Research Quality Targets:**\n",
    "- **PSNR > 30 dB:** Research-grade enhancement\n",
    "- **SSIM > 0.90:** Clinical-grade perceptual quality\n",
    "- **Processing Time < 100ms:** Real-time feasibility\n",
    "\n",
    "**Clinical Deployment Requirements:**\n",
    "- **Accuracy:** No false enhancement of pathological features\n",
    "- **Consistency:** Reliable performance across imaging conditions  \n",
    "- **Integration:** Seamless workflow incorporation\n",
    "- **Training:** Minimal additional staff education required\n",
    "\n",
    "This implementation provides a solid foundation for medical image enhancement research and potential clinical applications, with careful consideration of both technical performance and medical safety requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1316959,
     "sourceId": 7681479,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
